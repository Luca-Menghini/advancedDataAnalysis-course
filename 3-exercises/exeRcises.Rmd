---
title: 'ADVANCED DATA ANALYSIS \newline FOR PSYCHOLOGICAL SCIENCE'
subtitle: 'Homework exercises'
author:  |
 |
 | **Luca Menghini Ph.D.** \fontsize{9pt}{7.2}\selectfont
 |
 | luca.menghini@unipd.it
 |
 |
 | ***
 | Master degree in Developmental and Educational Psychology 
 |
 | University of Padova
 |
 | 2023-2024
 |
 | ![](img/logo.PNG){width=1.7in}
output:
  beamer_presentation:
    fonttheme: serif
    theme: Singapore
    slide_level: 2
    includes:
      in_header: mystyle.tex
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE,tidy.opts = list(width.cutoff=80))
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = NA
)
library(Cairo)

```

## Some instructions to solve the exercises

\fontsize{7.5pt}{12}\selectfont
The present document includes some optional homework exercises on the contents presented during lectures. Similar to the course slides, exercises will be progressively updated as the course progresses.

To check the **exercise solutions**, just look at the `exeRcises.Rmd` file on either Github or Moodle: each exercise text is followed by a chunk of R code showing point-by-point solutions (or some among the many possible solutions).

If you have any doubts on how to solve the exercises, feel free to write me an e-mail or (even better) try writing in the **Moodle forum**, so that all students can see your question and try to reply it. We can also solve some exercise during lectures, but let me know! ;)

# LM

## 1. Correlation & regression

\fontsize{7.5pt}{12}\selectfont

For each couple of variables ($x,y$) generated as specified below:

a) represent univariate (boxplot) and bivariate distributions (scatter plot)

b) compute their correlation

c) use the `lm()` function to get the slope coefficient $\beta_1$ and determinate whether the relationship significantly differs from zero \newline

1. `y <- rnorm(50)` and `x1 <- y`

2. `x2 <- y + 10`

3. `x3 <- rnorm(50)`

4. `x4 <- x3 + 10`

5. Which conclusions can we draw? Which relationship between correlation and regression coefficient?

\fontsize{5.5pt}{12}\selectfont ____ \newline Source: Pastore, M. (2021). Analisi dei dati in contesti di comunità. Course held at the University of Padova, Academic Year 2021-2022.

```{r echo=FALSE,eval=FALSE}
# SOLUTION TO EXERCISE #1

rm(list=ls()) # emptying the work environment

# 1. y <- rnorm(50) and x1 <- y
y <- rnorm(50) # assigning values
x1 <- y
# a) univariate (boxplot) and bivariate distributions (scatter plot)
# par(mfrow=c(1,2)) # to have 2 graphs per plot
boxplot(y)
boxplot(x1) # boxplot: they are the same
# par(mfrow=c(1,1)) # to go back to 1 graph per plot
plot(y ~ x1) # scatter plot: perfectly correlated
# b) correlation
cor(x1,y) # r = 1 (perfect correlation)
# c) simple linear regression
fit <- lm(y ~ x1)
coefficients(fit) # when x1 = 0, y = 0; when x1 increases by 1 unit, y increases by 1 unit (they are the same)

# 2. x2 <- y + 10
x2 <- y + 10 # assigning values
# a) univariate (boxplot) and bivariate distributions (scatter plot)
# par(mfrow=c(1,2)) # to have 2 graphs per plot
boxplot(y)
boxplot(x2) # boxplot: same shape but centered on different values (0 vs. 10)
# par(mfrow=c(1,1)) # to go back to 1 graph per plot
plot(y ~ x2) # scatter plot: still perfectly correlated
# b) correlation
cor(x2,y) # r = 1 (perfect correlation)
# c) simple linear regression
fit <- lm(y ~ x2)
coefficients(fit) # when x1 = 0, y = -10; when x1 increases by 1 unit, y increases by 1 unit (perfect correlation)

# 3. x3 <- rnorm(50)
x3 <- rnorm(50) # assigning values
# a) univariate (boxplot) and bivariate distributions (scatter plot)
# par(mfrow=c(1,2)) # to have 2 graphs per plot
boxplot(y)
boxplot(x3) # boxplot: different shapes (y variates more) but both centered on zero
# par(mfrow=c(1,1)) # to go back to 1 graph per plot
plot(y ~ x3) # scatter plot: the correlation is no longer perfect
# b) correlation
cor(x3,y) # I got r = 0.03 but you might get a different value since y and x3 are randomly generated
# c) simple linear regression
fit <- lm(y ~ x3)
coefficients(fit) # I got b0 = -0.21 (when x3 = 0, y = -0.21) and b1 = 0.03 (when x increase by 1 unit, y increases by 0.03 units)

# 4. x4 <- x3 + 10
x4 <- x3 + 10 # assigning values
# a) univariate (boxplot) and bivariate distributions (scatter plot)
par(mfrow=c(1,2)) # to have 2 graphs per plot
boxplot(y)
boxplot(x4) # boxplot: different shapes (y variates more) and different central value (0 vs. 10)
# par(mfrow=c(1,1)) # to go back to 1 graph per plot
plot(y ~ x4) # scatter plot: the correlation is no longer perfect
# b) correlation
cor(x4,y) # I got r = 0.03 but you might get a different value since y and x3 are randomly generated
# c) simple linear regression
fit <- lm(y ~ x4)
coefficients(fit) # I got b0 = -0.56 (when x3 = 0, y = -0.56) and b1 = 0.03 (when x increase by 1 unit, y increases by 0.03 units)

# Which conclusions can we draw? Which relationship between correlation and regression coefficient?
# when two variables are identical or just differ by a constant (here, 10) their correlation is perfect
# in contrast, two randomly generated variables poorly correlate, regardless of the contrast between them
# in this particular case, where observations are randomly sampled from the normal distribution
  # the regression coefficients (slope) is almost identical to the Pearson correlation coefficient
```

## 2. LM assumptions & diagnostics

\fontsize{7.5pt}{12}\selectfont

Using the “*Pregnancy during pandemics*” data* that we saw in class, graphically evaluate the diagnostics of the selected model `m2`:

1. **Linearity**: are model residuals centered on zero?

2. **Normality**: are model residuals normally distributed?

3. **Homoscedasticity**: is residual variance constant over the levels of any predictor?

4. **Independence error-predictor**: are residuals unrelated to any predictor?

5. **Independence of observations**: based on the considered variables (`depr`, `threat`, `NICU`, and `age`), are individual observations independent?

6. **Absence of influential observations**: is there any observation that strongly influence the estimated coefficients?

7. **Absence of multicollinearity**: are predictors mutually unrelated?

\fontsize{5.5pt}{12}\selectfont ____ \newline *To read the dataset, you can either use the code in `2-multilevel.pdf` slide #10 or download the `pregnancy.RData` file from Moodle/Github ("data" folder) and use the command \color{blue} `load("pregnancy.RData")`

```{r echo=FALSE,eval=FALSE}
# SOLUTION TO EXERCISE #2

rm(list=ls()) # emptying the work environment

# loading data (from Github)
library(osfr) # package to interact with the Open Science Framework platform
proj <- "https://osf.io/ha5dp/" # link to the OSF project
osf_download(osf_ls_files(osf_retrieve_node(proj))[2, ],conflicts="overwrite") # download
preg <- na.omit(read.csv("OSFData_Upload_2023_Mar30.csv",stringsAsFactors=TRUE)) # read data
colnames(preg)[c(2,5,12,14)] <- c("age","depr","NICU","threat") # set variable names

# alternative way for loading data (from working directory, once you downloaded them from GitHub or Moodle)
load("4-data/pregnancy.RData") # (note: I saved the file in the "4-data" subfolder of my working directory)
head(preg) # showing first rows (the dataset is called 'preg')

# fitting model m2
m2 <- lm(depr ~ threat + NICU + age, data = preg)

# 1. linearity
hist(residuals(m2)) # histogram of residuals: residuals seem quite (although not perfectly) centered on zero (ok)

# 2. normality
hist(residuals(m2)) # histogram of residuals: seem quite (although not perfectly) normally distributed (ok)
qqnorm(residuals(m2)); qqline(residuals(m2)) # only some deviation from normality in the lower tale of the residual distribution (i.e., positive skewness) (ok, negligible)

# alternative way to assess normality (note: both the performance and the qqplotr package should be installed)
plot(performance::check_normality(m2)) # the plot is equivalent to the previous one, but this representation seems to magnify the residual deviations from normality. Based on this plot, we might want to use alternative family distributions than the normal, due to the large deviation in the lower tail of the residual distribution

# 3. heteroscedasticity
plot(residuals(m2)~preg$threat) # residual variance (spreadness of dots) does not seem to change over the values of threat (ok)
plot(residuals(m2)~preg$age) # residual variance (spreadness of dots) does not seem to change over the values of age (ok)
plot(residuals(m2)~preg$NICU) # residual variance (size of the boxes) does not seem to change between NICU levels (ok)

# alternative way to assess heteroscedasticity considering all variables: reasiduals vs. fitted values
plot(m2,which=3) # residual vs. fitted values: if you see no clear trends, then you're ok (as in this case)
plot(performance::check_heteroscedasticity(m2)) # alternative way to generate the same plot

# 4. independence residuals - predictors
plot(residuals(m2)~preg$threat) # not a strong relationship between residuals and threat values (ok)
abline(lm(residuals(m2)~preg$threat),col="red") # to add a regression line
plot(residuals(m2)~preg$age) # not a strong relationship between residuals and age values (ok)
abline(lm(residuals(m2)~preg$age),col="red") # to add a regression line
plot(residuals(m2)~preg$NICU) # no substantial differences in residuals between NICU levels (ok)

# 5. independence of observations
# based on the considered variables, there are not uncontrolled factors that might account for local dependencies in the data (e.g., hospitals, cities, neighborhoods, etc.) (ok)

# 6. Absence of influential observations: is there any observation that strongly influence the estimated coefficients?
plot(m2,which=5) # no data points outside the Cook's distance cut-off region (which is not even included within the plot) (ok)

# alternative ways to evaluate influential cases
car::leveragePlots(m2) # this shows the leverage (a measure of influence) for each x-y couple
# participants 185 and 8491 show the most extreme values (although not excessively extreme), and might deserve a deeper inpsection (i.e., what happens if I remove them and re-run the model?)
car::outlierTest(m2) # testing whether outliers are present in the data (here, p > .05 so we're ok)
car::influencePlot(m2) # inspecting both Cook's distance and leverages: we see a few cases with high laverage (1196) or high Cook's distance (e.g., 9100), which might deserve a deeper inspection

# 7. Absence of multicollinearity: Are predictors mutually unrelated?
cor(preg[,c("threat","age")]) # threat and age only correlate at r = 0.02 (low linear correlation)
library(car)
barplot(vif(m2),ylim=c(0,10)); abline(h=5,lty=2,col="red") # no VIF higher than 5 (ok)
```

## 3. Towards multilevel modeling

\fontsize{7.5pt}{12}\selectfont

1. Download and read the “*Adolescent insomnia*” dataset `INSA.RData` (Moodle/Github, "data" folder)

2. Explore the variables `dayNr` (day of assessment), `stress` (bedtime rating of daily stress), `insomnia` (categorical: insomnia vs. controls), and `TST` (total sleep time, in minutes) &rightarrow; mean, SD, frequencies, plots, and correlations

3. Fit a null model `m0` predicting `TST`

4. Fit a simple regression model `m1` predicting `TST` by `stress`

5. Fit a multiple regression model `m3` predicting `TST` by `stress` and `insomnia`

6. Compare the three models with the AIC and the likelihood ratio test

7. Print and interpret the coefficients (and their statistical significance) of the selected model

8. Now create two subsets of the `insa` dataset: `insa1` only including observations from the participant `s001` and `insa2` with observations from participant `s002`: how many rows in each dataset?

9. Repeat points 3-7 by using the two subsets: Are results consistent with what you found in the full sample?

```{r echo=FALSE,eval=FALSE}
# SOLUTION TO EXERCISE #3

rm(list=ls()) # emptying the work environment

# 1. loading data from Github
repo <- "https://github.com/SRI-human-sleep/INSA-home" # loading datasets from GitHub
load(url(paste0(repo,"/raw/main/Appendix%20D%20-%20Data/emaFINAL.RData")))
# alternatively, you can download the ema dataset from Github on Moodle, paste it in your working directory, and run the following lines (note: I saved the file in the "4-data" subfolder of my working directory):
load("4-data/insa.RData")

# 2. Explore the variables `dayNr`, `stress`, `insomnia`, and `TST`
# descriptives
mean(insa$dayNr,na.rm=T); sd(insa$dayNr,na.rm=T) # on average, 34 days per participant (SD = 21.2 days)
mean(insa$stress,na.rm=T); sd(insa$stress,na.rm=T) # on average, stress = 2.2 (over 5) (SD = 1)
table(insa$insomnia) # 3122 observations from the insomnia group, 3097 from the control group
mean(insa$TST,na.rm=T); sd(insa$TST,na.rm=T) # on average, 414 minutes of sleep (SD = 80.9 minutes)
# plots
par(mfrow=c(2,2)) # to have 4 graphs in 1 plot
hist(insa$dayNr) # skewed variable with most values being below 50 days, and a few values up to 350 days
hist(insa$stress) # skewed variables with most values being 1 or 2, and a few values being 5
plot(insa$insomnia) # quite balanced number of observations between groups
hist(insa$TST) # quite normally distributed variable
# correlations: very small positive correlation between TST and dayNr, very small negative correlation between stress and both dayNr and TST
cor(insa[,c("dayNr","stress","TST")],use="complete.obs")

# 3. Fit a null model `m0` predicting `TST`
m0 <- lm(TST ~ 1, data=insa[!is.na(insa$stress),]) # note: selecting only cases with nonmissing stress values, otherwise this and the following models will have a different sample size (by default, R uses all nonmissing observations)

# 4. Fit a simple regression model `m1` predicting `TST` by `stress`
m1 <- lm(TST ~ stress, data=insa)

# 5. Fit a multiple regression model `m3` predicting `TST` by `stress` and `insomnia`
m2 <- lm(TST ~ stress + insomnia, data=insa)

# 6. Compare the three models with the AIC and the likelihood ratio test
AIC(m0,m1,m2) # AIC: the lower the better -> best model is m2
lmtest::lrtest(m0,m1,m2) # both m1 and m2 are significant -> best model is m2

# 7. Print and interpret the coefficients (and their statistical significance) of the selected model
summary(m2)$coefficients
# when stress = 0 and insomnia = 0 (control group), the most expected TST value (mean) is 418.16 minutes, which is significantly higher than zero
# when insomnia = 0 (control group), a 1-unit increase in stress predicts a 3.3-minute decrease in TST, which is statistically significant
# when stress = 0, the insomnia group is predicted to show a mean TST of 5.46 higher than the control group, but this difference is not statistically significant 

# 8. subsample data from participant s001 and s002: how many rows?
insa1 <- insa[insa$ID=="s001",]
nrow(insa1) # 93 rows (days) from participant s001
insa2 <- insa[insa$ID=="s002",]
nrow(insa2) # 65 rows (days) from participant s002

# 9. Repeat points 3-7 by using the two subsets: Are results consistent with what you found in the full sample?
# s001
m0 <- lm(TST ~ 1, data = insa1[!is.na(insa1$stress),])
m1 <- lm(TST ~ stress, data = insa1)
# m2 <- lm(TST ~ stress + insomnia, data = insa1) # error! Because when we only sample 1 participant, then the insomnia variable does not variate --> so we do not consider model m2, but only m0 and m1
AIC(m0,m1) # m0 is better than m1
lmtest::lrtest(m0,m1) # m1 is not significantly better than m0
# s002
m0 <- lm(TST ~ 1, data = insa2[!is.na(insa2$stress),])
m1 <- lm(TST ~ stress, data = insa2)
AIC(m0,m1) # m0 is better than m1
lmtest::lrtest(m0,m1) # m1 is not significantly better than m0
# conclusion: if we only consider participant s001 or participant s002, the results are different than those obtained from the full sample (i.e., the relationship between stress and TST is no longer significant)
```

# LMER

## 4. Multilevel data structure {#ex4}

\fontsize{7.5pt}{12}\selectfont

1. Download and read the “*Innovative teaching program*” dataset `studentData.csv` (Moodle/Github, "data" folder)

2. Explore the student-level variables `studId` (identification code of each student), `math_grade` (student grade in math) and `anxiety` (anxiety level). What is the total number of students? How many rows per students? What is the range of `math_grade` and `anxiety`? 

3. How many students per class? How many students per level of the `tp` variable?

4. How many classes per level of the `tp` variable? \fontsize{6pt}{12}\selectfont To answer that, you can create the **wide-form dataset** by taking only one row per class (e.g., try using the `duplicated()` function preceded by the `!` symbol to remove duplicated values of `classID`) \fontsize{7.5pt}{12}\selectfont

5. Compute the mean `math_grade` and `anxiety` value for each class and join them to the wide-form dataset: which is the class with the maximum `math_grade`? Which class has the maximum `anxiety` level?

6. Fit a simple linear regression model predicting `math_grade` by `anxiety` both on the long-form and on the wide-form dataset; inspect and interpret the estimated coefficients and their statistical significance.

7. Which model has the highest standard errors? Why?

```{r echo=FALSE,eval=FALSE}
# SOLUTION TO EXERCISE #4

# 1. Download and read the studentData.csv dataset
rm(list=ls()) # emptying the work environment
getwd() # getting where your working directory is --> copy the file in there
itp <- read.csv("studentData.csv")

# 2. explore student-level variables: No. of students? 
table(itp$studID) # number of observations per student (1)
nrow(itp) # number of students = 90
mean(itp$math_grade); sd(itp$math_grade) # on average, participants took 8 in math (SD = 0.6)
mean(itp$anxiety); sd(itp$anxiety) # on average, participants reported anxiety levels of 3.11 (SD = 0.06)
summary(itp[,c("math_grade","anxiety")]) # math_grade ranges from 6.9 to 9.3; anxiety ranges from 3.01 to 3.41
 
# 3. How many students per class? How many students per level of the `tp` variable (teaching program)?
table(itp$classID) # from 11 to 30 students per class
table(itp$tp) # 52 students in the control group, 38 students in the intervention group

# 4. How may classes per tp level?
itp_wide <- itp[!duplicated(itp$classID),] # from long-form to wide-form: removing duplicated classID values
nrow(itp_wide)  # itp_wide only has 4 rows (1 row per cluster)
table(itp_wide$tp) # 2 classes per level of the tp variable (2 in the control, 2 in the intervention group)

# 5. mean math_grade and anxiety per class, which class has the maximum value?
mathGr <- aggregate(math_grade ~ classID, data=itp, FUN = mean) # math_grade by class
anx <- aggregate(anxiety ~ classID, data = itp, FUN = mean) # anxiety by class
mathGr[which(mathGr$math_grade==max(mathGr$math_grade)),] # class D has the maximum math_grade
mathGr[which(anx$anxiety==max(anx$anxiety)),] # class A has the maximum anxiety
itp_wide <- cbind(itp_wide,
                  math_grade_mean=mathGr$math_grade,
                  anxiety_mean=anx$anxiety) # joining mean values to the wide-form dataset
itp_wide # showing the wide-form dataset

# 6. predict math_grade by anxiety on both datasets, inspect and interpret coefficients
fit_long <- lm(math_grade ~ anxiety, data=itp) # long-form dataset (N = 90)
summary(fit_long)$coefficients 
# when student anxiety = 0, student math_grade is 17.7 (out of range*), which is significantly higher than zero
# when student anxiety increases by 1 unit, student math_grade decreases by 3.11 units, which is statistically significant
fit_wide <- lm(math_grade_mean ~ anxiety_mean, data=itp_wide) # wide-form dataset (N = 4)
summary(fit_wide)$coefficients
# when class anxiety = 0, class math-grade is 63.44 (out of range*), which is significantly higher than zero
# when class anxiety increases by 1 unit, class math-grade decreases by 17.76, which is statistically significant

# *intercepts are out of range because predictors are not centered
# that is, since intercepts are estimated for anxiety = 0 (which is not possible), the model returns an impossible (i.e., out-of-range) value

# 7. which model has the highest standard errors? Why?
# the model fitted on the wide-form dataset (N = 4) shows the highest standard error (i.e., parameters are estimated with a poor precision) because the model uses a lower sample size (N = 4) compared to that used by the other model (N = 90)
```

## 5. Data centering

\fontsize{7.5pt}{12}\selectfont

Consider the long- and wide-form datasets from \color{blue}[exercise #4](#ex4)\color{black}:

1. Compute the **grand-mean-centered** `anxiety` values from the wide-form dataset

2. Fit a simple linear model predicting class-level `math_grade` by grand-mean-centered `anxiety` using the wide-form dataset. Inspect and interpret the estimated coefficients, and compare them with those estimated in the previous exercise

3. Use the `join()` function from the `plyr` package to join the cluster-level mean `anxiety` values to the long-form dataset

4. Compute the **cluster-mean-centered** `anxiety` values by subtracting mean class `anxiety` from student-level `anxiety`

5. Considering class `A`, how many students have an `anxiety` level below the class average? How many have a higher value than the average?

6. Fit a simple linear model predicting student-level `math_grade` by cluster-mean-centered `anxiety` values using the long-form dataset. Inspect and intepret the estimated coefficients, and compare them with those estimated in the previous exercise

```{r echo=FALSE,eval=FALSE}
# SOLUTION TO EXERCISE #5

# 1. grand-mean-centered anxiety from the wide-form dataset (which I called "itp_wide" in exercise #4)
itp_wide$anxiety.gmc <- itp_wide$anxiety_mean - mean(itp_wide$anxiety_mean)
itp_wide[,c("anxiety_mean","anxiety.gmc")] # showing uncentered and centered values

# 2. linear model predicting math_grade by grand-mean-centered anxiety in the wide-form dataset
fit_wide.cmc <- lm(math_grade_mean ~ anxiety.gmc, data = itp_wide)
summary(fit_wide)$coefficients # coefficients from the uncentered model (which I called "fit_wide" in ex #4)
summary(fit_wide.cmc)$coefficients # coefficients from the centered model
# intercept changed from 63.4 to 8.13 (now it's on range!), but still significantly higher than zero
# the slope is basically the same, still significantly lower than zero

# 3. join cluster-level mean anxiety to the long-form dataset
library(plyr)
itp <- join(itp, # selecting long-form dataset
            itp_wide[,c("classID","anxiety_mean")], # selecting the columns of interest from the wide-form dataset
            by="classID") # joining by classID
itp[,c("anxiety","anxiety_mean")] # showing individual-level and cluster-level anxiety

# 4. cluster-mean-centered anxiety from the long-form dataset
itp$anxiety.cmc <- itp$anxiety - itp$anxiety_mean
itp[,c("anxiety","anxiety_mean","anxiety.cmc")] # showing ind-level, clust-level, and clust-mean-centered anxiety

# 5. Focusing on class A, how many student anxiety values below/above the class average?
classA <- itp[itp$classID=="A",] # subsetting data from class A
nrow(classA[classA$anxiety < mean(classA$anxiety),]) # 16 with math_grade below the class average
nrow(classA[classA$anxiety.cmc < 0,]) # alternative way based on cluster-mean-centered values (i.e., average = 0)
nrow(classA[classA$anxiety > mean(classA$anxiety),]) # 14 with math_grade above the class average
nrow(classA[classA$anxiety.cmc > 0,]) # alternative way based on cluster-mean-centered values (i.e., average = 0)

# 6. linear model predicting math_grade by cluster-mean-centered `anxiety` values
fit_long.cmc <- lm(math_grade ~ anxiety.cmc, data = itp) 
summary(fit_long)$coefficients # coefficients from the uncentered model (which I called "fit_long" in ex #4)
summary(fit_long.cmc)$coefficients # coefficients from the centered model
# intercept changed from 17.75 to 8.05 (now it's on range!), but still significantly higher than zero
# the slope changed from -3.11 to -2.20, still significantly lower than zero
```

## 6. Data centering & level-specific correlations {#ex6}

\fontsize{7.5pt}{12}\selectfont
Do left- and right-side infant pupil sizes correlate more at the within-subject or at the between-subject level?

1. Download and read the "*Infant pupil*" dataset `infantPupil.csv`

2. Subset columns 15, 10, 11, 12, and 13, and rename them as `ID` (subject identification code), `pupil.left` (left-side pupil size in mm), `pupil.left_valid` (validity of left-sized pupil size measurement), `pupil.right` (right-side pupil size in mm), and `pupil.right_valid` (validity of the right-side pupil size measurement)

3. How many valid cases for each eye? (note: 1 = valid, 0 = invalid)

4. Remove all cases with invalid pupil size in either one or the other eye 

5. Compute the cluster means and the cluster-mean-centered values for `pupil.left` and `pupil.right` 

6. Compute the between-subject and the within-subject correlations between the two variables: Do left- and right-side infant pupil sizes correlate more at the within-subject or at the between-subject level?

```{r echo=FALSE,eval=FALSE}
# SOLUTION TO EXERCISE #6

rm(list=ls()) # emptying work environment

# 1. reading infant pupil dataset (note: I saved the file in the 'data' subfolder in my working dir)
pupil <- read.csv2("data/infantPupil.csv") # use read.csv2 for columns separated by ";" (csv2) rather than "," (csv)

# 2. subsetting and renaming columns
pupil <- pupil[,c(15,10:13)]
colnames(pupil) <- c("ID","pupil.left","pupil.left_valid","pupil.right","pupil.right_valid")
summary(as.factor(pupil$pupil.left_valid))

# 3. How many valid cases?
table(pupil$pupil.left_valid) # 16,379 valid left
100*table(pupil$pupil.left_valid)/nrow(pupil) # percentage (65%)
table(pupil$pupil.right_valid) # 15,337 valid right
100*table(pupil$pupil.right_valid)/nrow(pupil) # percentage (60.5%)

# 4. removing invalid cases in either eye
pupil <- pupil[pupil$pupil.right_valid==1 & pupil$pupil.left_valid==1,]

# 5. cluster means and cluster mean centered
wide <- aggregate(pupil.left ~ ID, data=pupil, FUN=mean) # cluster mean of pupil.left
# I got Warning: argument is not numeric or logical (i.e., pupile sizes are considered as characters)
pupil$pupil.left <- as.numeric(pupil$pupil.left) # converting to numeric
pupil$pupil.right <- as.numeric(pupil$pupil.right)
wide <- aggregate(pupil.left ~ ID, data=pupil, FUN=mean) # now it works
wide # let's take a look
pupil <- plyr::join(pupil,wide,by="ID") # joining cluster means to long-form dataset
colnames(pupil)[6] <- "pupil.left.cm" # renaming variable to avoid duplicated column names
pupil$pupil.left.cmc <- pupil$pupil.left - pupil$pupil.left.cm # cluster mean centering

# same operations for pupil.right
wide <- aggregate(pupil.right ~ ID, data=pupil, FUN=mean) 
pupil <- plyr::join(pupil,wide,by="ID")
colnames(pupil)[8] <- "pupil.right.cm"
pupil$pupil.right.cmc <- pupil$pupil.right - pupil$pupil.right.cm 

# 6. level-specific correlations
pupil_wide <- pupil[!duplicated(pupil$ID),] # taking only one row per subject
# between-subject correlation (between cluster means from the wide-form dataset)
cor(pupil_wide$pupil.left.cm, # r = 0.987
    pupil_wide$pupil.right.cm) 
# within-subject correlation (between cluster-mean-centered values from the long-form dataset)
cor(pupil$pupil.left.cmc, # r = 0.865
    pupil$pupil.right.cmc)

# 7. Do left- and right-side infant pupil sizes correlate more at the within-subject or at the between-subject level?
# they seem to correlate more at the between-subject level!
```

## 7. Intraclass correlation coefficient

\fontsize{8.5pt}{12}\selectfont
Using data from \color{blue}[exercise #6](#ex6)\color{black}, compute the intraclass correlation coefficient (ICC) for both pupil size measures.

1. Do they variate more at the within-subject (lv1) or at the between-subject (lv2) level?

2. What is the percentage of within-subject variability over the total variability?

3. Does one eye variate more within-subject than the other?

```{r echo=FALSE,eval=FALSE}
# SOLUTION TO EXERCISE #7

# first, you can run all the commands used in exercise #6

# computing left-side pupil size ICC from null LMER model
library(lme4) # package for fitting LMER models
m0.left <- lmer(pupil.left ~ (1|ID), data=pupil) # null LMER model
tau2.left <- summary(m0.left)$varcor$ID[[1]] # extracting the random intercept variance tau2
sigma2.left <- summary(m0.left)$sigma^2 # extracting the residual variance sigma2
ICC.left <- tau2.left/(tau2.left + sigma2.left) # ICC = var between / total var = tau2 / (tau2 + sigma2)
ICC.left # 0.60

# computing ICC for right-side pupil size
m0.right <- lmer(pupil.right ~ (1|ID), data=pupil) # null LMER model
tau2.right <- summary(m0.right)$varcor$ID[[1]] # tau2
sigma2.right <- summary(m0.right)$sigma^2 # sigma2
ICC.right <- tau2.right/(tau2.right + sigma2.right) # ICC
ICC.right # 0.64

# 1. Do they variate more at the within-subject (lv1) or at the between-subject (lv2) level?
# ICC range from .60 (i.e., 60% of variance at the between level) to .64 (i.e., 64% of variance at the between level), so they variate more at the between-subject level (lv2) than at the within-subject level (lv1)

# 2. What is the percentage of within-subject variability over the total variability?
# we simply need to compute 1 - ICC
1 - ICC.left # left side: 39% of within-subject variability
1 - ICC.right # right side: 36% of within-subject variability

# 3. Does one eye variate more within-subject than the other?
# yes, left-side pupil size variates within-subject variability is 3% higher than right-side pupil size
# however, we don't know whether this difference might be substantial or not
```

## 8. Model fit and coefficeint interpretation {#ex8}

\fontsize{8.5pt}{12}\selectfont

1. Download and read the “*Innovative teaching program*” dataset `studentData.csv` (Moodle/Github, "data" folder)

2. Cluster mean center the variable `anxiety` so that we can focus the related slope at the within-individual level

3. Fit a null LMER model `m0` predicting `math_grade`

4. Compute and interpret the ICC

5. Fit a model `m1` with `math_grade` being predicted by `anxiety.cmc`

6. Fit a model `m2` including a random slope for `anxiety.cmc`

7. Fit a model `m3` also including group differences based on `tp` (i.e., innovative teaching program: control vs. intervention) - note: the `tp` variable should be converted as a factor

8. Fit a model `m4` also including the interaction between `anxiety.cmc` and `tp`

9. Inspect and interpret the `summary()` of models `m3` and `m4`

10. What can we say from model `m4`? Does the innovative teaching program improve math achievement? What is the role of anxiety?

```{r echo=FALSE,eval=FALSE}
# SOLUTION TO EXERCISE #8

# 1. download and read the studentData.csv dataset
rm(list=ls()) # emptying the work environment
getwd() # getting where your working directory is --> copy the file in there
itp <- read.csv("studentData.csv")

# 2. cluster mean center the variable `anxiety`
head(itp) # remember: itp is the long-form dataset with individual observations (students with their math_grade and anxiety scores) are nested within clusters (classID)
wide <- aggregate(anxiety ~ classID, data=itp, FUN=mean) # cluster means
itp <- plyr::join(itp,wide,by="classID") # joining cluster means to the long-form dataset
colnames(itp)[6] <- "anxiety.cm" # renaming variable to avoid double column names
itp$anxiety.cmc <- itp$anxiety - itp$anxiety.cm # cluster mean centering: subtracting the cluster mean from original values

# now we have anxiety (i.e., each student anxiety level), anxiety.cm (i.e., mean anxiety level for each class, identically repeating over all rows from the same class), and anxiety.cmc (i.e., cluster-mean-centered anxiety, where positive values indicate a student with a higher anxiety level than the average of his/her class, whereas negative values indicate students with lower anxiety levels than their class)

# 3. Fit a null LMER model `m0` predicting `math_grade`
library(lme4) # opening lme4 package
m0 <- lmer(math_grade ~ (1|classID), data = itp)
# note: model m0 includes the fixed intercept, the random intercept, and the residuals

# 4. Compute and interpret the ICC
tau2 <- summary(m0)$varcor$classID[[1]] # tau_00 squared = variance of the random intercept
sigma2 <- summary(m0)$sigma^2 # sigma squared = variance of the residuals
ICC <- tau2 / (tau2 + sigma2) # ICC = random intercept variance / total variance = random intercept variance / (random intercept variance + residual variance)
ICC # ICC = 0.25 --> the 25% of the variance in math_grade is at the between-cluster level (i.e., math_grade mainly varies within cluster than between clusters)

# 5. Fit a model `m1` with `math_grade` being predicted by `anxiety.cmc`
m1 <- lmer(math_grade ~ anxiety.cmc + (1|classID), data = itp)
 
# 6. Fit a model `m2` including a random slope for `anxiety.cmc`
m2 <- lmer(math_grade ~ anxiety.cmc + (anxiety.cmc|classID), data = itp)

# 7. Fit a model `m3` also including group differences based on `tp` (i.e., innovative teaching program: control vs. intervention) - note: the `tp` variable should be converted as a factor
summary(itp$tp) # itp is a character vector
itp$tp <- as.factor(itp$tp) # converting tp as a factor
summary(itp$tp) # now it is a factor
m3 <- lmer(math_grade ~ anxiety.cmc + tp + (anxiety.cmc|classID), data = itp)
# note: I got a singular fit (message saying "boundary (singular) fit: see help('isSingular')") - this means that the model didn't converge well (but we ignore this for now)
 
# 8. Fit a model `m4` also including the interaction between `anxiety.cmc` and `tp`
m4 <- lmer(math_grade ~ anxiety.cmc * tp + (anxiety.cmc|classID), data = itp)
m4 <- lmer(math_grade ~ anxiety.cmc + tp + anxiety.cmc:tp + (anxiety.cmc|classID), data = itp) # alternative way to write the same model

# 9. Inspect and interpret the `summary()` of models `m3` and `m4`
summary(m3)
# interpretation:
# - fixed intercept: the predicted ('mean') math_grade value when anxiety.cmc = 0 (i.e., average anxiety level within a given cluster) and tp = 0 (i.e., control group) is 7.81
# - fixed anxiety slope: considering both control and intervention, a 1-unit increase in anxiety.cmc (i.e., higher anxiety than the class mean) predicts a decrease in math_grade by -2.35 points
# note: we say "considering both control and intervention" because the interaction is not yet included - so the model makes a sort of average between the two groups
# - fixed tp slope: when anxiety.cmc = 0 (i.e., average anxiety levels), the intervention group is predicted to show an average math_grade of 0.62 points higher than the average math_grade in the control group (i.e., 7.81 + 0.62 = 8.43)

summary(m4)
# interpretation:
# - fixed intercept: the predicted ('mean') TST value when anxiety.cmc = 0 (i.e., average anxiety level) and tp = 0 (i.e., control group) is 7.85 minutes ---> same interpretation than in the additive model m3
# - fixed anxiety slope: in the control group, a 1-unit increase in anxiety.cmc (i.e., higher anxiety than the class average) predicts a decrease in math_grade of -7.187 points
# note: we say "in the control group" because the interaction is included in the model - so the model focuses this coefficient in the control group and quantifies the difference between the two groups in the interactive term below
# - fixed tp slope: when anxiety.cmc = 0 (i.e., average anxiety level), the intervention group is predicted to show an average math_grade of 0.53 points higher than the average math_grade in the control group (i.e., 7.85 + 0.53 = 8.38) ---> same interpretation than in the additive model m3
# - interaction (first interpretation): the relationship between anxiety.cmc and math_grade in the intervention group is 2.91 point-per-anxiety lower than in the control group (i.e., -1.48 + (-2.92) = -4.40); in other words, a 1-unit increase in axiety.cmc (higher anxiety than the class mean) predicts a math_grade reduction of -1.48 in the control group and a decrease of -4.40 minutes in the intervention group
# - interaction (second interpretation): the math_grade difference between intervention and controls is 2.91 points smaller when anxiety.cmc increases by 1 unit (i.e., higher anxiety than the class mean) compared to when anxiety.cmc = 0 (i.e., average anxiety level); in other words, anxiety reduces the differences between the two groups, with students in the intervention groups getting slightly less improved scores than those in the control group

# 10. What can we say from model `m4`?
# Based on model m4, we can say that the innovative teaching program seems to increase math achievement (i.e., the t-value is 2.599, which is higher than 1.96)
# Students with higher anxiety than the mean of their class seem to have a reduced beneficial effect of the innovative teaching program (interaction = -2.91), but such a reduction in the estimated effect is not 'substantial' (i.e., the t-value is -0.829, which is higher than -1.96)
# So let's do the innovative teaching program in all classes! :)
```

## 9. Fixed effect visualization & effect plots

\fontsize{8.5pt}{12}\selectfont
Using models `m1`, `m2`, and `m3` from \color{blue}[exercise #8](#ex8)\color{black}:

1. Visualize and interpret the **forest plot of the fixed effects** by using the `pot_model()` function from the `sjPlot` package

2. Compute the 95% confidence intervals visualized in the plots that you have just generated

3. Visualize and interpret the **fixed effect plots** by adding the argument `type="pred"` within the `plot_model()` function

4. Which are the parameters of model `m1`? Which are those of model `m2` and model `m3`?

```{r echo=FALSE,eval=FALSE}
# SOLUTION TO EXERCISE #9

# note: first run the code to solve exercise #8

# 1. forest plot of the fixed effects by using the `pot_model()`
library(sjPlot) # if not yet installed, first run install.packages("sjPlot")
plot_model(m1) # here, we only have the coefficient estimated for anxiety.cmc, which is negative and does not include zero, so we can say that math_grade is 'significantly' lower in students with an anxiety level lower than the average anxiety level of their class
plot_model(m2) # we can see a very similar situation: the fixed effect of anxiety is still negative and significantly lower than zero, even now that the model includes the corresponding random slope (i.e., cluster-specific variability in the slope estimate does not substantially impact the fixed slope)
plot_model(m3) # now we have 2 fixed effects: anxiety.cmc - which is still negative but no longer significantly lower than zero - and tp - which is positive and show 95% confidence intervals excluding zero, so we can say that the difference between groups is significantly higher than zero

# 2. Compute the 95% confidence intervals visualized in the plots that you have just generated

# model m1
(fixeffs <- summary(m1)$coefficients) # extracting fixed effects estimates and standard error
(anx.slope <- fixeffs[2,1]) # fixed slope for anxiety.cmc
(anx.slope_se <- fixeffs[2,2]) # standard error of the fixed slope for anxiety
anx.slope - 1.96 * anx.slope_se # lower CI = -4.21
anx.slope + 1.96 * anx.slope_se # upper CI = -4.19

# model m2
(fixeffs <- summary(m2)$coefficients) # extracting fixed effects estimates and standard error
(anx.slope <- fixeffs[2,1]) # fixed slope for anxiety.cmc
(anx.slope_se <- fixeffs[2,2]) # standard error of the fixed slope for anxiety
anx.slope - 1.96 * anx.slope_se # lower CI = -6.51
anx.slope + 1.96 * anx.slope_se # upper CI = -0.24

# model m3
(fixeffs <- summary(m3)$coefficients) # extracting fixed effects estimates and standard error
(anx.slope <- fixeffs[2,1]) # fixed slope for anxiety.cmc
(anx.slope_se <- fixeffs[2,2]) # standard error of the fixed slope for anxiety
anx.slope - 1.96 * anx.slope_se # lower CI = -5.746
anx.slope + 1.96 * anx.slope_se # upper CI = 1.03
(tp.slope <- fixeffs[3,1]) # fixed slope for anxiety.cmc
(tp.slope_se <- fixeffs[3,2]) # standard error of the fixed slope for anxiety
tp.slope - 1.96 * tp.slope_se # lower CI = 0.287
tp.slope + 1.96 * tp.slope_se # upper CI = 0.959


# 3. Visualize and interpret the **fixed effect plots** by adding the argument `type="pred"` within the `plot_model()` function

# m1
plot_model(m1,type="pred") # the plot shows the regression line estimated for the relationship between anxiety.cmc and math_grade, showing that the latter is predicted to decrease when the former increases
# what are the bands surrounding the line? they are the 95% confidence intervals, which are narrower in the center (i.e., more data points when anxiety.cmc is zero --> higher precision in the estimation) and wider in the tails (i.e., less data points when anxiety is highly higher or lower than the cluster mean --> lower precision in the estimation)
# is the relationship significant? Yes because the math_grade value corresponding to the minimum anxiety.cmc value is not included in the intervals represented for the math_grade value corresponding to the maximum anxiety.cmc value, and viceversa

# m2
plot_model(m2,type="pred") # the plot is basically equivalent to the previous one but confidence intervals are overall narrower, probably due to the inclusion of the random slope that reduced the standard error associated with the slope for anxiety.cmc

# m3
plot_model(m3,type="pred")
# the first plot is similar to the previous ones, but we can see that the inclusion of tp as an additional model predictor led to an increase of the uncertainty in parameter estimates (or a decrease in its precision), that is standard errors have became higher. This mainly affects the upper tail of the distribution, where confidence intervals become so large that they include the math_grade value corresponding to the minimum value of anxiety.cmc --> i.e., the relationship is no longer significant!
# the second plot shows the predicted math_grade values in the two groups along with their confidence intervals. We can see that the predicted math_grade for the control group (i.e., the red dot on the left) is not included in the confidence intervals estimated for the intervention group (i.e., the dotted line on the right), and viceversa. So we can say that the two groups significantly differ in math_grade, or that the intervention group has a significantly higher math_grade than the control group
 
# 4. Which are the parameters of model `m1`? Which are those of model `m2` and model `m3`?
# model m1: fixed intercept, fixed slope for anxiety.cmc, variance of the random intercept, residual variance (i.e., 4 parameters)
# model m2: fixed intercept, fixed slope for anxiety.cmc, variance of the random intercept, variance of the random slope, covariance between random intercept and random slope, residual variance (i.e., 6 parameters)
# model m3: fixed intercept, fixed slope for anxiety.cmc, fixed slope for tp, variance of the random intercept, variance of the random slope, covariance between random intercept and random slope, residual variance (i.e., 7 parameters)
```

# LMER recap

## Summary exercises

\fontsize{8.5pt}{12}\selectfont
Exercises #10 and #11 summarize almost all contents that we have seen in Part 1. They are related to the same case study but the former includes some questions similar to those that you might find in the exam, whereas the latter requires you to analyze a multilevel dataset in R.

\fontsize{7.5pt}{12}\selectfont
Note: The reason for which I'm doing most exercises with R is because I find it the most effective way to consolidate the course contents: by concretely analyzing the data, you should better understand the theoretical stuff we see during lectures. However, it is understandable that you also want to get how the exam will be.

## 10. Exam-like exercise: Internet abuse (1/16)

\fontsize{7pt}{12}\selectfont 

\color{blue}

A research is conducted within a project on internet abuse prevention in primary-school children. Data were collected before and after a four-week intervention aiming at (1) reducing children **internet use** (IU), (2) improving the effectiveness of the **parental control** in reducing IU, controlling for children **sex**. Data collection involved 416 children from 14 classes within 7 schools (i.e., two classes per school; one that undertook the intervention and the other that did not). 

\color{black}

1. __What is the problem in analyzing such data with linear models (LM)?__

\fontsize{6pt}{12}\selectfont

A) There is *no problem* in analyzing such data with LM 
B) The local dependencies due to the nested data structure violate the LM assumption of *independence between observations*, possibly biasing the *standard errors*
C) The local dependencies due to the nested data structure violate the LM assumption of *homoscedasticity*, possibly biasing the *t-values*
D) The local dependencies due to the nested data structure violate the LM assumption of *independence between errors and predictors*, possibly biasing the *parameter estimates*

```{r echo=FALSE,eval=FALSE}
# right answer: B
```

\fontsize{5pt}{12}\selectfont
Source: Adapted from Pastore (2021). Analisi dei dati in contesti di comunità

## 10. Exam-like exercise: Internet abuse (2/16)

\fontsize{7pt}{12}\selectfont

2. __Why is this a nested data structure?__

\fontsize{6pt}{12}\selectfont

A) Because *schools* are nested *within* children
B) Because *children* are nested *between* schools
C) Because the *correlations* among children within the *same school* might be stronger than those between children from *different schools*
D) Because the *correlations* between schools can bias the estimation of the *standard errors*

```{r echo=FALSE,eval=FALSE}
# right answer: C
## A is wrong because it is children that are nested within schools
## B is wrong because children are nested *within* schools, not between
## C is right and reports the definition of 'local dependency'
## D is wrong because it is the correlations *within* schools that can bias the estimation of the standard errors
``` 

\fontsize{7pt}{12}\selectfont

3. __Which is the dependent variable?__

\fontsize{6pt}{12}\selectfont

A) Internet use
B) Children
C) Parental control
D) Intervention

```{r echo=FALSE,eval=FALSE}
# right answer: A 
## i.e., the study description says that the intervention aims at reducing internet use
``` 

## 10. Exam-like exercise: Internet abuse (3/16)

\fontsize{7pt}{12}\selectfont

4. __Which is the cluster variable?__

\fontsize{6pt}{12}\selectfont

A) Children
B) Classes
C) Schools
D) Both classes and schools can be considered cluster variables

```{r echo=FALSE,eval=FALSE}
# right answer: D
## i.e., both classes and schools can indeed produce local dependencies in children scores
```

\fontsize{7pt}{12}\selectfont

5. __How many clusters and individual observations?__

\fontsize{6pt}{12}\selectfont

A) 7 clusters, 416 individual observations
B) 7 clusters, 14 individual observations
C) 416 clusters, 7 individual observations
D) 416 clusters, 14 individual observations

```{r echo=FALSE,eval=FALSE}
# right answer: A
## i.e., although both schools (N=7) and classes (N=14) can be seen as clusters, the only options with a compatible number of clusters (7 or 14) are option (A) and (B), whereas the only option with the right number of individual observation is option (A)
```

## 10. Exam-like exercise: Internet abuse (4/16)

\fontsize{7pt}{12}\selectfont

6. __Which variables are at the individual-observation level (level 1)?__

\fontsize{6pt}{12}\selectfont

A) All variables are at the individual-observation level
B) No variables are at the individual-observation level
C) Internet use, intervention, and sex
D) Internet use, parental control, and sex

```{r echo=FALSE,eval=FALSE}
# right answer: D
## i.e., the intervention is implemented at the class level (level 2)
## level-1 variables are all variables that variate across children within classes
```

\fontsize{7pt}{12}\selectfont

7. __Which variables are at the cluster level (level 2)?__

\fontsize{6pt}{12}\selectfont

A) All variables are at the cluster level
B) No variables are at the cluster level
C) Intervention
D) Internet use, parental control, and sex

```{r echo=FALSE,eval=FALSE}
# right answer: B
## None of the variable is at the school level
## note: intervention is at the class level (it says "that one class undertook the intervention and one did not", within each school)
```

## 10. Exam-like exercise: Internet abuse (5/16)

\fontsize{7pt}{12}\selectfont

Here are the first 4 rows of the dataset. `ID` = children identifier, `sex` = children sex ("f" or "m"), `phase` = intervention phase ("pre" or "post"), `IU` = internet use, `CG` = parental control, `school` = school identifier.
```{r echo=FALSE}
# reading data
rm(list=ls()) # emptying the workspace
load("data/attiva.RData") # loading data
```
```{r }
head(attiva,4)
```

8. __What kind of dataset are we looking at?__

\fontsize{6pt}{12}\selectfont

A) This is the wide-form dataset, with one row per class
B) This is the long-form dataset, with one row per children 
C) This is the wide-form dataset, with multiple rows per class
D) This is the long-form dataset, with one row per school

```{r echo=FALSE,eval=FALSE}
# right answer: B
## option A is wrong because this is not the wide-form dataset and we have multiple rows per class
## option B is right because this is the long-form dataset and we have one row per children
## option C is wrong because this is not the wide-form dataset, although we have multiple row per class
## option D is wrong because this dataset has multiple rows per school, although it is called 'long-form' dataset
```

## 10. Exam-like exercise: Internet abuse (6/16)

```{r echo=FALSE,results='hide',warning=FALSE,message=FALSE}
# cluster mean centering and level-specific correlations
wide <- aggregate(attiva$IU, by=list(attiva$school), FUN=mean) # computing mean IU by school (cluster means)
colnames(wide) <- c("school","IU.cm") # renaming variables
wide$CG.cm <- aggregate(attiva$CG, by=list(attiva$school), FUN=mean)[,2] # adding cluster means of CG (note: I'm taking the second column [,2] from the output of the aggregate function) 
wide # showing wide-form dataset
library(plyr)
attiva <- join(attiva,wide,by="school") # joining cluster means to long-form dataset
attiva$IU.cmc <- attiva$IU - attiva$IU.cm # cluster mean centering IU
attiva$CG.cmc <- attiva$CG - attiva$CG.cm # cluster mean centering CG

# computing level-specific correlations
rb <- cor(wide$IU.cm,wide$CG.cm) # cor between (-0.10)
rw <- cor(attiva$IU.cmc,attiva$IU.cm) # cor within (0.00000000 ... 5) - basically zero
```

\fontsize{7pt}{12}\selectfont

9. __The between-school correlation between internet use and parental control is `r round(rb,2)` while the within-school correlation is `r round(rw,2)`. What does that mean?__

\fontsize{6pt}{12}\selectfont

A) A negative (weak) correlation is estimated between school means of internet use and parental control
B) A negative (weak) correlation is estimated between children internet use and parental control within the same school
C) Neither A nor B are true
D) Both A and B are true

```{r echo=FALSE,eval=FALSE}
# right answer: A
## option B is wrong because that is the definition of the within-level correlation, which is zero
## option C is wrong because option A is true (i.e., r = -0.10 is a weak negative correlation but it is not exactly zero)
## option D is wrong because option C is false
```

\fontsize{7pt}{12}\selectfont

10. __How can we compute within-school (level-1) correlations?__

A) We correlate the raw children scores from the long-form dataset
B) We correlate the school mean scores from the wide-form dataset
C) We correlate the cluster-mean-centered scores from the long-form dataset
D) We correlate the grand-mean-centered scores from the wide-form dataset

```{r echo=FALSE,eval=FALSE}
# right answer: C
## option A is wrong because we need to grand-mean-center the variables before correlating them
## option B is wrong because that is how we compute level-2 correlations
## option D is wrong because that is another way to compute level-2 correlations
```

## 10. Exam-like exercise: Internet abuse (7/16)

```{r echo=FALSE,results='hide'}
# null lmer model
library(lme4)
fit <- lmer(IU ~ (1|school), data=attiva) # fitting null LMER model

# extracting tau2_00 and sigma2
tau2 <- summary(fit)$varcor$school[[1]] # extracting the variance of the random intercept
sigma2 <- summary(fit)$sigma^2 # extracting the residual variance

# computing ICC
icc.school <- tau2 / (tau2 + sigma2) # computing ICC
```

\fontsize{7pt}{12}\selectfont

11. __The intraclass correlation coefficeint of internet use (IU) is `r round(icc.school,2)`. What does that mean?__

\fontsize{6pt}{12}\selectfont

A) IU mainly variates between schools (95%) than within school (5%)
B) IU mainly variates within school (95%) than between schools (5%)
C) IU equally variates between and within school (50%)
D) None of the previous options is correct

```{r echo=FALSE,eval=FALSE}
# right answer: B
## option A is wrong because the ICC tell us the proportion of between-cluster variability over the total variability --> an ICC of 0.05 means that the 5% of the variability is at the between-cluster level
## option C is wrong because the ICC is different than 0.50
## option D is wrong because option B is correct
```

\fontsize{7pt}{12}\selectfont

12. __How can we estimate the ICC of a variable?__

A) We divide the estimate of its between-cluster variability by its estimated total variability
B) We subtract the estimate of its between-cluster variability from its estimated total variability
C) We divide the estimate of its within-cluster variability by its estimated total variability
D) We subtract the estimate of its within-cluster variability from its estimated total variability

```{r echo=FALSE,eval=FALSE}
# right answer: A
## option A is correct because ICC = tau2_00 (variance of the random intercept, estimate of the between-cluster variability) / (tau2_00 + sigma2) (estimate of the total variability)
## option B is wrong because the former is *divided* by the latter, not *subtracted*
## option C is wrong because tau2_00 is the estimate of the *between*-cluster variability, not *within*
## option D is wrong because of the same reasons reported for option B and C
```

## 10. Exam-like exercise: Internet abuse (8/16)

\fontsize{7pt}{12}\selectfont

13. __Which of the following models is in line with the research goals? (i.e., evaluating intervention reducing internet use (IU) and improving the effectiveness of parental control, accounting for sex)__

\fontsize{6pt}{12}\selectfont

A) $IU_{ij} = \beta_{1}Intervention + \beta_{2j}Control + \beta_3Intervention\times Control + \beta_{4j}Sex$
B) $IU_{ij} = \beta_{0j} + \beta_{1}Intervention + \beta_{2j}Control + \beta_{3j}Sex + \epsilon_{ij}$
C) $Control_{ij} = \beta_{0j} + \beta_{1}Intervention + \beta_{2j}IU + \beta_3Intervention\times IU + \beta_{4j}Sex + \epsilon_{ij}$ 
D) $IU_{ij} = \beta_{0j} + \beta_{1}Intervention + \beta_{2j}Control + \beta_3Intervention\times Control + \beta_{4j}Sex + \epsilon_{ij}$ 

```{r echo=FALSE,eval=FALSE}
# right answer: D
## A is wrong because it includes neither the intercept nor the error term
## B is wrong because it does not include the interaction between intervention and control (i.e., second aim of improving the effectiveness of parental control)
## C is wrong because the outcome variable is Control rather than IU
## D is correct: we have the intercept and the errors, we have both the main effect of Intervention (goal 1) and its interaction with parental control (goal 2), and we have sex as a covariate
```

\fontsize{7pt}{12}\selectfont

14. __How many fixed coefficients are estimated by that model?__

\fontsize{6pt}{12}\selectfont

A) 4: 1 intercept, 3 slopes (intervention, parental control, and sex)
B) 5: 1 intercept, 4 slopes (intervention, parental control, their interaction, and sex)
C) 6: 1 fixed intercept, 1 variance of the random intercept, 4 slopes (intervention, parental control, their interaction, and sex)
D) 7: 1 fixed intercept, 1 variance of the random intercept, 4 slopes (intervention, parental control, their interaction, and sex), 1 residual variance

```{r echo=FALSE,eval=FALSE}
# right answer: B
## A is wrong because it does not include the interaction between parental control and intervention
## C is wrong because the random intercept is not a fixed coefficients
## D is wrong because both the random intercept and the residual variances are not fixed coefficients
```

## 10. Exam-like exercise: Internet abuse (9/16)

\fontsize{7pt}{12}\selectfont

Here are the results of a first model that only tests whether the intervention was able to reduce internet use, controlling for sex.
```{r echo=FALSE,warning=FALSE,message=FALSE, comment=NA, results=FALSE}
fit1 <- lmer(IU ~ phase + CG + sex + (CG|school), data=attiva)

# model summary from sjPlot
library(sjPlot); library(html2latex)
tab_model(fit1,show.se=TRUE, collapse.se=TRUE,string.est="b (SE)",show.r2=FALSE,dv.labels="",show.icc=FALSE,file = "temp1.html")

# from html to latex
html2pdf(filename = "temp1.html",table_width = 13, 
         silent = TRUE, style = TRUE, build_pdf = TRUE, clean = TRUE, name_table = "table1")

# from latex to Rmarkdown
tex2Rmd("temp1.tex",output_file="table1.txt")
```

\fontsize{6pt}{12}\selectfont
`r readr::read_file("table1.txt")`

\fontsize{5pt}{12}\selectfont
Note: `sjPlot` calls random effect variances $\tau$ rather than $\tau^2$

## 10. Exam-like exercise: Internet abuse (10/16)

\fontsize{7pt}{12}\selectfont
Considering the table shown in the previous slide:

15. __Was the intervention (variable `phase`) effective in reducing internet use?__

\fontsize{6pt}{12}\selectfont

A) No, because the coefficient $\beta$ = 1.96 is positive
B) Yes, because the coefficient $\beta$ = 1.96 is not lower than *t* = 1.96
C) No, because the coefficient $\beta$ = -0.98 is lower than *t* = -1.96
D) Yes, because the coefficient $\beta$ = -0.98 has a *p*-value lower than 0.05

```{r echo=FALSE,eval=FALSE}
# right answer: D
## option A is wrong because it is referred to the coefficient estimated for CG rather than that estimated for phase
## option B is wrong for the same reason of option A and because it compares the coefficient estimate with the t-value, but they are not the same thing
## option C is wrong because it compares the coefficient estimate with the t-value, but they are not the same thing
## option D is correct because it is based on a recognized inferential criterion (i.e., the p-value), which indicates a significant effect (p < 0.05)
```

\fontsize{7pt}{12}\selectfont

16. __What random effects are reported in the table?__

\fontsize{6pt}{12}\selectfont

A) Residual variance, random intercept (RI), and random slope (RS)
B) Residual variance, RI, RS, and correlation between RI and RS
C) Residual variance, RI, RS, and ICC
D) Residual variance, RI, RS, correlation between RI and RS, and ICC

```{r echo=FALSE,eval=FALSE}
# right answer: B
## option A is wrong because it does not include the correlation (rho) between RI and RS
## option C is wrong for the same reason, and because the ICC is not a random effect
## option D is wrong because ICC is not a random effect
```

## 10. Exam-like exercise: Internet abuse (11/16)

\fontsize{7pt}{12}\selectfont
Considering the table shown in the previous slide:

17. __How can we interpret the coefficient estimated for `phase`?__

\fontsize{6pt}{12}\selectfont

A) In the post-intervention group of children, the model predicts an IU value 0.98 points lower than in the pre-intervention group
B) For a one-unit increase in `phase`, the model predicts a decrease in IU by 0.98
C) In the post-intervention group of children, the model predicts an IU value 0.98 points higher than in the pre-intervention group
D) For a one-unit increase in `phase`, the model predicts an increase in IU by 0.98

```{r echo=FALSE,eval=FALSE}
# right answer: A
## option A and D are wrong because they interpret phase as it was a quantitative predictor, whereas it is a categorical predictor
## option C is wrong because the coefficient (-0.98) indicates lower IU in the post than in the pre-intervention group
```

\fontsize{7pt}{12}\selectfont

18. __If we trust the p-values, which fixed effects are significant?__

\fontsize{6pt}{12}\selectfont

A) Intercept, phase, and sex
B) Intercept, phase, and CG
C) Intercept, phase, CG, and random intercept
D) Intercept, phase, sex, and random intercept

```{r echo=FALSE,eval=FALSE}
# right answer: B
## option A and D is wrong because sex is not significant, whereas CG is significant
## option C and D are wrong because the random intercept is not a fixed effect
```

## 10. Exam-like exercise: Internet abuse (12/16)

\fontsize{7pt}{12}\selectfont

Here are the results of a second model that additionally tests whether the intervention was able to improve the effectiveness of the parental control in reducing internet use, controlling for sex.
```{r echo=FALSE,warning=FALSE,message=FALSE, comment=NA, results=FALSE}
fit2 <- lmer(IU ~ phase * CG + sex + (CG|school), data=attiva)

# model summary from sjPlot
library(sjPlot); library(html2latex)
tab_model(fit2,show.se=TRUE, collapse.se=TRUE,string.est="b (SE)",show.r2=FALSE,dv.labels="",show.icc=FALSE,file = "temp2.html")

# from html to latex
html2pdf(filename = "temp2.html",table_width = 13, 
         silent = TRUE, style = TRUE, build_pdf = TRUE, clean = TRUE, name_table = "table2")

# from latex to Rmarkdown
tex2Rmd("temp2.tex",output_file="table2.txt")
```

\fontsize{6pt}{12}\selectfont
`r readr::read_file("table2.txt")`

\fontsize{5pt}{12}\selectfont
Note: `sjPlot` calls random effect variances $\tau$ rather than $\tau^2$

## 10. Exam-like exercise: Internet abuse (13/16)

\fontsize{7pt}{12}\selectfont
Considering the table shown in the previous slide:

19. __What coefficient has been added in this scond model compared to the first one?__

\fontsize{6pt}{12}\selectfont

A) There is a new coefficient for the random slope for parental control
B) There is a new coefficient for the main effect of the intervention
C) There is a new coefficient for the interaction between intervention and parental control
D) The two models are equivalent

```{r echo=FALSE,eval=FALSE}
# right answer: C
## option A and D are wrong because both the random slopes for parental control and the main effect of the intervention were already included in the first model
## option D is wrong because the second model incudes the interaction, which was not included in the previous model
```

\fontsize{7pt}{12}\selectfont

20. __Considering the significance of the coefficients shared by both models, what has changed?__

\fontsize{6pt}{12}\selectfont

A) The coefficients estimated for phase and CG are no longer significant
B) The coefficient estimated for phase is no longer significant 
C) The coefficient estimated for CG is no longer significant
D) Nothing has changed in the significance of the shared coefficients

```{r echo=FALSE,eval=FALSE}
# right answer: B
## option A and C are wrong because the coefficient for CG is significant in both models
## option B is correct because the coefficient for phase is no longer significant (p > 0.05)
## option D is wrong because something has changed (i.e., phase)
```

## 10. Exam-like exercise: Internet abuse (14/16)

\fontsize{7pt}{12}\selectfont

Here are some plots showing the diagnostics of the second model.
```{r echo=FALSE,warning=FALSE,message=FALSE, comment=NA, results=FALSE,fig.width=12,fig.height=3}
par(mfrow=c(1,4))
qqnorm(residuals(fit),main="QQ plot residuals"); qqline(residuals(fit))
plot(residuals(fit) ~ fitted(fit),main="Residuals vs. fitted")
RS <- ranef(fit2)[[1]][,2]
qqnorm(RS,main="QQ plot random slope"); qqline(RS)
barplot(car::vif(fit2),ylim=c(0,10),main="Variance inflation factors"); abline(h=5)
```

\fontsize{7pt}{12}\selectfont

21. __Which LMER assumptions are evaluated by these plots?__

\fontsize{6pt}{12}\selectfont

A) Normality and linearity of residuals
B) Independence and homoscedasticity of residuals
C) Normality of random slope and absence of multicollinearity
D) All other options are true

```{r echo=FALSE,eval=FALSE}
# right answer: D
```

## 10. Exam-like exercise: Internet abuse (15/16)

\fontsize{7pt}{12}\selectfont

Here are some plots showing the diagnostics of the second model.
```{r echo=FALSE,warning=FALSE,message=FALSE, comment=NA, results=FALSE,fig.width=12,fig.height=3}
par(mfrow=c(1,4))
qqnorm(residuals(fit),main="QQ plot residuals"); qqline(residuals(fit))
plot(residuals(fit) ~ fitted(fit),main="Residuals vs. fitted")
RS <- ranef(fit2)[[1]][,2]
qqnorm(RS,main="QQ plot random slope"); qqline(RS)
barplot(car::vif(fit2),ylim=c(0,10),main="Variance inflation factors"); abline(h=5)
```

\fontsize{7pt}{12}\selectfont

22. __Which LMER assumptions are violated based on these plots?__

\fontsize{6pt}{12}\selectfont

A) Normality is violated for both residuals (slightly) and random slopes (more evident)
B) Homoscedasticity is violated for both residuals (more evident) and random slopes (slightly)
C) Normality is violated for both residuals (slightly) and random intercepts (more evident)
D) Normality of residuals (more evident) and absence of influential cases (slightly)

```{r echo=FALSE,eval=FALSE}
# right answer: A
## A is correct because residuals slightly deviate from normality (first plot) whereas the random slopes of the last two schools strongly deviate from normality
## B is wrong because homoscedasticity of random slopes is not plotted so we don't know whether this assumption has been violated or not; moreover the variability of the residuals does not seem to strongly increase or decrease over the range of fitted values (i.e., homoscedasticity seems ok)
## C is wrong because normality of the random intercept are not plotted so we don't know whether this assumption has been violated or not
## D is wrong because influential cases are not plotted so we don't know whether this assumption has been violated or not
```

## 10. Exam-like exercise: Internet abuse (16/16)

\fontsize{7pt}{12}\selectfont

Now we repeat the analysis described in the previous slides by using R. First, download and read the `attiva.RData` dataset (note `rda` files can be read similar to `RData` files).

```{r echo=FALSE,warning=FALSE,message=FALSE}
# likelihood ratio test
lrt <- anova(fit1,fit2)
chisq <- round(lrt$Chisq[2],2) # test statistic
df <- lrt$Df[2] # degrees of freedom
pval <- round(lrt$`Pr(>Chisq)`[2],2) # p-value

# AIC
aic <- AIC(fit1,fit2)
```

The likelihood ratio test and the Akaike Information Criterion (AIC) were used to compare the two models. The former resulted in a $\chi^2$ value of `r chisq` that with `r df` degrees of freedom is equivalent to a *p*-value of `r pval`. The latter resulted in an AIC value of `r round(aic[1,2],2)` for the first model and `r round(aic[2,2],2)` for the second model.

23. __What conclusion can be drawn from the likelihood ratio test results?__

\fontsize{6pt}{12}\selectfont

A) The first (simpler) model is better than the second (more complex) model
B) The second (more complex) model is better than the first (simpler) model
C) The two models are not different
D) None of the preceding options are true

```{r echo=FALSE,eval=FALSE}
# right answer: C
## A is wrong because the likelihood ratio test compare the second model with the first one, not vice versa
## B is wrong because the result is not significant (p > 0.05), meaning that the second model is not significantly better then the first one
## C is correct because saying that the second model is not better than the first one is equivalent to saying that the two models are not significantly different
## D is wrong because option C is true
```

\fontsize{7pt}{12}\selectfont

24. __What conclusion can be drawn from the AIC results?__

\fontsize{6pt}{12}\selectfont

A) The first (simpler) model is better than the second (more complex) model
B) The second (more complex) model is better than the first (simpler) model
C) The two models are not different
D) None of the preceding options are true

```{r echo=FALSE,eval=FALSE}
# right answer: A
## A is correct because the AIC of the first model is lower than the AIC of the second model (AIC: the lower the better)
## B is wrong because it says the opposite of what we found
## C is wrong because the two AIC values are not equal (the two models are not equivalent)
## D is wrong because option A is correct
```

## 11. R-based summary exercise

\fontsize{7pt}{12}\selectfont

Now we repeat the same steps described in the previous slides by using R. 

1. Download and read the `attiva.RData` dataset from Moodle or Github: `ID` = children identifier, `sex` = children sex ("f" or "m"), `phase` = intervention phase ("pre" or "post"), `IU` = internet use, `CG` = parental control, `school` = school identifier.

2. Explore the data (mean, SD, frequencies, and plots)

3. Compute the level-specific correlations between 

4. Create the variable `class` (identifying the class) by crossing `phase` and `school`. Then, compute the ICC of IU based on `school` and that based on `class`. What is the best cluster variable to be used?

5. Fit a random-intercept model `m1` including `phase`, `CG`, and `sex` to be compared with a second model `m2` that also include the random slope for `CG` using the likelihood ratio test and the AIC. Which is the best model?

6. Add the interaction between `phase` and `CG` to the model selected above and compare these two models with the likelihood ratio test and the AIC. Which is the best model?

7. Evaluate the diagnostics of the selected model

8. Print, visualize and interpret all the effects (fixed and random) estimated by the selected model

```{r echo=FALSE,results='hide'}
## SOLUTION EXERCISE #11

# 1. Download and read the dataset from Moodle or Github
rm(list=ls())
load("data/attiva.RData") # reading file (note: I save it in the 'data' folder within my working directory)

# 2. descriptive statistics and plots
hist(attiva$IU) # IU is not really symmatrical (positive skeweness)
mean(attiva$IU); sd(attiva$IU) # mean and sd IU
hist(attiva$CG) # CG is more symmetrically distributed (although it is on a clearly discrete scale)
mean(attiva$CG); sd(attiva$CG) # mean and sd CG
plot(attiva$sex) # barplot of sex (more males than females)
table(attiva$sex) # frequency of sexes 222 males, 190 females
plot(attiva$phase) # phase is quite balanced (similar number of observations)
table(attiva$phase) # frequency of phases it is actually perfectly balanced :)
plot(attiva$school) # a lot of participants from school D, less in school E
table(attiva$school) # frequency of schools

# 3. cluster mean centering and level-specific correlations
wide <- aggregate(attiva$IU, by=list(attiva$school), FUN=mean) # computing mean IU by school (cluster means)
colnames(wide) <- c("school","IU.cm") # renaming variables
wide$CG.cm <- aggregate(attiva$CG, by=list(attiva$school), FUN=mean)[,2] # adding cluster means of CG (note: I'm taking the second column [,2] from the output of the aggregate function) 
wide # showing wide-form dataset
library(plyr)
attiva <- join(attiva,wide,by="school") # joining cluster means to long-form dataset
attiva$IU.cmc <- attiva$IU - attiva$IU.cm # cluster mean centering IU
attiva$CG.cmc <- attiva$CG - attiva$CG.cm # cluster mean centering CG
(rb <- cor(wide$IU.cm,wide$CG.cm)) # cor between (-0.10)
(rw <- cor(attiva$IU.cmc,attiva$IU.cm)) # cor within (0.00000000 ... 5) - basically zero

# 4. crossing school and phase to create variable class and compute ICC
table(attiva$school,attiva$phase) # here we can see that each school has two groups of children (two classes) in the "pre" e "post" phase, respectively
attiva$class <- paste0(attiva$school,attiva$phase) # creating class by pasting school and phase
attiva$class <- as.factor(attiva$class) # class as a factor
plot(attiva$class)
table(attiva$class) # much more cases in the Dpost and Dpre classes
# ICC of UI based on school
library(lme4)
fit <- lmer(IU ~ (1|school), data=attiva) # fitting null LMER model
tau2 <- summary(fit)$varcor$school[[1]] # extracting the variance of the random intercept
sigma2 <- summary(fit)$sigma^2 # extracting the residual variance
icc.school <- tau2 / (tau2 + sigma2) # computing ICC
# ICC of UI based on school
fit <- lmer(IU ~ (1|class), data=attiva) 
tau2 <- summary(fit)$varcor$class[[1]] 
sigma2 <- summary(fit)$sigma^2 
icc.class <- tau2 / (tau2 + sigma2) 
# printing ICC values
icc.school # 0.05
icc.class # 0.05 --> they are basically equivalent, so we can use any of the two cluster variables

# 5. Fit a random-intercept model `m1` including `phase`, `CG`, and `sex` to be compared with a second model `m2` that also include the random slope for `CG` using the likelihood ratio test and the AIC. Which is the best model?
m1 <- lmer(IU ~ phase + CG + sex + (1|school), data=attiva)
m2 <- lmer(IU ~ phase + CG + sex + (CG|school), data=attiva)
library(lmtest)
lrtest(m1,m2) # m2 is not significantly better than m1 (the best model is m1 because it's simpler)
AIC(m1,m2) # model m2 is better because it has lower AIC
## note: the two criteria give different results. Here, we trust the AIC and we select the random slope model m2

# 6. Add the interaction between `phase` and `CG` to the model selected above and compare these two models with the likelihood ratio test and the AIC. Which is the best model?
m3 <- lmer(IU ~ phase + CG + sex + phase:CG + (CG|school), data=attiva) # adding interaction
# m3 <- lmer(IU ~ phase*CG + sex + (CG|school), data=attiva) # similar way to write the same model
lrtest(m2,m3) # m3 is not significantly better than m2 (the best model is m2 because it's simpler)
AIC(m2,m3) # model m2 is better because it has lower AIC
## note: the two criteria give consistent results, suggesting that m3 is not better than m2 (m2 is the best model)

# 7. Evaluate the diagnostics of the selected model
## normality and linearity of residuals
hist(residuals(m2)) # centered on zero, slightly asymmetrical
qqnorm(residuals(m2)); qqline(residuals(m2)) # some deviation from normality especially in the upper tail of the residual distribution (but overall OK)
## linearity and homoscedasticity of residuals
plot(m2) # no clear violation of linearity (no clear trend) or homoscedasticity (no different variance over fitted values) (OK)
# from long to wide: 1 row per subject
wide <- attiva[!duplicated(attiva$school),]
# extract random effects
RI <- ranef(m2)[[1]][,1] # r. intercept
RS <- ranef(m2)[[1]][,2] # r. slope
# normality and linearity of random effects
qqnorm(RI); qqline(RI) # random intercept: only one school strongly deviate from normality (KO)
qqnorm(RS); qqline(RS) # random slope: two schools in the upper tail substantially deviate from normality (KO)
## note: it makes no sense to plot random effects vs. predictors as all predictors variate within clusters
car::vif(m2) # absence of multicollinearity: no values with VIF > 5 (OK)
barplot(car::vif(m2)); abline(h=5)
boxplot(cooks.distance(m2)) # level-1 Cook's distances: we can see 2 extreme values (KO)
library(influence.ME)
plot(influence(m2, group="school"), which="cook") # level-2 Cook's distance: we can see that school D has an extreme value (KO)

# 8. Print, visualize and interpret all the effects (fixed and random) estimated by the selected model
# fixed effects
summary(m2)$coefficients
## Intercept: when phase = "pre", sex = "m", and CG = 0, the predicted value of IU is 9.45, which is 'substantially' higher than zero (t = 15.91)
## phasepost: when CG = 0, considering both sexes, "post" is 0.98 units lower than "pre", which is a 'substantial' negative difference (t = -2.41)
## CG: considering both sexes and intervention phases, a one-unit increase in CG predicts a 1.96-unit increase in IU, which is 'substantial' (t = 6.55)
## sexf: considering both sexes, when CG = 0, female internet use is 0.19 units higher than male internet use, which is not 'substantial' (t = 0.44)
summary(m2)$sigma^2 # residual variance (sigma2) - we don't interpret this
summary(m2)$varcor$school[[1]] # random intercept variance (tau2_00) - we don't interpret this
summary(m2)$varcor$school[[2]] # random slope variance (tau2_10) - we don't interpret this
## plotting fixed effects
library(sjPlot)
plot_model(m2) # we can see that phase effect is significantly lower than zero and CG effect is significantly higher than zero (in both cases, confidence intervals don't include zero), whereas sex differences are not significant (zero is included within the 95% CI)
plot_model(m2,type="pred") # effect plots are consistent with the forest plot, with the exception of phase: in the forest plot, 95% CI do not include zero (significant), whereas the effect plot show that each estimate is included in the 95% CI estimated for the other condition (not significant). Again, it appears that using p-value and 95% CI on the coefficients estimated by LM is not appropriate and can lead to inconsistencies
plot_model(m2,type="re") # variability in random intercept and random slope - we don't interpret this
```

# SEM

## 12. Variance, covariance, & correlation

\fontsize{8pt}{12}\selectfont

1. Simulate 100 values of a normally distributed variable `x1` with mean 10 and standard deviation 2 (note: use the `rnorm(n,mean,sd)` function where `n` is the sample size, `mean` is the simulated variable mean, and `sd` is the simulated variable standard deviation), then plot `x1`

2. Simulate 100 values of a second normally distributed variable `x2` with mean 10 and standard deviation 10, then plot `x2`

3. Try answering before running the code: Which variable has the highest **variance**?

4. Simulate a third variable `x3` by adding a small random quantity to `x1`: `x3 <- x1 + rnorm(n = 100, mean = 0, sd = 1)`, then plot `x3`

5. Simulate a fourth variable `x4` by adding a large random quantity to `x1`: `x4 <- x1 + rnorm(n = 100, mean = 0, sd = 20)`, then plot `x4`

6. Try answering before running the code: Which variable has the highest variance?

7. Try answering before running the code (but you can try plotting the variables): Which variable has the highest **covariance** with `x1`? Which has the lowest covariance?

8. Try answering before running the code: Which variable has the highest **correlation** with `x1`? Which has the lowest correlation?

```{r echo=FALSE,eval=FALSE}
# 1. Simulate and plot 100 values of a normally distributed variable `x1` with mean 10 and sd 2
x1 <- rnorm(n = 100, mean = 10, sd = 2) # simulating variable
plot(x1) # plotting variable (maybe not the right plot, eh?)
hist(x1) # now we're talking :)

# 2. Simulate 100 values of a second normally distributed variable `x2` with mean 10 and standard deviation 10, then plot `x2`
x2 <- rnorm(n = 100, mean = 10, sd = 10) # simulating variable
hist(x2) # plotting

# 3. Which variable has the highest **variance**?
## since SD is the root of the variance, and since SD are 2 and 10 for x1 and x2, respectively, x2 has a higher variance than x1
## this is also shown by the plots above, which we can visualize in a single plot (note: I'm setting xlim to have the same scale on the x axis and make the two plots more comparable:
par(mfrow=c(2,1))
hist(x1,xlim=c(min(x2),max(x2)))
hist(x2,xlim=c(min(x2),max(x2)))
## now let's verify this:
var(x1)
var(x2) # x2 has a higher variance than x1
var(x2) > var(x1) # to make it even more eplicit :)

# 4. Simulate a third variable `x3` by adding a small random quantity to `x1`: 
x3 <- x1 + rnorm(n = 100, mean = 0, sd = 1)
hist(x3) # x3 looks similar to x1 but it is centered on a higher value (because we added a quantity)

# 5. Simulate a fourth variable `x4` by adding a large random quantity to `x1`: 
x4 <- x1 + rnorm(n = 100, mean = 0, sd = 20)
hist(x4) # x4 is centered on a higher value of x1 and it is also wider

# 6. Which variable has the highest variance?
## x4 will have a higher variance than x3 because we summed x1 with a quantity with a greater SD (sd = 20) than when we created x3 (sd = 1)
## this is also shown by the plots above, which we can visualize in a single plot (note: I'm setting xlim to have the same scale on the x axis and make the two plots more comparable:
par(mfrow=c(2,1))
hist(x3,xlim=c(min(x4),max(x4)))
hist(x4,xlim=c(min(x4),max(x4)))
## now let's verify this:
var(x3)
var(x4) # x4 has a higher variance than x3
var(x3) < var(x4) # to make it even more eplicit :)

# 7. Which variable has the highest **covariance** with `x1`? Which has the lowest covariance?
## both x3 and x4 have been created by adding a quantity to x1, so they should covary with x1 more strongly than x2, which is the least covarying variable
## among x3 and x4, x3 should covary more strongly with x1 because the quantity that we added had a smaller SD than the quantity that we added when we created x4 (i.e., less random variability added to x1)
## this can be seen in the plots below (although the differences between x2 and x4 might not be so evident)
par(mfrow=c(1,3))
plot(x1 ~ x2)
plot(x1 ~ x3) # x3 shows the stronger covariance with x1
plot(x1 ~ x4) 
## now let's verify this:
cov(x1,x2) # x2 shows the weaker covariance with x1
cov(x1,x3)
cov(x1,x4)
## what? Does it look unexpected? It seems that x1 covaries more with x4 than with x3, how is that possible?
## that's because COVARIANCE VALUES DEPEND ON THE SCALE OF EACH VARIABLE
## since x4 has a larger scale (SD = x1 + 20) than x3 (SD = x1 + 1), its covariance is higher even if its relationship with x1 is lower --> that's why we need correlation ;)
## rule: you cannot compare two relationships between 3 or 4 variables with the covariance

# 8. Which variable has the highest **correlation** with `x1`? Which has the lowest correlation?
## correlation is the standardized version of covariance, ranging from -1 to 1
## with correlation, we can compare different relationships between variables using different scales
## so, now we can expect what we wrongly predicted above:
## - the strongest correlation between x1 and x3
## - then, between x1 and x4 (due to the large variability)
## - finally, between x1 and x2 (because x2 was created independently from x1)
## let's see:
cor(x1,x2) # the weakest correlation is between x1 and x2 (you got it)
cor(x1,x3) # the strongest one is between x1 and x3 (exactly as predicted)
cor(x1,x4) # the one between x1 and x4 is somewhere in the middle (great!)
## now you got the difference between covariance and correlation ;)
```

## 13. Covariance & correlation matrix

\fontsize{8pt}{12}\selectfont

1. Download and read the *Pregnancy during pandemics* data, then select the columns 2, 5, and 14, and rename them as "age", "depr", and "threat" (see `2-multilevel.pdf` slide #10)

2. Use the function `na.omit()` to remove all cases with missing values in one or more variables. How may observations have been removed?

3. Describe and plot the included variables

4. Compute the **covariance matrix** $S$ of the variable and use the logic operators `==` (is equal) and `!=` (is different) operators to check the two properties of the covariance matrix S (symmetry and main diagonal)

5. Standardize all variables (\color{blue} $z_{x_i} = (x_i - \overline{x})/s_x$ \color{black}), plot them, and compute their mean and SD. What do you observe?

6. Re-compute the covariance matrix on standardized variables. What do you observe?

7. Compute the **correlation matrix** of the original variables and compare it with the covariance matrix computed from standardized variables. What do you observe?

```{r echo=FALSE,eval=FALSE}
# SOLUTION TO EXERCISE #13

rm(list=ls()) # emptying the work environment

# 1. Download and read the *Pregnancy during pandemics* data, then select the columns 2, 5, and 14, and rename them as "age", "depr", and "threat" (see `2-multilevel.pdf` slide #10)
library(osfr) # package to interact with the Open Science Framework platform
proj <- "https://osf.io/ha5dp/" # link to the OSF project
osf_download(osf_ls_files(osf_retrieve_node(proj))[2, ],conflicts="overwrite") # download (note: you need to be connected to internet)
preg <- read.csv("OSFData_Upload_2023_Mar30.csv") # read data
preg <- preg[,c(2,5,14)] # selecting variables
colnames(preg) <- c("age","depr","threat") # renaming variables

# 2. Use the function `na.omit()` to remove all cases with missing values in one or more variables
originalN <- nrow(preg) # original number of observations
preg <- na.omit(preg) # removing rows with missing value in 1+ columns
originalN - nrow(preg) # we removed 1,204 observations with 1+ missing value

# 3. Describe and plot the included variables
summary(preg) # different variables with different scales: age (17-49.7), depr (0-30), threat (0-100)
lapply(preg,sd) # different variability (SD of 4.4, 5.5, 25.6, respectively)
par(mfrow=c(1,3)) # 3 plots in the same panel
hist(preg$age)
hist(preg$depr)
hist(preg$threat)

# 4. Compute the **covariance matrix** S of the variable and use the logic operators `==` (is equal) and `!=` (is different) operators to check the two properties of the covariance matrix S (symmetry and main diagonal)
S <- cov(preg) # computing covariance matrix
S # here's the covariance matrix of the trhee variables
## symmetry: S_ij = S_ji
S[1,2] == S[2,1]
S[1,3] == S[3,1]
S[2,3] == S[3,2]
## variances in the main diagonal
S[1,1] == var(preg$age)
S[2,2] == var(preg$depr)
S[3,3] == var(preg$threat)

# 5. Standardize all variables, plot them, and compute their mean and SD
age_z <- ( preg$age - mean(preg$age) ) / sd(preg$age)
depr_z <- ( preg$depr - mean(preg$depr) ) / sd(preg$depr)
threat_z <- ( preg$threat - mean(preg$threat) ) / sd(preg$threat)
## plotting standardized variables over original variables
par(mfrow=c(2,3)) # 6 plots in the same panel
hist(preg$age)
hist(preg$depr)
hist(preg$threat)
hist(age_z)
hist(depr_z)
hist(threat_z) # both mean (0) and variability (1) have been changed but the shape is somehow very similar
lapply(data.frame(age_z,depr_z,threat_z),mean) # the mean of each standardized variable is basically zero
lapply(data.frame(age_z,depr_z,threat_z),sd) # the SD of each standardized variable is 1
## that's because standardization forces a variable to have mean = 0 and SD = 1 (and thus, variance = 1)

# 6. Re-compute the covariance matrix of the standardized variables
S1 <- cov(data.frame(age_z,depr_z,threat_z))
S1
## the signs of covariances has remained the same of the original covariance matrix
## the properties of the matrix are the same (symmetry; main diagonal = variance)
## all values are bounded between -1 and +1
## all main diagonal values are equal to 1
## THIS IS A CORRELATION MATRIX!

# 6. Compute the **correlation matrix** of the original variables and compare it with the covariance matrix computed from standardized variables.
R <- cor(preg) # the correlation matrix R of the original variable is identical to the covariance matrix S1 of the standardized variables
S1 == R # only two values are different (this is due to the original variables not being perfectly normally distributed)
round(S1,2) == round(R,2) # but if we round at the second decimal, they are identical

## now we got that correlation is equivalent to the covariance of the standardized variables :)
```

## 14. Reading a SEM plot

\fontsize{8pt}{12}\selectfont
In the figure below:

1. How many **latent & observed** variables? Which ones?

2. How many **endogenous & exogenous** variables? Which ones?

3. How many **error terms**? Which ones? (note: this plot represents errors/residuals by using the third graphical notation shown in `3-multivariate.pdf` slide #21)

4. How many **path coefficients** (i.e., single-headed arrows)? Which ones?

5. Which path coefficients are included in the **measurement model**? Which ones in the **structural model**?

6. How many estimated parameters, in total?

```{r echo=FALSE}
# SOLUTION TO EXERCISE #14

# 1. How many **latent & observed** variables? Which ones?
## 2 latent variables: F1 and F2 (represented by circles)
## 7 observed variables: I1, I2, ..., I6, and x (represented by squares)

# 2. How many **endogenous & exogenous** variables? Which ones?
## 1 exogenous variable: x (i.e., the only variable that does not receive any arrow)
## 8 endogenous variables: I1, I2, ..., I6, F1, and F2 (all receiving 1 arrow)

# 3. How many **error terms**? Which ones? (note: this plot represents errors/residuals by using the third graphical notation shown in `3-multivariate.pdf` slide #21)
## 8 error terms, i.e., one per each endogenous variable (I1, I2, ..., I6, F1, and F2)
## note: only endogenous variables are represented with their error terms!

# 4. How many **path coefficients** (i.e., single-headed arrows)?
## 8 path coefficients: x -> F1, F1 -> F2, F1 -> I1,I2,I3, F2 -> I4,I5,I6
## note: in this example, since there are no endogenous variables receiving 2 or more arrows, the number of path coefficients is equal to the number of endogenous variables

# 5. Which path coefficients are included in the **measurement model**? Which ones in the **structural model**?
## the path coefficients linking the latent variable F1 to the observed variables I1,I2,I3, and those linking the latent variable F2 to the observed variables I4,I5,I6 are included in the measurement model (i.e., the model linking latent variables to the observed variables assumed to reflect them)
## the path coefficients linking x to F1 and F1 to F2 are included in the structural model (i.e., the model of structural relationships between the variables of interest)

# 6. How many estimated parameters, in total?
## number of estimated parameters 
##      = number of path coefficients + number of estimated errors 
##      = total number of arrows 
##      = 16

# this is to generate the figure
 if (knitr::is_html_output())
 {
   knitr::include_graphics("https://www.methods.manchester.ac.uk/themes/survey-and-statistical-methods/structural-equation-models/structural.jpg")
 } else {
   knitr::include_url("https://www.methods.manchester.ac.uk/themes/survey-and-statistical-methods/structural-equation-models/structural.jpg")
 }
```

## 15. Reading a path diagram (1/2)

\fontsize{8pt}{12}\selectfont
In the figure below (which indexes parameters using different letters than those we saw in class, but don't care about that):

1. How many **latent & observed** variables? Which ones?

2. How many **endogenous & exogenous** variables? Which ones?

3. How many **error terms**? Which ones? (note: this plot represents errors/residuals by using the first graphical notation shown in `3-multivariate.pdf` slide #21)

4. How many **path coefficients** (i.e., single-headed arrows)? Which ones?

5. How many **covariances**?

6. How many estimated parameters, in total?

```{r echo=FALSE}
# SOLUTION TO EXERCISE #15

# 1. How many **latent & observed** variables? Which ones?
## This is a path model and, thus, it only includes observed variables
## Yet, due to the used graphical notation to represent errors, we can say that it includes two latent variables, namely the errors of the two endogenous variables y1 and y2

# 2. How many **endogenous & exogenous** variables? Which ones?
## 2 endogenous variables: y1 (receiving 2 single-headed arrows) and y2 (receiving 3 single-headed arrows)
## 2 exogenous variables: x1 and x2 (receiving no single-headed arrows)
## note: we might also consider the two errors zeta_1 and zeta_2 as exogenous variables, but this is due to the graphical notation used to represent errors

# 3. How many **error terms**? Which ones?
## 2 error terms, i.e., one per each endogenous variable (zeta1 for y1 and zeta2 for y2)
## note: only endogenous variables are represented with their error terms!
## with this graphical notation, we can also see the variance of x1 and x2 (i.e., the rounded double-headed arrows starting and ending at the same variable), but these are the variable variances, not the variances of their error
## note: exogenous variables are assumed to be measured without error!

# 4. How many **path coefficients** (i.e., single-headed arrows)?
## 5 path coefficients: x1 -> y1, x1 -> y2, x2 -> y1, x2 -> y2, y1 -> y2

# 5. How many **covariances**?
## 1 covariance: x1 <-> x2 (covariances are represented by double-headed arrows linking two variables)

# 6. How many estimated parameters, in total?
## number of estimated parameters 
##      = number of path coefficients (5) + number of estimated errors (2) + number of covariances (1)
##      = 8
## this was tricky because of the way the model is represented:
## - for each error, it might seem that we need 2 parameters but only the error variance is a parameter (note that the relationship between error and variable is fixed to 1, so it doesn't count as a parameter to be estimated)
## - we don't count the variances of the exogenous variables (actually, we shouldn't count event the covariance, but let's consider it for now... se slide #24)

# this is to generate the figure
 if (knitr::is_html_output())
 {
   knitr::include_graphics("https://stats.oarc.ucla.edu/wp-content/uploads/2021/02/m4b.png")
 } else {
   knitr::include_url("https://stats.oarc.ucla.edu/wp-content/uploads/2021/02/m4b.png")
 }
```

## 15. Reading a path diagram (2/2)

\fontsize{8pt}{12}\selectfont

Which system of equations would you use to represent the model shown in the previous slide? (note 1: for better consistency, I'm using the same letters used in the figure, but don't care about it; note 2: variances and covariances are not reported in the equations)

A) $$ \begin{cases} y_2 = \gamma_{21}x_1 + \gamma_{22}x_2 + \zeta_2 \\ y_1 = \gamma_{11}x_1 + \gamma_{12}x_2 + \zeta_1 \end{cases} $$

B) $$ \begin{cases} y_2 = \gamma_{21}x_1 + \gamma_{22}x_2 + \zeta_2 \\ y_1 = \gamma_{11}x_1 + \gamma_{12}x_2 + \zeta_1 \\ x_2 = \beta_1x_1 + \gamma_{12}y_1 + \gamma_{22}y_2 + \epsilon_{x1} \end{cases} $$

C) $$ y_2 = \gamma_{21}x_1 + \gamma_{22}x_2 + \beta_{21}y_1 + \zeta_2  $$

D) $$ \begin{cases} y_2 = \gamma_{21}x_1 + \gamma_{22}x_2 + \beta_{21}y_1 + \zeta_2 \\ y_1 = \gamma_{11}x_1 + \gamma_{12}x_2 + \zeta_1 \end{cases} $$

```{r echo=FALSE}
# the correct answer is option D
## we can discard options B and C because we know that the number of equations should be equal to the number of endogenous variables (here, two) (see slide #20)
## then we can discard option A because it does not include the path coefficient beta_21 between y1 and y2
```