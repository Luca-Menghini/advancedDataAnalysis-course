---
title: 'ADVANCED DATA ANALYSIS \newline FOR PSYCHOLOGICAL SCIENCE'
subtitle: 'Homework exercises'
author:  |
 |
 | **Luca Menghini Ph.D.** \fontsize{9pt}{7.2}\selectfont
 |
 | luca.menghini@unipd.it
 |
 |
 | ***
 | Master degree in Developmental and Educational Psychology 
 |
 | University of Padova
 |
 | 2023-2024
 |
 | ![](img/logo.PNG){width=1.7in}
output:
  beamer_presentation:
    fonttheme: serif
    theme: Singapore
    slide_level: 2
    includes:
      in_header: mystyle_ex.tex
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE,tidy.opts = list(width.cutoff=80))
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = NA
)
library(Cairo)

```

## Some instructions to solve the exercises

\fontsize{7.5pt}{12}\selectfont
The present document includes some optional homework exercises on the contents presented during lectures. Similar to the course slides, exercises will be progressively updated as the course progresses.

To check the **exercise solutions**, just look at the `exeRcises.Rmd` file on either Github or Moodle: each exercise text is followed by a chunk of R code showing point-by-point solutions (or some among the many possible solutions).

If you have any doubts on how to solve the exercises, feel free to write me an e-mail or (even better) try writing in the **Moodle forum**, so that all students can see your question and try to reply it. We can also solve some exercise during lectures, but let me know! ;)

# LM

## 1. Correlation & regression

\fontsize{7.5pt}{12}\selectfont

For each couple of variables ($x,y$) generated as specified below:

a) represent univariate (boxplot) and bivariate distributions (scatter plot)

b) compute their correlation

c) use the `lm()` function to get the slope coefficient $\beta_1$ and determinate whether the relationship significantly differs from zero \newline

1. `y <- rnorm(50)` and `x1 <- y`

2. `x2 <- y + 10`

3. `x3 <- rnorm(50)`

4. `x4 <- x3 + 10`

5. Which conclusions can we draw? Which relationship between correlation and regression coefficient?

\fontsize{5.5pt}{12}\selectfont ____ \newline Source: Pastore, M. (2021). Analisi dei dati in contesti di comunità. Course held at the University of Padova, Academic Year 2021-2022.

```{r echo=FALSE,eval=FALSE}
# SOLUTION TO EXERCISE #1

rm(list=ls()) # emptying the work environment

# 1. y <- rnorm(50) and x1 <- y
y <- rnorm(50) # assigning values
x1 <- y
# a) univariate (boxplot) and bivariate distributions (scatter plot)
# par(mfrow=c(1,2)) # to have 2 graphs per plot
boxplot(y)
boxplot(x1) # boxplot: they are the same
# par(mfrow=c(1,1)) # to go back to 1 graph per plot
plot(y ~ x1) # scatter plot: perfectly correlated
# b) correlation
cor(x1,y) # r = 1 (perfect correlation)
# c) simple linear regression
fit <- lm(y ~ x1)
coefficients(fit) # when x1 = 0, y = 0; when x1 increases by 1 unit, y increases by 1 unit (they are the same)

# 2. x2 <- y + 10
x2 <- y + 10 # assigning values
# a) univariate (boxplot) and bivariate distributions (scatter plot)
# par(mfrow=c(1,2)) # to have 2 graphs per plot
boxplot(y)
boxplot(x2) # boxplot: same shape but centered on different values (0 vs. 10)
# par(mfrow=c(1,1)) # to go back to 1 graph per plot
plot(y ~ x2) # scatter plot: still perfectly correlated
# b) correlation
cor(x2,y) # r = 1 (perfect correlation)
# c) simple linear regression
fit <- lm(y ~ x2)
coefficients(fit) # when x1 = 0, y = -10; when x1 increases by 1 unit, y increases by 1 unit (perfect correlation)

# 3. x3 <- rnorm(50)
x3 <- rnorm(50) # assigning values
# a) univariate (boxplot) and bivariate distributions (scatter plot)
# par(mfrow=c(1,2)) # to have 2 graphs per plot
boxplot(y)
boxplot(x3) # boxplot: different shapes (y variates more) but both centered on zero
# par(mfrow=c(1,1)) # to go back to 1 graph per plot
plot(y ~ x3) # scatter plot: the correlation is no longer perfect
# b) correlation
cor(x3,y) # I got r = 0.03 but you might get a different value since y and x3 are randomly generated
# c) simple linear regression
fit <- lm(y ~ x3)
coefficients(fit) # I got b0 = -0.21 (when x3 = 0, y = -0.21) and b1 = 0.03 (when x increase by 1 unit, y increases by 0.03 units)

# 4. x4 <- x3 + 10
x4 <- x3 + 10 # assigning values
# a) univariate (boxplot) and bivariate distributions (scatter plot)
par(mfrow=c(1,2)) # to have 2 graphs per plot
boxplot(y)
boxplot(x4) # boxplot: different shapes (y variates more) and different central value (0 vs. 10)
# par(mfrow=c(1,1)) # to go back to 1 graph per plot
plot(y ~ x4) # scatter plot: the correlation is no longer perfect
# b) correlation
cor(x4,y) # I got r = 0.03 but you might get a different value since y and x3 are randomly generated
# c) simple linear regression
fit <- lm(y ~ x4)
coefficients(fit) # I got b0 = -0.56 (when x3 = 0, y = -0.56) and b1 = 0.03 (when x increase by 1 unit, y increases by 0.03 units)

# Which conclusions can we draw? Which relationship between correlation and regression coefficient?
# when two variables are identical or just differ by a constant (here, 10) their correlation is perfect
# in contrast, two randomly generated variables poorly correlate, regardless of the contrast between them
# in this particular case, where observations are randomly sampled from the normal distribution
  # the regression coefficients (slope) is almost identical to the Pearson correlation coefficient
```

## 2. LM assumptions & diagnostics

\fontsize{7.5pt}{12}\selectfont

Using the “*Pregnancy during pandemics*” data* that we saw in class, graphically evaluate the diagnostics of the selected model `m2`:

1. **Linearity**: are model residuals centered on zero?

2. **Normality**: are model residuals normally distributed?

3. **Homoscedasticity**: is residual variance constant over the levels of any predictor?

4. **Independence error-predictor**: are residuals unrelated to any predictor?

5. **Independence of observations**: based on the considered variables (`depr`, `threat`, `NICU`, and `age`), are individual observations independent?

6. **Absence of influential observations**: is there any observation that strongly influence the estimated coefficients?

7. **Absence of multicollinearity**: are predictors mutually unrelated?

\fontsize{5.5pt}{12}\selectfont ____ \newline *To read the dataset, you can either use the code in `2-multilevel.pdf` slide #10 or download the `pregnancy.RData` file from Moodle/Github ("data" folder) and use the command \color{blue} `load("pregnancy.RData")`

```{r echo=FALSE,eval=FALSE}
# SOLUTION TO EXERCISE #2

rm(list=ls()) # emptying the work environment

# loading data (from Github)
library(osfr) # package to interact with the Open Science Framework platform
proj <- "https://osf.io/ha5dp/" # link to the OSF project
osf_download(osf_ls_files(osf_retrieve_node(proj))[2, ],conflicts="overwrite") # download
preg <- na.omit(read.csv("OSFData_Upload_2023_Mar30.csv",stringsAsFactors=TRUE)) # read data
colnames(preg)[c(2,5,12,14)] <- c("age","depr","NICU","threat") # set variable names

# alternative way for loading data (from working directory, once you downloaded them from GitHub or Moodle)
load("4-data/pregnancy.RData") # (note: I saved the file in the "4-data" subfolder of my working directory)
head(preg) # showing first rows (the dataset is called 'preg')

# fitting model m2
m2 <- lm(depr ~ threat + NICU + age, data = preg)

# 1. linearity
hist(residuals(m2)) # histogram of residuals: residuals seem quite (although not perfectly) centered on zero (ok)

# 2. normality
hist(residuals(m2)) # histogram of residuals: seem quite (although not perfectly) normally distributed (ok)
qqnorm(residuals(m2)); qqline(residuals(m2)) # only some deviation from normality in the lower tale of the residual distribution (i.e., positive skewness) (ok, negligible)

# alternative way to assess normality (note: both the performance and the qqplotr package should be installed)
plot(performance::check_normality(m2)) # the plot is equivalent to the previous one, but this representation seems to magnify the residual deviations from normality. Based on this plot, we might want to use alternative family distributions than the normal, due to the large deviation in the lower tail of the residual distribution

# 3. heteroscedasticity
plot(residuals(m2)~preg$threat) # residual variance (spreadness of dots) does not seem to change over the values of threat (ok)
plot(residuals(m2)~preg$age) # residual variance (spreadness of dots) does not seem to change over the values of age (ok)
plot(residuals(m2)~preg$NICU) # residual variance (size of the boxes) does not seem to change between NICU levels (ok)

# alternative way to assess heteroscedasticity considering all variables: reasiduals vs. fitted values
plot(m2,which=3) # residual vs. fitted values: if you see no clear trends, then you're ok (as in this case)
plot(performance::check_heteroscedasticity(m2)) # alternative way to generate the same plot

# 4. independence residuals - predictors
plot(residuals(m2)~preg$threat) # not a strong relationship between residuals and threat values (ok)
abline(lm(residuals(m2)~preg$threat),col="red") # to add a regression line
plot(residuals(m2)~preg$age) # not a strong relationship between residuals and age values (ok)
abline(lm(residuals(m2)~preg$age),col="red") # to add a regression line
plot(residuals(m2)~preg$NICU) # no substantial differences in residuals between NICU levels (ok)

# 5. independence of observations
# based on the considered variables, there are not uncontrolled factors that might account for local dependencies in the data (e.g., hospitals, cities, neighborhoods, etc.) (ok)

# 6. Absence of influential observations: is there any observation that strongly influence the estimated coefficients?
plot(m2,which=5) # no data points outside the Cook's distance cut-off region (which is not even included within the plot) (ok)

# alternative ways to evaluate influential cases
car::leveragePlots(m2) # this shows the leverage (a measure of influence) for each x-y couple
# participants 185 and 8491 show the most extreme values (although not excessively extreme), and might deserve a deeper inpsection (i.e., what happens if I remove them and re-run the model?)
car::outlierTest(m2) # testing whether outliers are present in the data (here, p > .05 so we're ok)
car::influencePlot(m2) # inspecting both Cook's distance and leverages: we see a few cases with high laverage (1196) or high Cook's distance (e.g., 9100), which might deserve a deeper inspection

# 7. Absence of multicollinearity: Are predictors mutually unrelated?
cor(preg[,c("threat","age")]) # threat and age only correlate at r = 0.02 (low linear correlation)
library(car)
barplot(vif(m2),ylim=c(0,10)); abline(h=5,lty=2,col="red") # no VIF higher than 5 (ok)
```

## 3. Towards multilevel modeling

\fontsize{7.5pt}{12}\selectfont

1. Download and read the “*Adolescent insomnia*” dataset `INSA.RData` (Moodle/Github, "data" folder)

2. Explore the variables `dayNr` (day of assessment), `stress` (bedtime rating of daily stress), `insomnia` (categorical: insomnia vs. controls), and `TST` (total sleep time, in minutes) &rightarrow; mean, SD, frequencies, plots, and correlations

3. Fit a null model `m0` predicting `TST`

4. Fit a simple regression model `m1` predicting `TST` by `stress`

5. Fit a multiple regression model `m3` predicting `TST` by `stress` and `insomnia`

6. Compare the three models with the AIC and the likelihood ratio test

7. Print and interpret the coefficients (and their statistical significance) of the selected model

8. Now create two subsets of the `insa` dataset: `insa1` only including observations from the participant `s001` and `insa2` with observations from participant `s002`: how many rows in each dataset?

9. Repeat points 3-7 by using the two subsets: Are results consistent with what you found in the full sample?

```{r echo=FALSE,eval=FALSE}
# SOLUTION TO EXERCISE #3

rm(list=ls()) # emptying the work environment

# 1. loading data from Github
repo <- "https://github.com/SRI-human-sleep/INSA-home" # loading datasets from GitHub
load(url(paste0(repo,"/raw/main/Appendix%20D%20-%20Data/emaFINAL.RData")))
# alternatively, you can download the ema dataset from Github on Moodle, paste it in your working directory, and run the following lines (note: I saved the file in the "4-data" subfolder of my working directory):
load("4-data/insa.RData")

# 2. Explore the variables `dayNr`, `stress`, `insomnia`, and `TST`
# descriptives
mean(insa$dayNr,na.rm=T); sd(insa$dayNr,na.rm=T) # on average, 34 days per participant (SD = 21.2 days)
mean(insa$stress,na.rm=T); sd(insa$stress,na.rm=T) # on average, stress = 2.2 (over 5) (SD = 1)
table(insa$insomnia) # 3122 observations from the insomnia group, 3097 from the control group
mean(insa$TST,na.rm=T); sd(insa$TST,na.rm=T) # on average, 414 minutes of sleep (SD = 80.9 minutes)
# plots
par(mfrow=c(2,2)) # to have 4 graphs in 1 plot
hist(insa$dayNr) # skewed variable with most values being below 50 days, and a few values up to 350 days
hist(insa$stress) # skewed variables with most values being 1 or 2, and a few values being 5
plot(insa$insomnia) # quite balanced number of observations between groups
hist(insa$TST) # quite normally distributed variable
# correlations: very small positive correlation between TST and dayNr, very small negative correlation between stress and both dayNr and TST
cor(insa[,c("dayNr","stress","TST")],use="complete.obs")

# 3. Fit a null model `m0` predicting `TST`
m0 <- lm(TST ~ 1, data=insa[!is.na(insa$stress),]) # note: selecting only cases with nonmissing stress values, otherwise this and the following models will have a different sample size (by default, R uses all nonmissing observations)

# 4. Fit a simple regression model `m1` predicting `TST` by `stress`
m1 <- lm(TST ~ stress, data=insa)

# 5. Fit a multiple regression model `m3` predicting `TST` by `stress` and `insomnia`
m2 <- lm(TST ~ stress + insomnia, data=insa)

# 6. Compare the three models with the AIC and the likelihood ratio test
AIC(m0,m1,m2) # AIC: the lower the better -> best model is m2
lmtest::lrtest(m0,m1,m2) # both m1 and m2 are significant -> best model is m2

# 7. Print and interpret the coefficients (and their statistical significance) of the selected model
summary(m2)$coefficients
# when stress = 0 and insomnia = 0 (control group), the most expected TST value (mean) is 418.16 minutes, which is significantly higher than zero
# when insomnia = 0 (control group), a 1-unit increase in stress predicts a 3.3-minute decrease in TST, which is statistically significant
# when stress = 0, the insomnia group is predicted to show a mean TST of 5.46 higher than the control group, but this difference is not statistically significant 

# 8. subsample data from participant s001 and s002: how many rows?
insa1 <- insa[insa$ID=="s001",]
nrow(insa1) # 93 rows (days) from participant s001
insa2 <- insa[insa$ID=="s002",]
nrow(insa2) # 65 rows (days) from participant s002

# 9. Repeat points 3-7 by using the two subsets: Are results consistent with what you found in the full sample?
# s001
m0 <- lm(TST ~ 1, data = insa1[!is.na(insa1$stress),])
m1 <- lm(TST ~ stress, data = insa1)
# m2 <- lm(TST ~ stress + insomnia, data = insa1) # error! Because when we only sample 1 participant, then the insomnia variable does not variate --> so we do not consider model m2, but only m0 and m1
AIC(m0,m1) # m0 is better than m1
lmtest::lrtest(m0,m1) # m1 is not significantly better than m0
# s002
m0 <- lm(TST ~ 1, data = insa2[!is.na(insa2$stress),])
m1 <- lm(TST ~ stress, data = insa2)
AIC(m0,m1) # m0 is better than m1
lmtest::lrtest(m0,m1) # m1 is not significantly better than m0
# conclusion: if we only consider participant s001 or participant s002, the results are different than those obtained from the full sample (i.e., the relationship between stress and TST is no longer significant)
```

# LMER

## 4. Multilevel data structure {#ex4}

\fontsize{7.5pt}{12}\selectfont

1. Download and read the “*Innovative teaching program*” dataset `studentData.csv` (Moodle/Github, "data" folder)

2. Explore the student-level variables `studId` (identification code of each student), `math_grade` (student grade in math) and `anxiety` (anxiety level). What is the total number of students? How many rows per students? What is the range of `math_grade` and `anxiety`? 

3. How many students per class? How many students per level of the `tp` variable?

4. How many classes per level of the `tp` variable? \fontsize{6pt}{12}\selectfont To answer that, you can create the **wide-form dataset** by taking only one row per class (e.g., try using the `duplicated()` function preceded by the `!` symbol to remove duplicated values of `classID`) \fontsize{7.5pt}{12}\selectfont

5. Compute the mean `math_grade` and `anxiety` value for each class and join them to the wide-form dataset: which is the class with the maximum `math_grade`? Which class has the maximum `anxiety` level?

6. Fit a simple linear regression model predicting `math_grade` by `anxiety` both on the long-form and on the wide-form dataset; inspect and interpret the estimated coefficients and their statistical significance.

7. Which model has the highest standard errors? Why?

```{r echo=FALSE,eval=FALSE}
# SOLUTION TO EXERCISE #4

# 1. Download and read the studentData.csv dataset
rm(list=ls()) # emptying the work environment
getwd() # getting where your working directory is --> copy the file in there
itp <- read.csv("studentData.csv")

# 2. explore student-level variables: No. of students? 
table(itp$studID) # number of observations per student (1)
nrow(itp) # number of students = 90
mean(itp$math_grade); sd(itp$math_grade) # on average, participants took 8 in math (SD = 0.6)
mean(itp$anxiety); sd(itp$anxiety) # on average, participants reported anxiety levels of 3.11 (SD = 0.06)
summary(itp[,c("math_grade","anxiety")]) # math_grade ranges from 6.9 to 9.3; anxiety ranges from 3.01 to 3.41
 
# 3. How many students per class? How many students per level of the `tp` variable (teaching program)?
table(itp$classID) # from 11 to 30 students per class
table(itp$tp) # 52 students in the control group, 38 students in the intervention group

# 4. How may classes per tp level?
itp_wide <- itp[!duplicated(itp$classID),] # from long-form to wide-form: removing duplicated classID values
nrow(itp_wide)  # itp_wide only has 4 rows (1 row per cluster)
table(itp_wide$tp) # 2 classes per level of the tp variable (2 in the control, 2 in the intervention group)

# 5. mean math_grade and anxiety per class, which class has the maximum value?
mathGr <- aggregate(math_grade ~ classID, data=itp, FUN = mean) # math_grade by class
anx <- aggregate(anxiety ~ classID, data = itp, FUN = mean) # anxiety by class
mathGr[which(mathGr$math_grade==max(mathGr$math_grade)),] # class D has the maximum math_grade
mathGr[which(anx$anxiety==max(anx$anxiety)),] # class A has the maximum anxiety
itp_wide <- cbind(itp_wide,
                  math_grade_mean=mathGr$math_grade,
                  anxiety_mean=anx$anxiety) # joining mean values to the wide-form dataset
itp_wide # showing the wide-form dataset

# 6. predict math_grade by anxiety on both datasets, inspect and interpret coefficients
fit_long <- lm(math_grade ~ anxiety, data=itp) # long-form dataset (N = 90)
summary(fit_long)$coefficients 
# when student anxiety = 0, student math_grade is 17.7 (out of range*), which is significantly higher than zero
# when student anxiety increases by 1 unit, student math_grade decreases by 3.11 units, which is statistically significant
fit_wide <- lm(math_grade_mean ~ anxiety_mean, data=itp_wide) # wide-form dataset (N = 4)
summary(fit_wide)$coefficients
# when class anxiety = 0, class math-grade is 63.44 (out of range*), which is significantly higher than zero
# when class anxiety increases by 1 unit, class math-grade decreases by 17.76, which is statistically significant

# *intercepts are out of range because predictors are not centered
# that is, since intercepts are estimated for anxiety = 0 (which is not possible), the model returns an impossible (i.e., out-of-range) value

# 7. which model has the highest standard errors? Why?
# the model fitted on the wide-form dataset (N = 4) shows the highest standard error (i.e., parameters are estimated with a poor precision) because the model uses a lower sample size (N = 4) compared to that used by the other model (N = 90)
```

## 5. Data centering

\fontsize{7.5pt}{12}\selectfont

Consider the long- and wide-form datasets from \color{blue}[exercise #4](#ex4)\color{black}:

1. Compute the **grand-mean-centered** `anxiety` values from the wide-form dataset

2. Fit a simple linear model predicting class-level `math_grade` by grand-mean-centered `anxiety` using the wide-form dataset. Inspect and interpret the estimated coefficients, and compare them with those estimated in the previous exercise

3. Use the `join()` function from the `plyr` package to join the cluster-level mean `anxiety` values to the long-form dataset

4. Compute the **cluster-mean-centered** `anxiety` values by subtracting mean class `anxiety` from student-level `anxiety`

5. Considering class `A`, how many students have an `anxiety` level below the class average? How many have a higher value than the average?

6. Fit a simple linear model predicting student-level `math_grade` by cluster-mean-centered `anxiety` values using the long-form dataset. Inspect and intepret the estimated coefficients, and compare them with those estimated in the previous exercise

```{r echo=FALSE,eval=FALSE}
# SOLUTION TO EXERCISE #5

# 1. grand-mean-centered anxiety from the wide-form dataset (which I called "itp_wide" in exercise #4)
itp_wide$anxiety.gmc <- itp_wide$anxiety_mean - mean(itp_wide$anxiety_mean)
itp_wide[,c("anxiety_mean","anxiety.gmc")] # showing uncentered and centered values

# 2. linear model predicting math_grade by grand-mean-centered anxiety in the wide-form dataset
fit_wide.cmc <- lm(math_grade_mean ~ anxiety.gmc, data = itp_wide)
summary(fit_wide)$coefficients # coefficients from the uncentered model (which I called "fit_wide" in ex #4)
summary(fit_wide.cmc)$coefficients # coefficients from the centered model
# intercept changed from 63.4 to 8.13 (now it's on range!), but still significantly higher than zero
# the slope is basically the same, still significantly lower than zero

# 3. join cluster-level mean anxiety to the long-form dataset
library(plyr)
itp <- join(itp, # selecting long-form dataset
            itp_wide[,c("classID","anxiety_mean")], # selecting the columns of interest from the wide-form dataset
            by="classID") # joining by classID
itp[,c("anxiety","anxiety_mean")] # showing individual-level and cluster-level anxiety

# 4. cluster-mean-centered anxiety from the long-form dataset
itp$anxiety.cmc <- itp$anxiety - itp$anxiety_mean
itp[,c("anxiety","anxiety_mean","anxiety.cmc")] # showing ind-level, clust-level, and clust-mean-centered anxiety

# 5. Focusing on class A, how many student anxiety values below/above the class average?
classA <- itp[itp$classID=="A",] # subsetting data from class A
nrow(classA[classA$anxiety < mean(classA$anxiety),]) # 16 with math_grade below the class average
nrow(classA[classA$anxiety.cmc < 0,]) # alternative way based on cluster-mean-centered values (i.e., average = 0)
nrow(classA[classA$anxiety > mean(classA$anxiety),]) # 14 with math_grade above the class average
nrow(classA[classA$anxiety.cmc > 0,]) # alternative way based on cluster-mean-centered values (i.e., average = 0)

# 6. linear model predicting math_grade by cluster-mean-centered `anxiety` values
fit_long.cmc <- lm(math_grade ~ anxiety.cmc, data = itp) 
summary(fit_long)$coefficients # coefficients from the uncentered model (which I called "fit_long" in ex #4)
summary(fit_long.cmc)$coefficients # coefficients from the centered model
# intercept changed from 17.75 to 8.05 (now it's on range!), but still significantly higher than zero
# the slope changed from -3.11 to -2.20, still significantly lower than zero
```

## 6. Data centering & level-specific correlations {#ex6}

\fontsize{7.5pt}{12}\selectfont
Do left- and right-side infant pupil sizes correlate more at the within-subject or at the between-subject level?

1. Download and read the "*Infant pupil*" dataset `infantPupil.csv`

2. Subset columns 15, 10, 11, 12, and 13, and rename them as `ID` (subject identification code), `pupil.left` (left-side pupil size in mm), `pupil.left_valid` (validity of left-sized pupil size measurement), `pupil.right` (right-side pupil size in mm), and `pupil.right_valid` (validity of the right-side pupil size measurement)

3. How many valid cases for each eye? (note: 1 = valid, 0 = invalid)

4. Remove all cases with invalid pupil size in either one or the other eye 

5. Compute the cluster means and the cluster-mean-centered values for `pupil.left` and `pupil.right` 

6. Compute the between-subject and the within-subject correlations between the two variables: Do left- and right-side infant pupil sizes correlate more at the within-subject or at the between-subject level?

```{r echo=FALSE,eval=FALSE}
# SOLUTION TO EXERCISE #6

rm(list=ls()) # emptying work environment

# 1. reading infant pupil dataset (note: I saved the file in the 'data' subfolder in my working dir)
pupil <- read.csv2("data/infantPupil.csv") # use read.csv2 for columns separated by ";" (csv2) rather than "," (csv)

# 2. subsetting and renaming columns
pupil <- pupil[,c(15,10:13)]
colnames(pupil) <- c("ID","pupil.left","pupil.left_valid","pupil.right","pupil.right_valid")
summary(as.factor(pupil$pupil.left_valid))

# 3. How many valid cases?
table(pupil$pupil.left_valid) # 16,379 valid left
100*table(pupil$pupil.left_valid)/nrow(pupil) # percentage (65%)
table(pupil$pupil.right_valid) # 15,337 valid right
100*table(pupil$pupil.right_valid)/nrow(pupil) # percentage (60.5%)

# 4. removing invalid cases in either eye
pupil <- pupil[pupil$pupil.right_valid==1 & pupil$pupil.left_valid==1,]

# 5. cluster means and cluster mean centered
wide <- aggregate(pupil.left ~ ID, data=pupil, FUN=mean) # cluster mean of pupil.left
# I got Warning: argument is not numeric or logical (i.e., pupile sizes are considered as characters)
pupil$pupil.left <- as.numeric(pupil$pupil.left) # converting to numeric
pupil$pupil.right <- as.numeric(pupil$pupil.right)
wide <- aggregate(pupil.left ~ ID, data=pupil, FUN=mean) # now it works
wide # let's take a look
pupil <- plyr::join(pupil,wide,by="ID") # joining cluster means to long-form dataset
colnames(pupil)[6] <- "pupil.left.cm" # renaming variable to avoid duplicated column names
pupil$pupil.left.cmc <- pupil$pupil.left - pupil$pupil.left.cm # cluster mean centering

# same operations for pupil.right
wide <- aggregate(pupil.right ~ ID, data=pupil, FUN=mean) 
pupil <- plyr::join(pupil,wide,by="ID")
colnames(pupil)[8] <- "pupil.right.cm"
pupil$pupil.right.cmc <- pupil$pupil.right - pupil$pupil.right.cm 

# 6. level-specific correlations
pupil_wide <- pupil[!duplicated(pupil$ID),] # taking only one row per subject
# between-subject correlation (between cluster means from the wide-form dataset)
cor(pupil_wide$pupil.left.cm, # r = 0.987
    pupil_wide$pupil.right.cm) 
# within-subject correlation (between cluster-mean-centered values from the long-form dataset)
cor(pupil$pupil.left.cmc, # r = 0.865
    pupil$pupil.right.cmc)

# 7. Do left- and right-side infant pupil sizes correlate more at the within-subject or at the between-subject level?
# they seem to correlate more at the between-subject level!
```

## 7. Intraclass correlation coefficient

\fontsize{8.5pt}{12}\selectfont
Using data from \color{blue}[exercise #6](#ex6)\color{black}, compute the intraclass correlation coefficient (ICC) for both pupil size measures.

1. Do they variate more at the within-subject (lv1) or at the between-subject (lv2) level?

2. What is the percentage of within-subject variability over the total variability?

3. Does one eye variate more within-subject than the other?

```{r echo=FALSE,eval=FALSE}
# SOLUTION TO EXERCISE #7

# first, you can run all the commands used in exercise #6

# computing left-side pupil size ICC from null LMER model
library(lme4) # package for fitting LMER models
m0.left <- lmer(pupil.left ~ (1|ID), data=pupil) # null LMER model
tau2.left <- summary(m0.left)$varcor$ID[[1]] # extracting the random intercept variance tau2
sigma2.left <- summary(m0.left)$sigma^2 # extracting the residual variance sigma2
ICC.left <- tau2.left/(tau2.left + sigma2.left) # ICC = var between / total var = tau2 / (tau2 + sigma2)
ICC.left # 0.60

# computing ICC for right-side pupil size
m0.right <- lmer(pupil.right ~ (1|ID), data=pupil) # null LMER model
tau2.right <- summary(m0.right)$varcor$ID[[1]] # tau2
sigma2.right <- summary(m0.right)$sigma^2 # sigma2
ICC.right <- tau2.right/(tau2.right + sigma2.right) # ICC
ICC.right # 0.64

# 1. Do they variate more at the within-subject (lv1) or at the between-subject (lv2) level?
# ICC range from .60 (i.e., 60% of variance at the between level) to .64 (i.e., 64% of variance at the between level), so they variate more at the between-subject level (lv2) than at the within-subject level (lv1)

# 2. What is the percentage of within-subject variability over the total variability?
# we simply need to compute 1 - ICC
1 - ICC.left # left side: 39% of within-subject variability
1 - ICC.right # right side: 36% of within-subject variability

# 3. Does one eye variate more within-subject than the other?
# yes, left-side pupil size variates within-subject variability is 3% higher than right-side pupil size
# however, we don't know whether this difference might be substantial or not
```

## 8. Model fit and coefficeint interpretation {#ex8}

\fontsize{8.5pt}{12}\selectfont

1. Download and read the “*Innovative teaching program*” dataset `studentData.csv` (Moodle/Github, "data" folder)

2. Cluster mean center the variable `anxiety` so that we can focus the related slope at the within-individual level

3. Fit a null LMER model `m0` predicting `math_grade`

4. Compute and interpret the ICC

5. Fit a model `m1` with `math_grade` being predicted by `anxiety.cmc`

6. Fit a model `m2` including a random slope for `anxiety.cmc`

7. Fit a model `m3` also including group differences based on `tp` (i.e., innovative teaching program: control vs. intervention) - note: the `tp` variable should be converted as a factor

8. Fit a model `m4` also including the interaction between `anxiety.cmc` and `tp`

9. Inspect and interpret the `summary()` of models `m3` and `m4`

10. What can we say from model `m4`? Does the innovative teaching program improve math achievement? What is the role of anxiety?

```{r echo=FALSE,eval=FALSE}
# SOLUTION TO EXERCISE #8

# 1. download and read the studentData.csv dataset
rm(list=ls()) # emptying the work environment
getwd() # getting where your working directory is --> copy the file in there
itp <- read.csv("studentData.csv")

# 2. cluster mean center the variable `anxiety`
head(itp) # remember: itp is the long-form dataset with individual observations (students with their math_grade and anxiety scores) are nested within clusters (classID)
wide <- aggregate(anxiety ~ classID, data=itp, FUN=mean) # cluster means
itp <- plyr::join(itp,wide,by="classID") # joining cluster means to the long-form dataset
colnames(itp)[6] <- "anxiety.cm" # renaming variable to avoid double column names
itp$anxiety.cmc <- itp$anxiety - itp$anxiety.cm # cluster mean centering: subtracting the cluster mean from original values

# now we have anxiety (i.e., each student anxiety level), anxiety.cm (i.e., mean anxiety level for each class, identically repeating over all rows from the same class), and anxiety.cmc (i.e., cluster-mean-centered anxiety, where positive values indicate a student with a higher anxiety level than the average of his/her class, whereas negative values indicate students with lower anxiety levels than their class)

# 3. Fit a null LMER model `m0` predicting `math_grade`
library(lme4) # opening lme4 package
m0 <- lmer(math_grade ~ (1|classID), data = itp)
# note: model m0 includes the fixed intercept, the random intercept, and the residuals

# 4. Compute and interpret the ICC
tau2 <- summary(m0)$varcor$classID[[1]] # tau_00 squared = variance of the random intercept
sigma2 <- summary(m0)$sigma^2 # sigma squared = variance of the residuals
ICC <- tau2 / (tau2 + sigma2) # ICC = random intercept variance / total variance = random intercept variance / (random intercept variance + residual variance)
ICC # ICC = 0.25 --> the 25% of the variance in math_grade is at the between-cluster level (i.e., math_grade mainly varies within cluster than between clusters)

# 5. Fit a model `m1` with `math_grade` being predicted by `anxiety.cmc`
m1 <- lmer(math_grade ~ anxiety.cmc + (1|classID), data = itp)
 
# 6. Fit a model `m2` including a random slope for `anxiety.cmc`
m2 <- lmer(math_grade ~ anxiety.cmc + (anxiety.cmc|classID), data = itp)

# 7. Fit a model `m3` also including group differences based on `tp` (i.e., innovative teaching program: control vs. intervention) - note: the `tp` variable should be converted as a factor
summary(itp$tp) # itp is a character vector
itp$tp <- as.factor(itp$tp) # converting tp as a factor
summary(itp$tp) # now it is a factor
m3 <- lmer(math_grade ~ anxiety.cmc + tp + (anxiety.cmc|classID), data = itp)
# note: I got a singular fit (message saying "boundary (singular) fit: see help('isSingular')") - this means that the model didn't converge well (but we ignore this for now)
 
# 8. Fit a model `m4` also including the interaction between `anxiety.cmc` and `tp`
m4 <- lmer(math_grade ~ anxiety.cmc * tp + (anxiety.cmc|classID), data = itp)
m4 <- lmer(math_grade ~ anxiety.cmc + tp + anxiety.cmc:tp + (anxiety.cmc|classID), data = itp) # alternative way to write the same model

# 9. Inspect and interpret the `summary()` of models `m3` and `m4`
summary(m3)
# interpretation:
# - fixed intercept: the predicted ('mean') math_grade value when anxiety.cmc = 0 (i.e., average anxiety level within a given cluster) and tp = 0 (i.e., control group) is 7.81
# - fixed anxiety slope: considering both control and intervention, a 1-unit increase in anxiety.cmc (i.e., higher anxiety than the class mean) predicts a decrease in math_grade by -2.35 points
# note: we say "considering both control and intervention" because the interaction is not yet included - so the model makes a sort of average between the two groups
# - fixed tp slope: when anxiety.cmc = 0 (i.e., average anxiety levels), the intervention group is predicted to show an average math_grade of 0.62 points higher than the average math_grade in the control group (i.e., 7.81 + 0.62 = 8.43)

summary(m4)
# interpretation:
# - fixed intercept: the predicted ('mean') TST value when anxiety.cmc = 0 (i.e., average anxiety level) and tp = 0 (i.e., control group) is 7.85 minutes ---> same interpretation than in the additive model m3
# - fixed anxiety slope: in the control group, a 1-unit increase in anxiety.cmc (i.e., higher anxiety than the class average) predicts a decrease in math_grade of -7.187 points
# note: we say "in the control group" because the interaction is included in the model - so the model focuses this coefficient in the control group and quantifies the difference between the two groups in the interactive term below
# - fixed tp slope: when anxiety.cmc = 0 (i.e., average anxiety level), the intervention group is predicted to show an average math_grade of 0.53 points higher than the average math_grade in the control group (i.e., 7.85 + 0.53 = 8.38) ---> same interpretation than in the additive model m3
# - interaction (first interpretation): the relationship between anxiety.cmc and math_grade in the intervention group is 2.91 point-per-anxiety lower than in the control group (i.e., -1.48 + (-2.92) = -4.40); in other words, a 1-unit increase in axiety.cmc (higher anxiety than the class mean) predicts a math_grade reduction of -1.48 in the control group and a decrease of -4.40 minutes in the intervention group
# - interaction (second interpretation): the math_grade difference between intervention and controls is 2.91 points smaller when anxiety.cmc increases by 1 unit (i.e., higher anxiety than the class mean) compared to when anxiety.cmc = 0 (i.e., average anxiety level); in other words, anxiety reduces the differences between the two groups, with students in the intervention groups getting slightly less improved scores than those in the control group

# 10. What can we say from model `m4`?
# Based on model m4, we can say that the innovative teaching program seems to increase math achievement (i.e., the t-value is 2.599, which is higher than 1.96)
# Students with higher anxiety than the mean of their class seem to have a reduced beneficial effect of the innovative teaching program (interaction = -2.91), but such a reduction in the estimated effect is not 'substantial' (i.e., the t-value is -0.829, which is higher than -1.96)
# So let's do the innovative teaching program in all classes! :)
```

## 9. Fixed effect visualization & effect plots

\fontsize{8.5pt}{12}\selectfont
Using models `m1`, `m2`, and `m3` from \color{blue}[exercise #8](#ex8)\color{black}:

1. Visualize and interpret the **forest plot of the fixed effects** by using the `pot_model()` function from the `sjPlot` package

2. Compute the 95% confidence intervals visualized in the plots that you have just generated

3. Visualize and interpret the **fixed effect plots** by adding the argument `type="pred"` within the `plot_model()` function

4. Which are the parameters of model `m1`? Which are those of model `m2` and model `m3`?

```{r echo=FALSE,eval=FALSE}
# SOLUTION TO EXERCISE #9

# note: first run the code to solve exercise #8

# 1. forest plot of the fixed effects by using the `pot_model()`
library(sjPlot) # if not yet installed, first run install.packages("sjPlot")
plot_model(m1) # here, we only have the coefficient estimated for anxiety.cmc, which is negative and does not include zero, so we can say that math_grade is 'significantly' lower in students with an anxiety level lower than the average anxiety level of their class
plot_model(m2) # we can see a very similar situation: the fixed effect of anxiety is still negative and significantly lower than zero, even now that the model includes the corresponding random slope (i.e., cluster-specific variability in the slope estimate does not substantially impact the fixed slope)
plot_model(m3) # now we have 2 fixed effects: anxiety.cmc - which is still negative but no longer significantly lower than zero - and tp - which is positive and show 95% confidence intervals excluding zero, so we can say that the difference between groups is significantly higher than zero

# 2. Compute the 95% confidence intervals visualized in the plots that you have just generated

# model m1
(fixeffs <- summary(m1)$coefficients) # extracting fixed effects estimates and standard error
(anx.slope <- fixeffs[2,1]) # fixed slope for anxiety.cmc
(anx.slope_se <- fixeffs[2,2]) # standard error of the fixed slope for anxiety
anx.slope - 1.96 * anx.slope_se # lower CI = -4.21
anx.slope + 1.96 * anx.slope_se # upper CI = -4.19

# model m2
(fixeffs <- summary(m2)$coefficients) # extracting fixed effects estimates and standard error
(anx.slope <- fixeffs[2,1]) # fixed slope for anxiety.cmc
(anx.slope_se <- fixeffs[2,2]) # standard error of the fixed slope for anxiety
anx.slope - 1.96 * anx.slope_se # lower CI = -6.51
anx.slope + 1.96 * anx.slope_se # upper CI = -0.24

# model m3
(fixeffs <- summary(m3)$coefficients) # extracting fixed effects estimates and standard error
(anx.slope <- fixeffs[2,1]) # fixed slope for anxiety.cmc
(anx.slope_se <- fixeffs[2,2]) # standard error of the fixed slope for anxiety
anx.slope - 1.96 * anx.slope_se # lower CI = -5.746
anx.slope + 1.96 * anx.slope_se # upper CI = 1.03
(tp.slope <- fixeffs[3,1]) # fixed slope for anxiety.cmc
(tp.slope_se <- fixeffs[3,2]) # standard error of the fixed slope for anxiety
tp.slope - 1.96 * tp.slope_se # lower CI = 0.287
tp.slope + 1.96 * tp.slope_se # upper CI = 0.959


# 3. Visualize and interpret the **fixed effect plots** by adding the argument `type="pred"` within the `plot_model()` function

# m1
plot_model(m1,type="pred") # the plot shows the regression line estimated for the relationship between anxiety.cmc and math_grade, showing that the latter is predicted to decrease when the former increases
# what are the bands surrounding the line? they are the 95% confidence intervals, which are narrower in the center (i.e., more data points when anxiety.cmc is zero --> higher precision in the estimation) and wider in the tails (i.e., less data points when anxiety is highly higher or lower than the cluster mean --> lower precision in the estimation)
# is the relationship significant? Yes because the math_grade value corresponding to the minimum anxiety.cmc value is not included in the intervals represented for the math_grade value corresponding to the maximum anxiety.cmc value, and viceversa

# m2
plot_model(m2,type="pred") # the plot is basically equivalent to the previous one but confidence intervals are overall narrower, probably due to the inclusion of the random slope that reduced the standard error associated with the slope for anxiety.cmc

# m3
plot_model(m3,type="pred")
# the first plot is similar to the previous ones, but we can see that the inclusion of tp as an additional model predictor led to an increase of the uncertainty in parameter estimates (or a decrease in its precision), that is standard errors have became higher. This mainly affects the upper tail of the distribution, where confidence intervals become so large that they include the math_grade value corresponding to the minimum value of anxiety.cmc --> i.e., the relationship is no longer significant!
# the second plot shows the predicted math_grade values in the two groups along with their confidence intervals. We can see that the predicted math_grade for the control group (i.e., the red dot on the left) is not included in the confidence intervals estimated for the intervention group (i.e., the dotted line on the right), and viceversa. So we can say that the two groups significantly differ in math_grade, or that the intervention group has a significantly higher math_grade than the control group
 
# 4. Which are the parameters of model `m1`? Which are those of model `m2` and model `m3`?
# model m1: fixed intercept, fixed slope for anxiety.cmc, variance of the random intercept, residual variance (i.e., 4 parameters)
# model m2: fixed intercept, fixed slope for anxiety.cmc, variance of the random intercept, variance of the random slope, covariance between random intercept and random slope, residual variance (i.e., 6 parameters)
# model m3: fixed intercept, fixed slope for anxiety.cmc, fixed slope for tp, variance of the random intercept, variance of the random slope, covariance between random intercept and random slope, residual variance (i.e., 7 parameters)
```