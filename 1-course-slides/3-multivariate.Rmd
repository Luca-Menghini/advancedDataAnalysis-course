---
title: 'ADVANCED DATA ANALYSIS \newline FOR PSYCHOLOGICAL SCIENCE'
subtitle: 'Part 2. Introduction to multivariate modeling'
author:  |
 |
 | **Luca Menghini Ph.D.** \fontsize{9pt}{7.2}\selectfont
 |
 | luca.menghini@unipd.it
 |
 |
 | ***
 | Master degree in Developmental and Educational Psychology 
 |
 | University of Padova
 |
 | 2023-2024
 |
 | ![](img/logo.PNG){width=1.7in}
output:
  beamer_presentation:
    fonttheme: serif
    theme: Singapore
    slide_level: 2
    includes:
      in_header: mystyle.tex
---

## Outline of Part 2

\fontsize{8pt}{12}\selectfont
- **`sem()` intro**: Gentle introduction to the world of structural equation modeling (SEM)

- **Path analysis**: Introduction to path analysis (aka SEM with observed variables) and focus on *mediation models*

- **Model fit & mediation**: How to fit a path analysis in R, to interpret model results, to conduct a mediation analysis `r fontawesome::fa(name = "r-project", height = "1em")`

\color{blue}

- **`cfa()`**: How to conduct a confirmatory factor analysis (CFA) and to interpret its results `r fontawesome::fa(name = "microscope", height = "1em")`

- **Model evaluation**: How to evaluate model fit and compare multiple models `r fontawesome::fa(name = "microscope", height = "1em")`

\color{black}

___ \newline \fontsize{5pt}{12}\selectfont \color{blue}
`r fontawesome::fa(name = "microscope", height = "1em")` = not for the exam \newline \color{black}
`r fontawesome::fa(name = "r-project", height = "1em")` = exercises with R (bring your laptop!)

# sem() intro

## Multivariate analyses for a multivariate reality

```{r , echo = FALSE, fig.width=12,fig.height=3, warning=FALSE,message=FALSE}
library(ggplot2);library(gridExtra); library(plotly) # loading packages
y <- rnorm(n=50) # y = 50 random values from standard normal distribution
df <- data.frame(y=y, # dataframe to be used in the plot
                 x=y + rnorm(n=50,sd=0.8),
                 z=y - rnorm(n=50,sd=1.2)) # x = y + some random value
grid.arrange(ggplot(df,aes(y)) + geom_histogram() + theme(axis.text=element_blank(),title=element_text(size=18)) +
               ggtitle("Univariate"),
             ggplot(df,aes(x,y)) + geom_point() + theme(axis.text=element_blank(),title=element_text(size=18)) +
               ggtitle("Bivariate"),
             ggplot(df,aes(z,y)) + geom_point() + theme(axis.text=element_blank(),title=element_text(size=18)) +
               ggtitle("Still bivariate"),
             ggplot(df,aes(x,y,color=z)) + geom_point() + 
               theme(axis.text=element_blank(),legend.text = element_blank(),title=element_text(size=18)) +
               scale_color_gradient2(low="white",high="purple") +
               ggtitle("Multivariate"),
             nrow=1)
```

\fontsize{7.5pt}{12}\selectfont

- In psychology, we mainly inspect empirical data focusing on **univariate** (*y*) \newline or **bivariate** relationships (either *y* by *x* or *y* by *z*)

- But reality (particularly psychosocial reality) is complex, it is **multivariate** \newline i.e., more than two variables covarying at the same time

- It is *reductionist* to separately analyze our variables without considering their overall interactions &rightarrow; **biased effect estimates**

- **Structural equation modeling (SEM)** allow to analyze the relationships of interest by accounting for the multivariate reality of psychosocial phenomena (e.g., *y* by *x* covarying with *z*; *x* affects *y* through *z*)

## Observed indicators & latent variables

```{r , echo = FALSE,fig.width=12,fig.height=3,out.width="280px",fig.align='center'}
knitr::include_graphics("img/latentvars.PNG")
```

\fontsize{7.5pt}{12}\selectfont

- In psychology, we are mainly interested in **latent variables** = phenomena that we cannot directly observe, but we can estimate from 1+ **observed indicators** (e.g., 10-item scale measuring anxiety)

- Are we allowed to do that? Yes (let's say yes), provided that we trust the indicator **construct validity** = their relationship with the latent variable they claim to measure

- **SEM** allow to evaluate that by ***quantifying*** **the latent variables** and their relationships with observed indicators

## Structural what!?

\fontsize{8.5pt}{12}\selectfont \color{violet}
Structural equation modeling (SEM) \newline = multivariate *linear* models formalized by **systems of equations** \newline \color{black}

\begincols
  \begincol{.5\textwidth}

\fontsize{7pt}{12}\selectfont
**Linear models** (LM): determining the link between a dependent and 1+ independent variables through a **single equation** like: \color{violet} $PERF = \beta_1IQ + \beta_2ANX + \epsilon$ \color{black}
```{r echo=FALSE,warning=FALSE,message=FALSE}
library(DiagrammeR); library(DiagrammeRsvg); library(rsvg)

tmp <- grViz(
'digraph boxes_and_circles {
  graph [layout = neato]
  node [shape = box]
  x1 [label = <IQ> pos="-2,0.5!"]
  x2 [label = <ANX> pos="-2,-0.5!"]
  y [label = <PERF> pos="0,0!"]
  node [shape = plaintext]
  e [label = <&epsilon;> pos="1,0!"]
  # edges
  x1->y [label = <&beta;<SUB>1</SUB>>]
  x2->y [label = <&beta;<SUB>2</SUB>>]
  e->y}')

# Convert to SVG, then save as png
tmp = export_svg(tmp)
tmp = charToRaw(tmp) # flatten
rsvg_png(tmp, "img/lm.png") # saved graph as png in current working directory
```
```{r , echo = FALSE, out.width = "130px",fig.align="center"}
knitr::include_graphics("img/lm.png")
```

LM can only predict **one dependent variable at a time**, being either *univariate* (without predictors, i.e., intercept-only) or *bivariate* (with predictors).

  \endcol
\begincol{.5\textwidth}

\fontsize{7pt}{12}\selectfont
**SEM** allow to simultaneously model multiple ~~dependent~~ *endogenous* variables \newline with a **system of equations** like: \color{violet}

$$ \begin{cases} ANX = \beta_{1}SEFF + \epsilon_2 \\\\ PERF = \beta_{2}SEFF + \beta_{3}ANX + \epsilon_3 \end{cases} $$

```{r echo=FALSE}
library(DiagrammeR); library(DiagrammeRsvg); library(rsvg)

tmp <- grViz(
'digraph boxes_and_circles {
  graph [layout = neato]
  node [shape = box]
  x1 [label = <SEFF> pos="-2,0!"]
  y2 [label = <ANX> pos="0,0.5!"]
  y3 [label = <PERF> pos="0,-0.5!"]
  node [shape = plaintext]
  e2 [label = <&epsilon;<SUB>2</SUB>> pos="1,0.5!"]
  e3 [label = <&epsilon;<SUB>3</SUB>>pos="1,-0.5!"]
  # edges
  x1->y2 [label = <&beta;<SUB>1</SUB>>]
  x1->y3 [label = <&beta;<SUB>2</SUB>>]
  y2->y3 [label = <&beta;<SUB>3</SUB>>]
  e2->y2
  e3->y3}')

# Convert to SVG, then save as png
tmp = export_svg(tmp)
tmp = charToRaw(tmp) # flatten
rsvg_png(tmp, "img/sem1.png") # saved graph as png in current working directory
```
```{r , echo = FALSE, out.width = "130px",fig.align="center"}
knitr::include_graphics("img/sem1.png")
```

  \endcol
\endcols

## The SEM family

\fontsize{8pt}{12}\selectfont
SEM = broad family of statistical models within which LM, ANOVA, and even correlation can be included.

Particularly, 2 main sub-families can be distinguished based on whether \newline __latent variables__ are included in the model or not: \fontsize{7pt}{12}\selectfont

- __Path analysis__: multivariate linear models with observed variables only

- __Confirmatory factor analysis (CFA)__: multivariate linear models with both observed and latent variables

```{r , echo = FALSE, out.width = "270px",fig.align='center'}
knitr::include_graphics("img/SEMfamily.PNG")
```

\fontsize{5pt}{12}\selectfont Source: Beaujean (2014)

## Path models & path analysis

\fontsize{8pt}{12}\selectfont
**Path models/diagrams** = multivariate models with observed variables only \newline = pictorial representations (*diagrams*) of a theory of variable relationships

```{r , echo = FALSE, out.width = "160px",fig.align="center"}
knitr::include_graphics("img/sem1.png")
```

**Paths** = arrows (*edges*) linking the variables (*nodes*) in a model

**Path analysis** = analysis of multivariate relationships between observed variables ('*quantification of the paths accounting for all other paths and errors*')

## Latent factors & CFA

\begincols
  \begincol{.67\textwidth}
  
\fontsize{7pt}{12}\selectfont

- \color{blue} __Observed/Manifest variable (OV)__ \color{black} \newline variable that is directly observable (e.g., height, heart rate, item responses)

- \color{magenta} __Latent variable/factor (LV)__ \color{black} \newline variable that is *not* directly observable (e.g., anxiety, intelligence), but can be indexed by \newline one or more observed variables

- In SEM, \color{blue}**OV**s \color{black} are represented by \color{blue}squares/rectangles \color{black} and indexed with \color{blue}lower case letters (e.g., *x*)\color{black}, whereas \color{magenta}**LV**s \color{black} are represented by \color{magenta}circles/ellipses \color{black} \newline and indexed by the \color{magenta}Greek letter $\eta$ \color{black}

```{r , echo = FALSE, out.width = "130px"}
knitr::include_graphics("img/latobs.PNG")
```
  
  \endcol
\begincol{.4\textwidth}

Confirmatory factor analysis (CFA)

\fontsize{7pt}{12}\selectfont
= analysis of the relationships (*factor loadings*) between a set of OVs and one or more LVs \newline

CFA uses **latent variable models** to *form* or *quantify* LVs \newline and their relationships with OVs \newline (evaluation of **construct validity**) \newline

```{r echo=FALSE,message=FALSE,warning=FALSE, out.width = "100px"}
library(DiagrammeR); library(DiagrammeRsvg); library(rsvg)
tmp <- grViz(
'digraph boxes_and_circles {
  graph [layout = neato]
  node [shape = oval]
  E1 [label = <&eta;<SUB>1</SUB>> pos="0.5,0!", style=filled, fillcolor="#ED028C"]
  node [shape = box]
  x1 [label = <x<SUB>1</SUB>> pos="2,1!", style=filled, fillcolor="#00AEEF"]
  x2 [label = <x<SUB>2</SUB>> pos="2,0!", style=filled, fillcolor="#00AEEF"]
  x3 [label = <x<SUB>3</SUB>> pos="2,-1!", style=filled, fillcolor="#00AEEF"]
  node [shape = plaintext]
  e1 [label = <&epsilon;<SUB>1</SUB>> pos="3,1!"]
  e2 [label = <&epsilon;<SUB>2</SUB>>pos="3,0!"]
  e3 [label = <&epsilon;<SUB>3</SUB>>pos="3,-1!"]
  # edges
  E1->x1 
  E1->x2 
  E1->x3 
  e1->x1
  e2->x2
  e3->x3}')
# Convert to SVG, then save as png
tmp = export_svg(tmp)
tmp = charToRaw(tmp) # flatten
rsvg_png(tmp, "img/sem2.png") # saved graph as png in current working directory
knitr::include_graphics("img/sem2.png")
```

  \endcol
\endcols

## SEM: Measurement & Structural model

\fontsize{7pt}{12}\selectfont
To properly talk about 'full SEM' (or just SEM), we need both OVs and LVs

```{r echo=FALSE,message=FALSE,warning=FALSE}
library(DiagrammeR); library(DiagrammeRsvg); library(rsvg)
tmp <- grViz(
'digraph boxes_and_circles {
  graph [layout = neato]
  node [shape = oval]
  E1 [label = <&eta;<SUB>1</SUB>> pos="-0.5,0!"]
  E2 [label = <&eta;<SUB>2</SUB>> pos="0.5,0!"]
  node [shape = box]
  x1 [label = <x<SUB>1</SUB>> pos="-2,1!"]
  x2 [label = <x<SUB>2</SUB>> pos="-2,0!"]
  x3 [label = <x<SUB>3</SUB>> pos="-2,-1!"]
  x4 [label = <x<SUB>4</SUB>> pos="2,1!"]
  x5 [label = <x<SUB>5</SUB>> pos="2,0!"]
  x6 [label = <x<SUB>6</SUB>> pos="2,-1!"]
  node [shape = plaintext]
  e1 [label = <&epsilon;<SUB>1</SUB>> pos="-3,1!"]
  e2 [label = <&epsilon;<SUB>2</SUB>>pos="-3,0!"]
  e3 [label = <&epsilon;<SUB>3</SUB>>pos="-3,-1!"]
  e4 [label = <&epsilon;<SUB>4</SUB>> pos="3,1!"]
  e5 [label = <&epsilon;<SUB>5</SUB>>pos="3,0!"]
  e6 [label = <&epsilon;<SUB>6</SUB>>pos="3,-1!"]
  z2 [label = <&zeta;<SUB>2</SUB>>pos="0.5,0.8!"]
  # edges
  E1->x1 [color="#ED028C"]
  E1->x2 [color="#ED028C"] 
  E1->x3 [color="#ED028C"]
  E2->x4 [color="#ED028C"]
  E2->x5 [color="#ED028C"] 
  E2->x6 [color="#ED028C"] 
  E1->E2 [color="blue"] 
  
  e1->x1
  e2->x2
  e3->x3
  e4->x4
  e5->x5
  e6->x6
  z2->E2}')
# Convert to SVG, then save as png
tmp = export_svg(tmp)
tmp = charToRaw(tmp) # flatten
rsvg_png(tmp, "img/sem3.png") # saved graph as png in current working directory
```
```{r , echo = FALSE, out.width = "180px",fig.align="center"}
knitr::include_graphics("img/sem3.png")
```

A SEM consists of two parts:

1. \color{blue}**Structural model**\color{black}: Regression-like relationships among the variables, working similar to *path analysis*

2. \color{magenta}**Measurement model** (or latent variable model)\color{black}: Relationships between OVs and LVs, working a little differently

\fontsize{5pt}{12}\selectfont
Notes: \newline
In this sense, we may say that a CFA model is a 'full SEM' whereas a path model is not \newline
A CFA is a SEM with just the measurement part (without the structural model)

## A new classification: From in/dependent to exo/endogenous variables

\fontsize{7pt}{12}\selectfont
In both SEM (e.g., CFA) and path models, the classic independent vs. dependent classification is replaced with a more meaningful one:

```{r echo=FALSE,message=FALSE,warning=FALSE}
library(DiagrammeR); library(DiagrammeRsvg); library(rsvg)
tmp <- grViz(
'digraph boxes_and_circles {
  graph [layout = neato]
  node [shape = oval]
  E1 [label = <&eta;<SUB>1</SUB>> pos="-0.5,0!" color="blue"]
  E2 [label = <&eta;<SUB>2</SUB>> pos="0.5,0!" color="#ED028C"]
  node [shape = box]
  x1 [label = <x<SUB>1</SUB>> pos="-2,1!" color="#ED028C"]
  x2 [label = <x<SUB>2</SUB>> pos="-2,0!" color="#ED028C"]
  x3 [label = <x<SUB>3</SUB>> pos="-2,-1!" color="#ED028C"]
  x4 [label = <x<SUB>4</SUB>> pos="2,1!" color="#ED028C"]
  x5 [label = <x<SUB>5</SUB>> pos="2,0!" color="#ED028C"]
  x6 [label = <x<SUB>6</SUB>> pos="2,-1!" color="#ED028C"]
  node [shape = plaintext]
  e1 [label = <&epsilon;<SUB>1</SUB>> pos="-3,1!"]
  e2 [label = <&epsilon;<SUB>2</SUB>>pos="-3,0!"]
  e3 [label = <&epsilon;<SUB>3</SUB>>pos="-3,-1!"]
  e4 [label = <&epsilon;<SUB>4</SUB>> pos="3,1!"]
  e5 [label = <&epsilon;<SUB>5</SUB>>pos="3,0!"]
  e6 [label = <&epsilon;<SUB>6</SUB>>pos="3,-1!"]
  z2 [label = <&zeta;<SUB>2</SUB>>pos="0.5,0.8!"]
  # edges
  E1->x1 
  E1->x2 
  E1->x3
  E2->x4
  E2->x5
  E2->x6
  E1->E2
  e1->x1
  e2->x2
  e3->x3
  e4->x4
  e5->x5
  e6->x6
  z2->E2}')
# Convert to SVG, then save as png
tmp = export_svg(tmp)
tmp = charToRaw(tmp) # flatten
rsvg_png(tmp, "img/sem4.png") # saved graph as png in current working directory
```
```{r , echo = FALSE, out.width = "180px",fig.align="center"}
knitr::include_graphics("img/sem4.png")
```

\fontsize{7.5pt}{12}\selectfont

- \color{blue}**Exogenous variables**\color{black}: variables (both OVs and LVs) without a direct 'cause' from inside the model (predictors), without error estimate \newline

- \color{magenta}**Endogenous variables**\color{black}: variables (both OVs and LVs) directly 'caused' from inside the model (predictors & outcomes), with error estimate $\epsilon$ (OV) or $\zeta$ (LV)

## A new starting point: From dataset columns to covariance matrices {#startingPoint}

\begincols
  \begincol{.5\textwidth}

\fontsize{7pt}{12}\selectfont
The starting point of LM(ER) is a vector (or a set of vectors) of variable values, usually corresponding to one or more columns from a dataset.
```{r , echo = FALSE}
data( earlymath, package = "ADati" )
rownames(earlymath) <- 1:nrow(earlymath)
df <- earlymath[,2:5]
```
```{r comment=NA}
head(df,4)
```

  \endcol
\begincol{.5\textwidth}

\fontsize{7pt}{12}\selectfont
The starting point of SEM and path models is the **covariance matrix of the observed variables**. \newline \color{blue} `r fontawesome::fa(name = "microscope", fill = "blue", height = "1em")` $cov(x,y) =\sum(x_i - \overline{x})(y_i-\overline{y})/N$ \color{black}
```{r eval=FALSE}
cov(df[,c("MAT","QI","WM","STM")])
```
```{r echo=FALSE,comment=NA}
round(cov(df[,c("MAT","QI","WM","STM")]),2)
```

  \endcol
\endcols

\color{white}_ \newline \color{black}

\fontsize{7.5pt}{12}\selectfont
SEM estimate a number of parameters $\theta$ so that the **implied covariance matrix** $\hat\sum(\theta)$ (i.e., the covariance matrix predicted by the model based on the parameter estimates) is as close as possible to the **sample covariance matrix** $S$ \newline

\fontsize{6pt}{12}\selectfont \color{blue}

`r fontawesome::fa(name = "microscope", fill = "blue", height = "1em")` Note: even the model parameters are estimated within **matrices of parameters** `r fontawesome::fa(name = "face-flushed", height = "1em")`

## Covariance & correlation

\fontsize{7.5pt}{12}\selectfont

- __Variance__ = Expected value of the **squared deviation from the mean** of a random variable, or degree to which it deviates from its expected value \newline \color{blue}
`r fontawesome::fa(name = "microscope", fill = "blue", height = "1em")` $var(x) = \sigma^2_x =\frac{\sum(x_i -\overline{x})^2}{N}$ \color{black}

- __Covariance__ = Measure of the **joint variability** of two random variables, or Degree to which they tend to deviate from their expected values in similar ways, either directly (positive cov) or inversely (negative cov), whose value depends on the variable scales of measurement (from $-\infty$ to $+\infty$) \newline \color{blue}
`r fontawesome::fa(name = "microscope", fill = "blue", height = "1em")` $cov(x_1,x_2) =\frac{\sum(x_{1i} - \overline{x_1})(x_{2i}-\overline{x_2})}{N}$ \color{black}

- __Correlation__ = standardized covariance of two random variables \newline Correlation ranges from -1 (perfectly negative) to +1 (perfectly positive) \newline \color{blue} `r fontawesome::fa(name = "microscope", fill = "blue", height = "1em")` $cor(x_1,x_2) = \frac{cov(x_1,x_2)}{\sigma^2_{x_1}\sigma^2_{x_2}}$

```{r echo=FALSE,fig.width=9,fig.height=2,out.width="300px"}
par(mfrow=c(1,5))
x <- rnorm(mean=100,n=50,sd=1)
hist(x,main=paste("var =",round(var(x))),breaks=30,xlim=c(80,120))
x <- rnorm(mean=100,n=50,sd=10)
hist(x,main=paste("var =",round(var(x))),breaks=30,xlim=c(80,120))
x1 <- x
x2 <- x + rnorm(n=50,sd=10)
plot(x1~x2,
     main=paste("cov =",round(cov(x1,x2),2),"\ncor =",
                           round(cor(x1,x2),2)))
abline(lm(x1~x2),col="red")
x2 <- -x + rnorm(n=50,sd=10)
plot(x1~x2,
     main=paste("cov =",round(cov(x1,x2),2),"\ncor =",
                           round(cor(x1,x2),2)))
abline(lm(x1~x2),col="red")
x2 <- x + rnorm(n=50,sd=1000)
plot(x1~x2,
     main=paste("cov =",round(cov(x1,x2),2),"\ncor =",
                           round(cor(x1,x2),2)))
abline(lm(x1~x2),col="red")
```

\fontsize{5pt}{12}\selectfont
Important notes: \color{blue} $cov(x_1,x_1) = var(x_1)$ ; $cor(x_1,x_1)=1$

## Covariance matrix (*S*)

\begincols
  \begincol{.45\textwidth}

\fontsize{7.5pt}{12}\selectfont
Given a set of *p* variables, we can define the covariance matrix:

$$S = \begin{bmatrix} s_{11}~~...,~~s_{1j}~~...~~s_{1p} \\ ...~~...~~...~~...~~... \\ s_{i1}~~...~~s_{ij}~~...~~s_{ip} \\ ...~~...~~...~~...~~... \\ s_{p1}~~...~~s_{pj}~~...~~s_{pp}\end{bmatrix}$$

```{r eval=FALSE}
cov(df[,c("MAT","QI","WM","STM")])
```

\fontsize{10.5pt}{12}\selectfont

```{r echo=FALSE,comment=NA}
round(cov(df[,c("MAT","QI","WM","STM")]),2)
```

  \endcol
\begincol{.6\textwidth}

\fontsize{10.5pt}{12}\selectfont

Properties of the covariance matrix:

1. __Symmetrical__: $s_{ij} = s_{ji}$

2. The **main diagonal** shows the **variances** (= covariance between each variable and itself)

  \endcol
\endcols

\color{white}_\color{black} \newline

\fontsize{7.5pt}{12}\selectfont
SEM estimate a number of parameters $\theta$ so that the **implied covariance matrix** $\hat\sum(\theta)$ (i.e., the covariance matrix predicted by the model based on the parameter estimates) is as close as possible to the **sample covariance matrix** $S$ 

## That's all for now!

\fontsize{8pt}{12}\selectfont 
__Questions?__ \newline

__Homework__ (optional):

- read the slides presented today \newline and write in the Moodle forum if you have any doubts

- **exe`r fontawesome::fa(name = "r-project", fill="#3333B2",height = "1em")`cises 12-13** from `exeRcises.pdf` \newline \newline

\fontsize{6pt}{12}\selectfont ____ \newline 
For each exercise, the solution (or one of the possible solutions) can be found in dedicated chunk of commented code within the `exeRcises.Rmd` file

# Path analysis

## In the last episode...

\begincols
  \begincol{.4\textwidth}
  
\fontsize{8.5pt}{12}\selectfont 
__The problem__ \fontsize{7pt}{12}\selectfont  \newline
Psychosocial reality is complex: it's **multivariate** (3+ variables interacting at the same time) and involves **latent variables** (not directly measurable) \newline

\fontsize{8.5pt}{12}\selectfont 
__The solution__ \fontsize{7pt}{12}\selectfont  \newline
SEM allows to analyze the multivariate relationships among observed and latent variables through **systems of equations**:

$$ \begin{cases} ANX = \beta_{21}SEFF + \epsilon_2 \\\\ PERF = \beta_{31}SEFF + \beta_{32}ANX + \epsilon_3 \end{cases} $$

  \endcol
\begincol{.6\textwidth}

\fontsize{8.5pt}{12}\selectfont 
__SEM basics__ \fontsize{7pt}{12}\selectfont \newline
__- Observed__ ($x$) __vs latent variables__ ($\eta$) depending on whether can be directly measured or not \newline
__- Exogenous vs endogenous variables__ depending on whether directly caused inside the model or not \newline
__- Structural vs measurement model__ depending on whether focusing on structural relationships or construct validity of the observed indicators \newline \color{violet}
__- Path model__: SEM with observed variables only \color{black} \newline
__- CFA__ = SEM with measurement model only \newline
- Starting point of any SEM = **covariance matrix**

```{r echo=FALSE,message=FALSE,warning=FALSE}
library(DiagrammeR); library(DiagrammeRsvg); library(rsvg)
tmp <- grViz(
'digraph boxes_and_circles {
  graph [layout = neato]
  node [shape = oval]
  E1 [label = <&eta;<SUB>1</SUB>> pos="-0.5,0!"]
  E2 [label = <&eta;<SUB>2</SUB>> pos="0.5,0!"]
  node [shape = box]
  x1 [label = <x<SUB>1</SUB>> pos="-2,1!"]
  x2 [label = <x<SUB>2</SUB>> pos="-2,0!"]
  x3 [label = <x<SUB>3</SUB>> pos="-2,-1!"]
  x4 [label = <x<SUB>4</SUB>> pos="2,1!"]
  x5 [label = <x<SUB>5</SUB>> pos="2,0!"]
  x6 [label = <x<SUB>6</SUB>> pos="2,-1!"]
  node [shape = plaintext]
  e1 [label = <&epsilon;<SUB>1</SUB>> pos="-3,1!"]
  e2 [label = <&epsilon;<SUB>2</SUB>>pos="-3,0!"]
  e3 [label = <&epsilon;<SUB>3</SUB>>pos="-3,-1!"]
  e4 [label = <&epsilon;<SUB>4</SUB>> pos="3,1!"]
  e5 [label = <&epsilon;<SUB>5</SUB>>pos="3,0!"]
  e6 [label = <&epsilon;<SUB>6</SUB>>pos="3,-1!"]
  z2 [label = <&zeta;<SUB>2</SUB>>pos="0.5,0.8!"]
  # edges
  E1->x1 
  E1->x2 
  E1->x3
  E2->x4
  E2->x5
  E2->x6
  E1->E2
  e1->x1
  e2->x2
  e3->x3
  e4->x4
  e5->x5
  e6->x6
  z2->E2}')
# Convert to SVG, then save as png
tmp = export_svg(tmp)
tmp = charToRaw(tmp) # flatten
rsvg_png(tmp, "img/sem5.png") # saved graph as png in current working directory
```
```{r , echo = FALSE, out.width = "170px",fig.align="center"}
knitr::include_graphics("img/sem5.png")
```

  \endcol
\endcols

## Path models: SEM with observed variables

\fontsize{8pt}{12}\selectfont
A path model is a pictorial representation (*diagram*) of a theory of variable relationships. Path analysis is widely used to model complex multivariate relationships (e.g., *mediation models*).

- Path analysis tests models of ***causal*** **relationships*** among observed variables

- All variables in path analysis are **observed**

- Path analysis uses **systems of regression equations** \newline \newline

\fontsize{5pt}{12}\selectfont
*Note: Within path analysis (and SEM) we assume that the relationships are _causal_, but this is not necessarily true (e.g., observational studies) &rightarrow; causation requires experimental manipulation, control group, etc.

## Case study: Early mathematical abilities `r fontawesome::fa(name = "calculator",fill="#3333B2", height = "0.8em")`

\begincols
  \begincol{.45\textwidth}

\fontsize{6.5pt}{12}\selectfont \color{white} _ \newline \newline \color{blue}

A sample of 120 first-grade children (58 females; mean age: 6 years, 3 months) was assessed over the following variables: \newline

  \endcol
\begincol{.6\textwidth}

```{r , echo = FALSE, out.width = "150px"}
knitr::include_graphics("img/earlyMath.png")
```

  \endcol
\endcols

\fontsize{6.5pt}{12}\selectfont  \color{blue}

- \color{blue}`MAT`: early mathematical abilities (e.g., comparison, classification) measured with the Early Numeracy Test

- \color{blue}`QI`: intelligence level measured with the Wechsler Intelligence Scale for Children (WISC-III)

- \color{blue}`WM`: working memory capacity measured with the Backward word recall task

- \color{blue}`STM`: short-term memory capacity measured with the Forward word recall task

- \color{blue}`ANS`: approximate number system = innate system for approximate quantity manipulation (e.g., approximate computations, comparing 2+ sets of elements \newline without counting), measured with several tasks

\fontsize{7pt}{12}\selectfont
RQ: **How much can MAT abilities be attributed to memory & ANS?** \newline

\fontsize{5pt}{12}\selectfont \color{black} Source: Pastore (2021). Analisi dei dati in ambito di comunità  

## Data exploration

\fontsize{7pt}{12}\selectfont
First, let's explore the data: \fontsize{5.5pt}{12}\selectfont
```{r , echo = FALSE}
rm(list=ls())
```
```{r , eval = FALSE}
library(devtools); install_github("https://github.com/masspastore/ADati") # install ADati pkg
```
```{r ,comment=NA}
data( earlymath, package = "ADati" ) # loading earlymath dataset from ADati pkg
head(earlymath,3) # showing first 3 rows

summary(earlymath[,c(2,4:ncol(earlymath))]) # summarizing variables (not showing QI due to space limits)

round( cor(earlymath[,2:ncol(earlymath)]), 2) # correlations
```
## Linear model as a path diagram

\fontsize{7pt}{12}\selectfont
Let's fit a multiple linear model: \color{blue} $MAT = \beta_0 + \beta_1WM + \beta_2STM + \beta_3ASN + \epsilon$ \color{black}
```{r }
lm.fit <- lm(MAT ~ WM + STM + ANS, data = earlymath) # fitting LM
```

\begincols
  \begincol{.7\textwidth}

\fontsize{6.5pt}{12}\selectfont
```{r eval=FALSE}
summary(lm.fit)$coefficients # LM regression table
```
```{r echo=FALSE,comment=NA}
round(summary(lm.fit)$coefficients,2) # LM regression table
```

  \endcol
\begincol{.3\textwidth}

Residual variance $\sigma^2$:
```{r comment=NA}
summary(lm.fit)$sigma^2
```

\color{white}_ \newline

  \endcol
\endcols

This model can be graphically represented as a path diagram and further simplified by removing the \color{red} intercept $\beta_0$ \color{black} (note: **triangles represent constants**)
```{r , echo = FALSE, out.width = "250px"}
knitr::include_graphics("img/lm2path.png")
```

\color{blue} How many parameters? **Five**: Intercept, 3 slopes, residual variance

## Multivariate path models {#multipath}

\fontsize{7pt}{12}\selectfont
In the previous example, we only considered **bivariate relationships** (i.e., 2 variables at a time, controlling for other variables). But what if we include `IQ` as a common predictor of both `WM` and `MAT`? We would have 3 variables interacting at the same time.

\begincols
  \begincol{.5\textwidth}

```{r , echo = FALSE, out.width = "160px"}
knitr::include_graphics("img/sem6_colvar.png")
```

  \endcol
\begincol{.5\textwidth}

Both \color{magenta} `MAT` and `WM` are **endogenous variables** \color{black} because they receive 1+ arrow(s) and have error variance \color{red} $\sigma^2$\color{black}. 

In contrast, \color{cyan} `STM`, `ANS`, and `QI` are **exogenous variables** \color{black} because they do not receive any arrow and have no errors. \newline

A single LM equation is insufficient to describe this model. We need 2 separated equations: one for each variable that depends upon another variable \newline

  \endcol
\endcols

Path analysis (and SEM) uses **one equation per endogenous variable**: \color{blue}
$$ \begin{cases} MAT_1 = \beta_{12}WM_2 + \beta_{13}STM_3 + \beta_{14}ANS_4 + \beta_{15}QI_5 + \epsilon_1 \\ WM_2 = \beta_{25}QI_5 + \epsilon_2 \end{cases} $$

## Graphical notation (1/3): Error terms

\fontsize{7pt}{12}\selectfont \color{blue}
$$ \begin{cases} MAT_1 = \beta_{12}WM_2 + \beta_{13}STM_3 + \beta_{14}ANS_4 + \beta_{15}QI_5 + \color{red}{\epsilon_1} \\ WM_2 = \beta_{25}QI_5 + \color{red}{\epsilon_2} \end{cases} $$ \color{black}

__Errors__ = *residuals* or *disturbances* = discrepancy between observed and predicted values (as in LM!), they represent something *unexplained* = **exogenous** \newline and *not directly observable* = **latent**

\begincols
  \begincol{.5\textwidth}

```{r , echo = FALSE, out.width = "160px"}
knitr::include_graphics("img/sem6_colerror.png")
```

\fontsize{6.5pt}{12}\selectfont $\sigma^2$ = variance of a variable error (residual var.)

  \endcol
\begincol{.5\textwidth}

\fontsize{6.5pt}{12}\selectfont 
Alternative ways to represent errors: some highlight their latent nature (#1 and #2), some highlight their variance (#1 and #4), and some highlight both (#1). \color{red} You need to know them 
```{r , echo = FALSE, out.width = "120px"}
knitr::include_graphics("img/sem_errors.png")
```

\color{blue} \fontsize{5pt}{12}\selectfont 
In this course, we will mainly use notation #4.

  \endcol
\endcols

## Graphical notation (2/3): Arrows & coefficients {#pathcoeff}

\fontsize{7pt}{12}\selectfont \color{blue}
$$ \begin{cases} MAT_1 = \color{red}\beta_{12}\color{black}WM_2 + \color{red}\beta_{13}\color{black}STM_3 + \color{red}\beta_{14}\color{black}ANS_4 + \color{red}\beta_{15}\color{black}QI_5 + \epsilon_1 \\ WM_2 = \color{red}\beta_{25}\color{black}QI_5 + \epsilon_2 \end{cases} $$ \color{black}

__Arrows__ = *relationships* between 2 variables (*paths* or *slopes*) or between a variable and itself (*residual variance*), such that we do not include an arrow when a relationship is not expected (e.g., between `QI` and `ASN`) &rightarrow; path models are *complete*


\begincols
  \begincol{.5\textwidth}

```{r , echo = FALSE, out.width = "160px"}
knitr::include_graphics("img/sem6_colpath.png")
```

  \endcol
\begincol{.5\textwidth}

\fontsize{6.5pt}{12}\selectfont 
__How to index variables and paths:__

- __Variables__ are indexed from the one receiving most arrows ($MAT_1$) to the last exogenous variable ($QI_5$)

>- __Path coefficients $\beta$__ are indexed by firstly reporting the index of the endogenous variable and then that of the exogenous variable \newline

  \endcol
\endcols

\fontsize{7pt}{12}\selectfont
__From plot to equations__: endogenous v. **~** sum of all linked exogenous v. + error 

## Graphical notation (3/3): Covariances

\fontsize{7pt}{12}\selectfont \color{blue}
$$ \begin{cases} MAT_1 = \beta_{12}WM_2 + \beta_{13}STM_3 + \beta_{14}ANS_4 + \beta_{15}QI_5 + \epsilon_1 \\ WM_2 = \beta_{25}QI_5 + \epsilon_2 \\ \color{red}{Cov(ANS_4,QI_5)=\gamma_{ANS_4,QI_5}} \end{cases} $$ \color{black}

__Covariances__ = *non-directional (symmetric)* relationships between 2 ***exogenous*** v.

\begincols
  \begincol{.5\textwidth}

```{r , echo = FALSE, out.width = "160px"}
knitr::include_graphics("img/sem6_colcov.png")
```

  \endcol
\begincol{.5\textwidth}

\fontsize{6.5pt}{12}\selectfont 

- Covariances are usually *not* reported in the system of equations, but they *can* be graphically represented with (rounded) **double-headed arrows**

- __Endogenous variables cannot covary but their errors $\epsilon$ can__

```{r , echo = FALSE, out.width = "120px", fig.align='center'}
knitr::include_graphics("img/covariances.png")
```

  \endcol
\endcols

## `r fontawesome::fa(name = "microscope", fill = "blue", height = "1em")` Clarification on covariance terms in SEM

\fontsize{7pt}{12}\selectfont
Covariances $\gamma$ are intrinsic relationships between observed variables (we saw that SEM are fitted on the covariance matrix of observed variables). \newline

In \color{blue} [slide #22](#pathcoeff) \color{black}, we saw that path models are assumed to be *complete* models (i.e., we don't include an arrow when a relationship is not expected). \newline 
However, this rule only applies to single-headed arrows (path coefficients $\beta$), whereas it **does not applies to the covariances $\gamma$**. \newline
Covariances $\gamma$ are always there, whether you estimate them or not. In contrast, if we don't specify a path coefficient $\beta$ between two variables, the two variables can only covariate but they are not in a symmetric relationship. \newline

&rightarrow; the explicit inclusion of covariances $\gamma$ does not affect the estimation of the path coefficients $\beta$, it only means that the models estimate the covariance parameter and its standard errror, but we will see this later... \newline

\color{blue}For the exam, you only need to know that double-headed arrows between two variables represent covariances, and that endogenous variables cannot covary but their errors can

## Regression, partial correlation, and path coefficients

\fontsize{7pt}{12}\selectfont
**Path coefficients** (single-headed arrows) are **partial regression coefficients** (*slopes*): as in LM, they index the *effect* of *x* on *y* by controlling for (i.e., after removing the effect of) other predictors, which are fixed to zero

**Covariances** between two exogenous variables (double-headed arrows), or between the errors of two endogenous variables, are **partial correlation coefficients**: they express the relationship between two variables by controlling for (i.e., after removing the effect of) all other correlated variables, which are fixed to zero \newline

\fontsize{6pt}{12}\selectfont \color{blue}
For instance, the figure below (source: Beaujeau, 2014) shows a path model of a partial correlation. Variables $y_1$ and $y_2$ are not allowed to covary since they are endogenous, but their errors are allowed to do so. Thus, the $c$ coefficient is the relationship between $y_1$ and $y_2$ after removing the effect of $x_1$ from both variables.

```{r , echo = FALSE, out.width = "130px", fig.align='center'}
knitr::include_graphics("img/partcorr.png")
```

## Graphical notation: Recap

```{r , echo = FALSE, out.width = "200px",fig.align='center'}
knitr::include_graphics("img/graphNotation.PNG")
```

## `r fontawesome::fa(name = "microscope", fill = "blue", height = "1em")` Tracing rules & path coefficients

\begincols
  \begincol{.25\textwidth}

```{r , echo = FALSE, out.width = "80px"}
knitr::include_graphics("img/tracingRules.png")
```

  \endcol
\begincol{.75\textwidth}

\fontsize{6pt}{12}\selectfont \color{blue}
Sewall Wright (1889–1988): US geneticist that firstly developed rules for how to estimate values for a path
model’s coefficients by tracing the paths within it (i.e., path analysis). \newline

\color{black}
\fontsize{7pt}{12}\selectfont __Tracing rules__ = rules to estimate the covariance between 2 variables by *summing* the appropriate connecting paths: \fontsize{6.5pt}{12}\selectfont

1. Trace all paths between 2 variables multiplying all coefficients

2. Start by going backwards along single-headed arrows, no loops

3. Once you start going forward, you cannot no longer go back

4. Each path can only include one double-headed arrow

\fontsize{7pt}{12}\selectfont
Starting from *observed covariances* (or correlations), we can compute the value of path coefficients. For instance, to compute path \color{blue} *a* \color{black} starting from the observed correlations between \color{blue} $X_1$ \color{black} and \color{blue} $Y$ \color{black} (e.g., \color{blue}*r* = .70\color{black}), between \color{blue} $X_1$ \color{black} and \color{blue} $X_2$ \color{black} (e.g., \color{blue}*d* = .24\color{black}), and between \color{blue} $X_1$ \color{black} and \color{blue} $X_3$ \color{black} (e.g., \color{blue}*e* = .20\color{black}): \newline

  \endcol
\endcols

\fontsize{7pt}{12}\selectfont \color{blue}
$r_{X_1,Y}$ = a + db + ec \color{black}&rightarrow; \color{blue}.70 = a + .24c + .20b \color{black}&rightarrow; \color{blue}a = .70 - .24c - .20b

## Standardized vs. *Un*standardized coefficients

\fontsize{7.5pt}{12}\selectfont 
Path coefficients are **partial regression coefficients** (relationship between an exogenous *x* and an endogenous variable *y*, controlling for all other exogenous variable affecting *y*). Similar to LM, they can be either *un*standardized or standardized:

- __Unstandardized coefficients__ are obtained when the model is fitted on the variables expressed in their *natural metrics* (raw score units of measurement) \newline &rightarrow; useful when raw score units are meaningful (e.g., age, meters, bpm) and when comparing the same variable relationship across samples

- __Standardized coefficients (ranging from -1 to 1)__ are obtained when the model is fitted on standardized variables (i.e., variables transformed into \color{blue} ***z-scores***: $z_{x_i} = (x_i - \overline{x})/s_x$\color{black}) &rightarrow; useful to compare coefficients within the same model and/or the same sample \newline 

\color{blue}

`r fontawesome::fa(name = "microscope", fill = "blue", height = "1em")` To standardize an unstandardized coefficient: $b^* = b \times(s_Y / s_X)$

`r fontawesome::fa(name = "microscope", fill = "blue", height = "1em")` To unstandardize a standardized coefficient: $b = b^* \times(s_X / s_Y)$

## That's all for now!

\fontsize{8pt}{12}\selectfont 
__Questions?__ \newline

__Homework__ (optional):

- read the slides presented today \newline and write in the Moodle forum if you have any doubts

- **exe`r fontawesome::fa(name = "r-project", fill="#3333B2",height = "1em")`cises 14-15** from `exeRcises.pdf` \newline \newline

\fontsize{6pt}{12}\selectfont ____ \newline 
For each exercise, the solution (or one of the possible solutions) can be found in dedicated chunk of commented code within the `exeRcises.Rmd` file

# Model fit

## In the last episodes...

\begincols
  \begincol{.4\textwidth}
  
\fontsize{8.5pt}{12}\selectfont 
__The problem & the solution__ \fontsize{7pt}{12}\selectfont  \newline
Reality is **multivariate** and involves **latent variables**; SEM allows to analyze them through **systems of equations**:
$$ \begin{cases} y_2 = \beta_{32}x_3 + \epsilon_2 \\ y_1 = \beta_{21}y_2 + \beta_{31}x_3 + \epsilon_1 \end{cases} $$
\fontsize{8.5pt}{12}\selectfont 
__SEM basics__ \fontsize{7pt}{12}\selectfont  \newline
- Observed ($x$) vs latent ($\eta$) \newline
- Exogenous vs endogenous \newline
- Structural vs measurement model  \newline \color{violet}
- Path model: obs. variables only \color{black} \newline
- CFA = measurement model only \newline
- Starting point of any SEM \newline = **covariance matrix**

  \endcol
\begincol{.6\textwidth}

\fontsize{8.5pt}{12}\selectfont \color{violet}
__Path analysis__ \fontsize{7pt}{12}\selectfont \color{black} \newline
Pictorial representation of a theory of (observed) variable relationship \newline

\fontsize{8.5pt}{12}\selectfont 
__Graphical notation__ \fontsize{7pt}{12}\selectfont \newline
- Variables: end.&rightarrow;with error; ex.&rightarrow;without error \newline
- Errors: always exogenous and latent \newline
- Path coefficients: single-headed arrows, *complete* \newline
- (Co)variances: double-headed arrows - they are always there, whether you estimate them or not \newline

\fontsize{8.5pt}{12}\selectfont 
__Path coefficients__ \fontsize{7pt}{12}\selectfont \newline
= partial regression coefficients (slopes) \newline
either **unstandardized** (computed from raw variables, depending on the variable scale of measurement) or **standardized** (computed from standardized variables, ranging from -1 to 1)

  \endcol
\endcols

## Case study: Early mathematical abilities `r fontawesome::fa(name = "calculator",fill="#3333B2", height = "0.8em")`

\begincols
  \begincol{.45\textwidth}

\fontsize{6.5pt}{12}\selectfont \color{white} _ \newline \newline \color{blue}

A sample of 120 first-grade children (58 females; mean age: 6 years, 3 months) was assessed over the following variables: \newline

  \endcol
\begincol{.6\textwidth}

```{r , echo = FALSE, out.width = "150px"}
knitr::include_graphics("img/earlyMath.png")
```

  \endcol
\endcols

\fontsize{6.5pt}{12}\selectfont  \color{blue}

- \color{blue}`MAT`: early mathematical abilities (e.g., comparison, classification) measured with the Early Numeracy Test

- \color{blue}`QI`: intelligence level measured with the Wechsler Intelligence Scale for Children (WISC-III)

- \color{blue}`WM`: working memory capacity measured with the Backward word recall task

- \color{blue}`STM`: short-term memory capacity measured with the Forward word recall task

- \color{blue}`ANS`: approximate number system = innate system for approximate quantity manipulation (e.g., approximate computations, comparing 2+ sets of elements \newline without counting), measured with several tasks

\fontsize{7pt}{12}\selectfont
RQ: **How much can MAT abilities be attributed to memory & ANS?** \newline

\fontsize{5pt}{12}\selectfont \color{black} Source: Pastore (2021). Analisi dei dati in ambito di comunità

## Data structure in multivariate analyses

\fontsize{7.5pt}{12}\selectfont
In SEM (including path analysis and CFA), data analyses are usually based on wide-form datasets with one row per participant:
```{r , comment = NA}
head(earlymath) # showing first 6 rows
```

Provided that we have **no missing data** (but there are ways to deal with that), such wide-form dataset is used by the model to automatically compute the **covariance matrix of observed variables**, which is the starting points to fit the models.

```{r , comment = NA}
cov(earlymath[,2:ncol(earlymath)])
```

\fontsize{6pt}{12}\selectfont \color{blue}
`r fontawesome::fa(name = "microscope", fill = "blue", height = "1em")` Note: since the covariance matrix is the starting point, many software (including R) can fit SEM directly on the covariance matrix

## Fitting a (bivariate) path model with R

\fontsize{7pt}{12}\selectfont
We will use the `lavaan` (\color{blue}*la*\color{black}tent \color{blue}*va*\color{black}riable \color{blue}*an*\color{black}alysis) package (\color{blue}[Rosseel, 2012](https://doi.org/10.18637/jss.v048.i02)\color{black}), which uses the `sem()` function to fit SEM with observed (path analysis) and/or latent variables.
```{r , warning=FALSE, message=FALSE}
library(lavaan)
```

Let's start with a bivariate model (with only one endogenous variable) to highlight the differences between path analysis and LM in the model specification:

\begincols
  \begincol{.4\textwidth}
  
__Linear model (LM)__

```{r , comment=NA,eval=FALSE}
# fitting model
fit.lm <- lm(MAT ~ WM + STM + ANS, 
             data = earlymath)
# parameter estimates
summary(fit.lm)$coefficients
```
```{r , comment=NA,echo=FALSE}
# fitting model
fit.lm <- lm(MAT ~ WM + STM + ANS, 
             data = earlymath)
# parameter estimates
s <- round( summary(fit.lm)$coefficients, 2)
rownames(s)[1] <- "(Int)"
s
```
```{r , comment=NA,eval=FALSE}
# residual variance sigma2
summary(fit.lm)$sigma^2
```
```{r , comment=NA,echo=FALSE}
# residual variance sigma2
round(summary(fit.lm)$sigma^2,2)
```

  \endcol
\begincol{.6\textwidth}

__Path model (PM)__

```{r , comment=NA,eval=FALSE}
# specifying model
mymodel <- 'MAT ~ WM + STM + ANS'
# fitting model to the data
fit.sem <- sem(model = mymodel, data = earlymath)
parameterestimates(fit.sem) # par. estimates
```
```{r , comment=NA,echo=FALSE}
# specifying model
mymodel <- 'MAT ~ WM + STM + ANS'
# fitting model to the data
fit.sem <- sem(model = mymodel, data = earlymath)
p <- parameterestimates(fit.sem) # par. estimates
p[,4:ncol(p)] <- round(p[,4:ncol(p)],2)
p
```

  \endcol
\endcols

## Path model summary

\fontsize{7pt}{12}\selectfont

\begincols
  \begincol{.5\textwidth}

\fontsize{6pt}{12}\selectfont  

```{r , comment=NA}
summary(fit.sem)
```

  \endcol
\begincol{.5\textwidth}

- __First lines__: info on convergence, parameter estimation method (ML), optimization (...), and number of estimated parameters (3 path coeff. + 1 residual variance)

- __Model test User Model__: info on model fit (we will see this later)

- __Parameter Estimates__: other info on parameter estimation method

- __Regressions__: path coefficients estimated by the **structural model**, with their **standard error**, **z-value**, and **p-value**

>- __Variances__: estimated **residual variance** of any **endogenous** variable

  \endcol
\endcols

## Path coefficient interpretation

\fontsize{6pt}{12}\selectfont

\begincols
  \begincol{.5\textwidth}
  
```{r , comment=NA}
summary(fit.sem)
```

  \endcol
\begincol{.5\textwidth}

- `Estimate` = estimated parameter/coefficient (predicted difference or change for a 1-unit increase in the exogenous variable)

- `Std.Error` = standard error (uncertainty) of the estimate

- `z-value` = test statistic computed as \newline \color{blue}*z = Estimate/Std.Error* \color{black}

- `P(>|z|)` = *p* corresponding to the *t*-value \color{white}spa \color{black} with \color{blue} *No. Obs. $-$ No. Coeff. $-$ 1* \newline \color{white}  \color{white}space \color{black} degrees of freedom 

>- Here, for instance: \newline - a 1-unit increase in `WM` predicts an increase in `MAT` by 0.812 units \newline - the residual variance of `MAT` is 60.787 \newline - *p*-values suggest that all path coefficients are significant

  \endcol
\endcols

## Path model vs. linear model estimates

\fontsize{7pt}{12}\selectfont
We can see that the coefficients estimated by the path model are very similar to those estimated with LM:
```{r , comment=NA,eval=FALSE}
coef(fit.lm); summary(fit.lm)$sigma^2 # LM estimate
```
```{r , comment=NA,echo=FALSE}
round(c( coef(fit.lm), sigma2 = summary(fit.lm)$sigma^2 ) ,3) # LM estimates 
```
```{r , comment=NA}
coef(fit.sem) # path model estimates
```

__They are the same, but where is the intercept?__

In SEM, intercepts are usually not considered as 'direct' model parameters. To estimate them, we need to set `meanstructure = TRUE`
```{r , comment=NA}
fit.sem <- sem(model = mymodel, data = earlymath, meanstructure= TRUE)
coef(fit.sem) # Here's the intercept!
```

Note: Path analysis coefficients can be interpreted identically to LM coefficients

## Hands on `r fontawesome::fa(name = "r-project", fill="#3333B2",height = "1em")` (part 1)

\fontsize{6.5pt}{12}\selectfont

1. Open the `earlymath` dataset from the `ADati package`

```{r , eval=FALSE}
# how to install the ADati package:
library(devtools) # install and open the devtools package
install_github("https://github.com/masspastore/ADati") # install the ADati pkg
data(earlymath, package = "ADati") # load earlymath dataset from ADati pkg
```

2. Fit a linear model `lm1` predicting `MAT` by `WM`, `STM`, `ANS`, and `QI`

3. Fit a path model `pm1` with the same 'outcome' and 'predictor' variables

4. Print, interpret, and compare the parameters estimated by both models

5. Inspect the predicted covariance matrix by running \color{blue} `inspect(pm1,"estimates")$psi[2:5,2:5]` \color{black} and compare it with the observed covariance matrix of exogenous variables \color{blue} `cov(earlymath[,c("WM","STM","ANS","QI")])` \color{black}

6. Standardize all variables (\color{blue}$z_{x_i} = (x_i - \overline{x}) / s_x$\color{black}), re-fit the same model (call it `pm1.z`), and print the estimated parameters

7. Standardize the parameters estimated by the original model `pm1` by running \color{blue}`standardizedsolution(pm1)` \color{black} and compare the output with that of `parameterestimates(pm1.z)`

## *Un*standardized vs. standardized solution

\fontsize{7pt}{12}\selectfont
In SEM (including path analysis and CFA), we refer to the ***un*****standardized solution** when the parameters are unstandardized, i.e., they are estimated from unstandardized variables and their size depends on the measurement scale of each variable
```{r , echo=FALSE,warning=FALSE,message=FALSE}
rm(list=ls())
data(earlymath, package = "ADati") # load earlymath dataset from ADati pkg
library(lavaan)
mymodel <- 'MAT ~ WM + STM + ANS + QI'
pm1 <- sem(model = mymodel, data = earlymath)
```
```{r , comment=NA}
# unstandardized solution
parameterestimates(pm1)[1:5,]
```

In contrast, we refer to the **standardized solution** when the parameters are standardized, i.e., they range from -1 to +1 because they have been either estimated from standardized variables or transformed into standardized coefficient after estimation
```{r , comment=NA}
# standardized solution
standardizedsolution(pm1)[1:5,]
```

## `r fontawesome::fa(name = "microscope", fill = "blue", height = "1em")` Parameter matrices

\fontsize{7pt}{12}\selectfont
As anticipated in \color{blue}[slide #11](#startingPoint)\color{black}, SEM works with matrices: it starts from a matrix (i.e., the observed covariance matrix) and it returns **matrices of estimated parameters**. Whereas we saw that parameters can be printed into regression-like tables, something more complex is happening under the hood: the model returns matrices of estimates:

\fontsize{6pt}{12}\selectfont

\begincols
  \begincol{.5\textwidth}

$\lambda$ = matrix of **factor loadings**
```{r , comment=NA}
inspect( pm1, "estimates")[1]
```
$\theta$ = matrix of **latent factor (co)variances**
```{r , comment=NA}
inspect( pm1, "estimates")[2]
```

Note: \color{blue} $\lambda$ and $\theta$ require latent variables

  \endcol
\begincol{.5\textwidth}

$\psi$ = matrix of observed variable **(co)variances** (i.e., the (co)variances estimated by the model)
```{r , comment=NA}
inspect( pm1, "estimates")[3]
```
$\beta$ = matrix of **regression coefficients** (paths)
```{r , comment=NA}
inspect( pm1, "estimates")[4]
```

  \endcol
\endcols

## Fitting a (multivariate) path model with R

\fontsize{7pt}{12}\selectfont
As we saw in \color{blue} [slide #20](#multipath) \color{black}, path analysis uses **one equation per endogenous variable**, and so does the model syntax used by `lavaan`:

\begincols
  \begincol{.5\textwidth}

- 1 endogenous = 1 equation
```{r , comment=NA}
model <- 'y1 ~ x1 + x2'
```
```{r echo=FALSE,warning=FALSE,message=FALSE}
library(DiagrammeR); library(DiagrammeRsvg); library(rsvg)

tmp <- grViz(
'digraph boxes_and_circles {
  graph [layout = neato]
  node [shape = box]
  x1 [pos="-2,0.5!"]
  x2 [pos="-2,-0.5!"]
  y1 [pos="0,0!"]
  node [shape = plaintext]
  e1 [label = <&epsilon;1> pos="1,0!"]
  # edges
  x1->y1 
  x2->y1
  e1->y1}')

# Convert to SVG, then save as png
tmp = export_svg(tmp)
tmp = charToRaw(tmp) # flatten
rsvg_png(tmp, "img/1end.png") # saved graph as png in current working directory
```
```{r , echo = FALSE, out.width = "110px",fig.align="center"}
knitr::include_graphics("img/1end.png")
```

- 2 endogenous = 2 equations
```{r , comment=NA}
model <- 'y1 ~ x1 + x2
          y2 ~ x1 + x2 + y1'
```
```{r echo=FALSE,warning=FALSE,message=FALSE}
library(DiagrammeR); library(DiagrammeRsvg); library(rsvg)

tmp <- grViz(
'digraph boxes_and_circles {
  graph [layout = neato]
  node [shape = box]
  x1 [pos="-2,0.5!"]
  x2 [pos="-2,-0.5!"]
  y1 [pos="0,0.5!"]
  y2 [pos="0,-0.5!"]
  node [shape = plaintext]
  e1 [label = <&epsilon;1> pos="1,0.5!"]
  e2 [label = <&epsilon;2> pos="1,-0.5!"]
  # edges
  x1->y1 
  x2->y1
  x1->y2 
  x2->y2
  y1->y2
  e1->y1
  e2->y2}')

# Convert to SVG, then save as png
tmp = export_svg(tmp)
tmp = charToRaw(tmp) # flatten
rsvg_png(tmp, "img/2end.png") # saved graph as png in current working directory
```
```{r , echo = FALSE, out.width = "110px",fig.align="center"}
knitr::include_graphics("img/2end.png")
```

  \endcol
\begincol{.5\textwidth}

- 3 endogenous = 3 equations
```{r , comment=NA}
model <- 'y1 ~ x1 + x2
          y2 ~ x1 + x2 + y1
          y3 ~ x1 + x2 + y1 + y2'
```
```{r echo=FALSE,warning=FALSE,message=FALSE}
library(DiagrammeR); library(DiagrammeRsvg); library(rsvg)

tmp <- grViz(
'digraph boxes_and_circles {
  graph [layout = neato]
  node [shape = box]
  x1 [pos="-2,0.5!"]
  x2 [pos="-2,-0.5!"]
  y1 [pos="0,0.5!"]
  y2 [pos="0,-0.5!"]
  y3 [pos="0,-1.5!"]
  node [shape = plaintext]
  e1 [label = <&epsilon;1> pos="1,0.5!"]
  e2 [label = <&epsilon;2> pos="1,-0.5!"]
  e3 [label = <&epsilon;3> pos="1,-1.5!"]
  # edges
  x1->y1 
  x2->y1
  x1->y2 
  x2->y2
  x1->y3
  x2->y3
  y1->y2
  y2->y3
  e1->y1
  e2->y2
  e3->y3}')

# Convert to SVG, then save as png
tmp = export_svg(tmp)
tmp = charToRaw(tmp) # flatten
rsvg_png(tmp, "img/3end.png") # saved graph as png in current working directory
```
```{r , echo = FALSE, out.width = "110px",fig.align="center"}
knitr::include_graphics("img/3end.png")
```

Note: similar to `lm()` and `lmer()`, we do not specify the error term in the formula, but just the exogenous variables (*x*) related to each endogenous variable (*y*)

  \endcol
\endcols

## Hands on `r fontawesome::fa(name = "r-project", fill="#3333B2",height = "1em")` (part 2)

\fontsize{8pt}{12}\selectfont

1. Using the `sem()` function from the `lavaan` package, fit a model corresponding to the path diagram represented in \color{blue} [slide #20](multipath) \color{black}; you can use the `semPaths(model_name)` function from the `semPlot` package to check whether you did it right

2. How many unknown parameters? Try answering before running the code

3. Print, interpret, and evaluate the statistical significance of the parameters estimated by the **unstandardized solution** and those estimated by the **standardized solution**

4. In the model formula, label the path$^1$ $\beta_{25}$ as "a", the $\beta_{12}$ as "b", and the path $\beta_{15}$ as "c", then add a new line of equation: \color{blue}`ab := a*b` \color{black}$^2$, fit the model again, and print the parameters \newline

\fontsize{6pt}{12}\selectfont

$^1$Note: to __label__ a path with a letter (or a word), just write the letter before the corresponding predictor and put a `*` between them, for example: \color{blue} `MAT ~ a*QI` \color{black}

$^2$ The symbol \color{blue} `:=` \color{black} stands for "Define non-model parameter" (i.e., creating a parameter by combining other parameters)

## `r fontawesome::fa(name = "microscope", fill = "blue", height = "1em")` Labeled and composed parameters in lavaan

\fontsize{7.5pt}{12}\selectfont
In `lavaan`, it is possible to **label parameters** (i.e., to give a name to a parameter, similar to how we do when we create an R object with the `<-` symbol) by 'multiplying' the parameter label with the name of the variable corresponding to that parameter.

For instance, here we call the path between QI and WM "a", whereas we call "b" and "c" the paths linking `MAT` to `WM` and `QI`, respectively. The path linking `STM` to `MAT` is called "tony" :)
```{r , comment=NA}
model <- 'MAT ~ b*WM + c*QI + ANS + tony*STM
          WM ~ a*QI'
```

Why should we label parameters? Because this allows **creating new parameters as a combination** of other parameters. And this is needed in many analyses, including **mediation**.

```{r , comment=NA}
model <- 'MAT ~ b*WM + c*QI + ANS + tony*STM
          WM ~ a*QI
          # the new parameter "ab" is the product between "a" and "b" 
          ab := a*b
          # the new parameter "total" is the sum of "c" and "ab"
          total := c + ab'
```

## Mediation analysis

\fontsize{7pt}{12}\selectfont
A **mediation model** is a multivariate model that attempts to identify and explain the relationship between a **predictor** (*X*) and an **outcome** variable (*Y*) when we hypothesize that a third variable (**mediator** *M*) can influence the direct relationship between *X* and *Y*. \newline

\fontsize{8pt}{12}\selectfont
__Note: mediation is different from moderation__ \newline

\begincols
  \begincol{.5\textwidth}

\color{blue}
__Mediation &rightarrow; indirect effect__ \fontsize{7pt}{12}\selectfont \color{black}

A **mediator** is expected to be influenced by the predictor and to influence the outcome &rightarrow; **indirect effect** of the predictor *through* the mediator.

```{r , comment=NA}
model <- 'Y ~ M
          M ~ X'
```
```{r echo=FALSE,warning=FALSE,message=FALSE}
library(DiagrammeR); library(DiagrammeRsvg); library(rsvg)

tmp <- grViz(
'digraph boxes_and_circles {
  graph [layout = neato]
  node [shape = box]
  X [pos="-2,1!"]
  M [pos="0,1!"]
  Y [pos="2,1!"]
  node [shape = plaintext]
  eM [label = <&epsilon;<sub>M</sub>> pos="0,0!"]
  eY [label = <&epsilon;<sub>Y</sub>> pos="2,0!"]
  # edges
  X->M
  M->Y
  eM->M
  eY->Y}')

# Convert to SVG, then save as png
tmp = export_svg(tmp)
tmp = charToRaw(tmp) # flatten
rsvg_png(tmp, "img/med.png") # saved graph as png in current working directory
```
```{r , echo = FALSE, out.width = "110px",fig.align="center"}
knitr::include_graphics("img/med.png")
```
  
  \endcol
\begincol{.5\textwidth}

\color{blue}
__Moderation &rightarrow; interaction__ \fontsize{7pt}{12}\selectfont \color{black}

A **moderator** is expected to modulate the relationship between predictor and outcome (e.g., stronger/weaker relationship for higher levels of the moderators), without being necessarily related to *X* and *Y*.

```{r , comment=NA}
model <- 'Y ~ M * X'
```
```{r , echo = FALSE, out.width = "115px",fig.align="center"}
knitr::include_graphics("img/mod.png")
```

  \endcol
\endcols

## Types of effects in a mediation model

\fontsize{8pt}{12}\selectfont
A mediation model involves three types of effects:

- __Direct effects__: direct influence of the predictor *X* on the outcome *Y* \newline (path \color{red}*c*\color{black}), as indexed by regression/path coefficients $\beta$

- __Indirect effects__: indirect influence of *X* on *Y* through a third variable *M* that mediates the two of them, computed as the **product of the direct effects** of *X* on *M* and *Y* (\color{red}$a \times b$ \color{black})

- __Total effects__: sum of direct and indirect effects of *X* on *Y* (\color{red} $a \times b + c$\color{black})

```{r echo=FALSE,warning=FALSE,message=FALSE}
library(DiagrammeR); library(DiagrammeRsvg); library(rsvg)

tmp <- grViz(
'digraph boxes_and_circles {
  graph [layout = neato]
  node [shape = box]
  X [pos="-2,1!"]
  M [pos="0,0!"]
  Y [pos="2,1!"]
  node [shape = plaintext]
  eM [label = <&epsilon;<sub>M</sub>> pos="0,-1!"]
  eY [label = <&epsilon;<sub>Y</sub>> pos="2,0!"]
  # edges
  X->M [label = <a>]
  M->Y [label = <b>]
  X->Y [label = <c>]
  eM->M
  eY->Y}')

# Convert to SVG, then save as png
tmp = export_svg(tmp)
tmp = charToRaw(tmp) # flatten
rsvg_png(tmp, "img/med2.png") # saved graph as png in current working directory
```
```{r , echo = FALSE, out.width = "150px"}
knitr::include_graphics("img/med2.png")
```

Note: **when the direct effect is equal to zero** (and thus, total effect = indirect effect), we call it \color{violet} **full mediation**\color{black}, otherwise we call it \color{violet} **partial mediation**

## Mediation analysis in lavaan

\fontsize{7.5pt}{12}\selectfont

\begincols
  \begincol{.5\textwidth}

First, we specify the model as we are used:
```{r , comment=NA}
model <- 'Y ~ X + M
          M ~ X'
```

Second, to distinguish *direct* and *mediation* effects, we can rewrite the same model by splitting the first equation in two different equation (i.e., equivalent to the first one):
```{r , comment=NA}
model <- '# direct effect
          Y ~ X
          # mediation effects
          Y ~ M
          M ~ X'
```

\color{white}_\color{black} \newline \newline

\fontsize{6.5pt}{12}\selectfont
Note: the symbol \color{red} `:=` \color{black} stands for "*non-model parameter defined as*"

  \endcol
\begincol{.5\textwidth}

Third, we label the effects:
```{r , comment=NA}
model <- '# direct effect
          Y ~ c*X
          # mediation effects
          Y ~ b*M
          M ~ a*X'
```

Finally, we define new parameters as the combination of the other parameters:
```{r , comment=NA}
model <- '# direct effect
          Y ~ c*X
          # mediation effects
          Y ~ b*M
          M ~ a*X
          # indirect effect
          ab := a*b
          # total effect
          tot := c + (a*b)'
```

  \endcol
\endcols

## Mediation model fit

\fontsize{8pt}{12}\selectfont
Here's our mediation model of `QI` (predictor), `WM` (mediator), and `MAT` (outcome):
```{r , comment=NA}
mymodel <- '# direct effect
            MAT ~ c*QI
            # mediation effects
            MAT ~ b*WM
            WM ~ a*QI
            # indirect effect
            ab := a*b
            # total effect
            tot := c + (a*b)'
fit <- sem(model = mymodel, data = earlymath)
```
```{r echo=FALSE,warning=FALSE,message=FALSE}
library(DiagrammeR); library(DiagrammeRsvg); library(rsvg)

tmp <- grViz(
'digraph boxes_and_circles {
  graph [layout = neato]
  node [shape = box]
  X [label = <QI> pos="-2,1!"]
  M [label = <WM> pos="0,0!"]
  Y [label = <MAT> pos="2,1!"]
  node [shape = plaintext]
  eM [label = <&epsilon;<sub>M</sub>> pos="0,-1!"]
  eY [label = <&epsilon;<sub>Y</sub>> pos="2,0!"]
  # edges
  X->M [label = <a>]
  M->Y [label = <b>]
  X->Y [label = <c>]
  eM->M
  eY->Y}')

# Convert to SVG, then save as png
tmp = export_svg(tmp)
tmp = charToRaw(tmp) # flatten
rsvg_png(tmp, "img/med3.png") # saved graph as png in current working directory
```
```{r , echo = FALSE, out.width = "150px",fig.align='center'}
knitr::include_graphics("img/med3.png")
```

## Mediation model output

\fontsize{8pt}{12}\selectfont
Here are the parameters estimated by our mediation model:
```{r , comment=NA}
parameterestimates(fit)[,1:8]
```
```{r , comment=NA, echo=FALSE}
p <- parameterestimates(fit)[,1:8]
```

\color{blue}
How to interpret them? Is this a partial or a full mediation? \color{black}

- __Direct effect = \color{red}$c$\color{black}__ = `r round(p[1,5],3)` (SE = `r round(p[1,6],3)`), *z* = `r round(p[1,7],3)`, *p* = `r round(p[1,8],3)` &rightarrow; in this case it is positive and significant (i.e., thus, it is a **partial mediation**)

>- __Indirect effect = \color{red}$a \times b$\color{black}__ = `r round(p[7,5],3)` (SE = `r round(p[7,6],3)`), *z* = `r round(p[7,7],3)`, *p* = `r round(p[7,8],3)` &rightarrow; in this case it is positive and significant

>- __Total effect = \color{red}$c + (a \times b)$\color{black}__ = `r round(p[8,5],3)` (SE = `r round(p[8,6],3)`), *z* = `r round(p[8,7],3)`, *p* = `r round(p[8,8],3)` &rightarrow; in this case it is positive and significant

## Hands on `r fontawesome::fa(name = "r-project", fill="#3333B2",height = "1em")` (part 3)

\fontsize{8pt}{12}\selectfont

1. Modify the model specified in the last point of Part 2 by adding the indirect and total effect

2. Print and interpret the estimated parameters

3. Visualize the model by using the `semPaths(model_name)` function from the `semPlot` package

## That's all for now!

\fontsize{8pt}{12}\selectfont 
__Questions?__ \newline

__Homework__ (optional):

- read the slides presented today \newline and write in the Moodle forum if you have any doubts

- **exe`r fontawesome::fa(name = "r-project", fill="#3333B2",height = "1em")`cises 16-17** from `exeRcises.pdf` \newline \newline

\fontsize{6pt}{12}\selectfont ____ \newline 
For each exercise, the solution (or one of the possible solutions) can be found in dedicated chunk of commented code within the `exeRcises.Rmd` file

# Resources

## Credits

\fontsize{8pt}{12}\selectfont
The present slides are partially based on: \fontsize{6pt}{12}\selectfont

- Beaujean, A. A. (2014) Latent Variable Modeling Using R. A Step-by-Step Guide. New York: Routledge

- Pastore, M. (2015). Analisi dei dati in psicologia (e applicazioni in R). Il Mulino.

- Pastore, M. (2021). Analisi dei dati in ambito di comunità

## Achronyms & Greek letters

\begincols
  \begincol{.45\textwidth}

\fontsize{6pt}{12}\selectfont

- CFA: confirmatory factor analysis

- LM: linear models/modeling

- LV: latent variable

- OV: observed variable

- SEM: structural equation models/modeling

>- SS: sum of squares

  \endcol
\begincol{.6\textwidth}

\fontsize{6pt}{12}\selectfont

- $\beta$ = *beta*, indexing path coefficients (or regression coefficients)

- $\epsilon$ = *epsilon*, indexing the error of an observed variable

- $\sigma$ = *sigma*, indexing the variance $\sigma^2$ of the errors $\epsilon$

- $\eta$ = *eta*, indexing latent variables

- $\theta$ = *theta*, indexing overall model parameters

>- ciao

  \endcol
\endcols